---
title: Yet Another Titanic Solve
author: Rahul Sangole
date: '2018-03-30'
slug: yet-another-titanic-solve
categories:
  - R
tags:
  - caret
  - R
  - kaggle
output:
    blogdown::html_page:
        toc: true
---


<div id="TOC">
<ul>
<li><a href="#objectives">Objectives</a></li>
<li><a href="#read-in-the-dataset">Read in the dataset</a></li>
<li><a href="#train-test-split">Train-Test Split</a></li>
<li><a href="#missing-values-analysis">Missing values analysis</a></li>
<li><a href="#eda">EDA</a><ul>
<li><a href="#target-variable">Target Variable</a></li>
<li><a href="#predictor-variables">Predictor Variables</a><ul>
<li><a href="#univariate-bivariate">Univariate &amp; Bivariate</a></li>
<li><a href="#multivariate-analyses">Multivariate Analyses</a></li>
</ul></li>
</ul></li>
<li><a href="#data-preparation">Data Preparation</a><ul>
<li><a href="#missing-values-imputation">Missing Values Imputation</a></li>
<li><a href="#derived-variables">Derived Variables</a></li>
<li><a href="#final-data-review">Final Data Review</a></li>
</ul></li>
<li><a href="#modeling">Modeling</a><ul>
<li><a href="#extreme-gradient-boosting">Extreme Gradient Boosting</a></li>
<li><a href="#elastinet">Elastinet</a></li>
<li><a href="#k-nn">k-NN</a></li>
<li><a href="#svm">SVM</a></li>
<li><a href="#c5.0">C5.0</a></li>
<li><a href="#averaged-neural-networks">Averaged Neural Networks</a></li>
<li><a href="#conditional-inference-random-forests">Conditional Inference Random Forests</a></li>
</ul></li>
<li><a href="#compare-models">Compare models</a></li>
<li><a href="#test-set-evaluation">Test Set Evaluation</a><ul>
<li><a href="#create-test-set">Create test set</a></li>
<li><a href="#predict-test-results">Predict test results</a></li>
</ul></li>
<li><a href="#kaggle-performance">Kaggle Performance</a></li>
</ul>
</div>

<p><strong>tl;dr:</strong> Another titanic solve. Uses <code>caret</code> and <code>tidyverse</code> for everything. Best model is elastinet and gets me a 0.72-0.76 score on kaggle. Enjoy.</p>
<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ol style="list-style-type: decimal">
<li>End to end analysis using R</li>
<li>Learn the <code>caret</code> package for ML</li>
<li>Learn to present the case using R Notebooks</li>
</ol>
<hr />
</div>
<div id="read-in-the-dataset" class="section level1">
<h1>Read in the dataset</h1>
<p>I stored the raw files on Github, so I used <a href="https://cran.r-project.org/web/packages/RCurl/index.html">RCurl</a> with <a href="https://github.com/wehrley/wehrley.github.io/blob/master/SOUPTONUTS.md">Wehrley’s method</a> that utilizes read.csv to the fullest. It’s one of the best ways I’ve found to read in data and also set data-types at the same time. He’s done a great job on that function. The dataset contains one ID variable, one response variable and ten predictor variables.</p>
<pre class="r"><code>library(RCurl,quietly = T)
library(tidyverse,quietly = T)
library(ggplot2,quietly = T)
library(gridExtra,quietly = T)
library(beanplot,quietly = T)
library(caret,quietly = T)
library(stringr,quietly = T)
library(party, quietly = T)
library(xgboost, quietly = T)
library(skimr, quietly = T)
library(alluvial, quietly = T)
library(pROC, quietly = T)
library(ggrepel, quietly = T)

readData &lt;- function(path.name, file.name, column.types, missing.types) {
    gurl &lt;- paste(path.name,file.name,sep=&quot;&quot;)
    download.file(gurl,file.name,method=&quot;curl&quot;,quiet = T)
    tbl_df(read.csv(file.name,colClasses=column.types,
             na.strings=missing.types))
}

Titanic.path &lt;- &quot;https://raw.githubusercontent.com/rsangole/Titanic/master/&quot;
train.data.file &lt;- &quot;train.csv&quot;
test.data.file &lt;- &quot;test.csv&quot;
missing.types &lt;- c(&quot;NA&quot;, &quot;&quot;)
train.column.types &lt;- c(&#39;integer&#39;,   # PassengerId
                        &#39;factor&#39;,    # Survived
                        &#39;factor&#39;,    # Pclass
                        &#39;character&#39;, # Name
                        &#39;factor&#39;,    # Sex
                        &#39;numeric&#39;,   # Age
                        &#39;integer&#39;,   # SibSp
                        &#39;integer&#39;,   # Parch
                        &#39;character&#39;, # Ticket
                        &#39;numeric&#39;,   # Fare
                        &#39;character&#39;, # Cabin
                        &#39;factor&#39;     # Embarked
)

test.column.types &lt;- train.column.types[-2]     # # no Survived column in test.csv
train.raw &lt;- readData(path.name = Titanic.path,
                      file.name = train.data.file,
                      column.types = train.column.types,
                      missing.types = missing.types)
kaggletest.raw &lt;- readData(path.name = Titanic.path,
                     file.name = test.data.file,
                     column.types = test.column.types,
                     missing.types = missing.types)

prep_data &lt;- function(D) {
    if (!is.null(D$Survived)) {
        D$Survived &lt;- factor(D$Survived,
                             levels = c(1, 0),
                             labels = c(&#39;Survived&#39;, &#39;Dead&#39;))
        }
    D$Pclass &lt;- factor(D$Pclass,
                       levels = c(1, 2, 3),
                       labels = c(&#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;))
    D$PassengerId &lt;- NULL
    D
}

train.raw &lt;- prep_data(train.raw)
kaggletest.raw &lt;- prep_data(kaggletest.raw)</code></pre>
<hr />
</div>
<div id="train-test-split" class="section level1">
<h1>Train-Test Split</h1>
<p>Here, I’m splitting up the <code>train</code> dataset into a training and a testing dataset. Right now, I don’t intend to upload any results back into Kaggle, so I’m just going to evaluate my models on my testing split.</p>
<pre class="r"><code>set.seed(1071)
training_rows &lt;- caret::createDataPartition(y = train.raw$Survived, p = 0.7, list = F)
test.raw &lt;- train.raw %&gt;% filter(!(rownames(.) %in% training_rows))
train.raw &lt;- train.raw %&gt;% filter(rownames(.) %in% training_rows)
dim(train.raw)</code></pre>
<pre><code>## [1] 625  11</code></pre>
<pre class="r"><code>dim(test.raw)</code></pre>
<pre><code>## [1] 266  11</code></pre>
<hr />
</div>
<div id="missing-values-analysis" class="section level1">
<h1>Missing values analysis</h1>
<p>Quick investigation of missing values can be done using the <code>complete.cases()</code>, and more thorough graphical summary can be done using Amelia. Overall, 79% of the observations have <em>some</em> missing data.</p>
<pre class="r"><code>#Complete cases (percentages)
round(prop.table(table(complete.cases(train.raw))),2)</code></pre>
<pre><code>##
## FALSE  TRUE
##  0.79  0.21</code></pre>
<p>Amelia lets us graphically investigate which variables have missing data. <code>purr::map_xxx()</code> gives this same information numerically in a succint fashion.</p>
<pre class="r"><code>Amelia::missmap(train.raw, main=&#39;Missing Values Analysis using Amelia ordered by % missing&#39;, col=c(&#39;red&#39;, &#39;gray&#39;),legend = F,rank.order = T)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>#Missing cases (numbers):
map_int(train.raw,~sum(is.na(.x)))</code></pre>
<pre><code>## Survived   Pclass     Name      Sex      Age    SibSp    Parch   Ticket
##        0        0        0        0      117        0        0        0
##     Fare    Cabin Embarked
##        0      478        2</code></pre>
<p>Cabin has a large number of missing values (77% missing). Imputing this variable may prove challenging or even useless. Age (19.9% missing) and Embarked (0.2%) missing are much more managable.</p>
<hr />
</div>
<div id="eda" class="section level1">
<h1>EDA</h1>
<p>The first step in the analysis is to explore the data numerically and graphically. I always split up my EDA investigation as follows:</p>
<ul>
<li>Target Variable</li>
<li>Predictor Variables
<ul>
<li>Univariate</li>
<li>Bivariate</li>
<li>Multivariate</li>
</ul></li>
</ul>
<p>This gives me a structured approach towards larger datasets. My <a href="http://www.syamalasrinivasan.com/">professor</a> at Northwestern taught me to always complete a thorough intimate numeric &amp; graphical EDA on the data, no matter how large the data <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. <a href="http://www.jstor.org/stable/2682899">Anscombe</a> (1973) clearly shows the importance of graphical analyses.</p>
<div id="target-variable" class="section level2">
<h2>Target Variable</h2>
<p><code>Survived</code> is the response variable. As we can see, a large majority of the passengers did not survive the accident. The response variable is a False/True boolean variable. Thus, the analysis techniques used later will be those appropriate for classification problems.</p>
<pre class="r"><code>round(prop.table(table(train.raw$Survived)),2)</code></pre>
<pre><code>##
## Survived     Dead
##     0.38     0.62</code></pre>
<p>The classes are ~40:60 split which isn’t too bad. Probably don’t have to do any class balancing work.</p>
<hr />
</div>
<div id="predictor-variables" class="section level2 tabset tabset-fade">
<h2>Predictor Variables</h2>
<div id="univariate-bivariate" class="section level3">
<h3>Univariate &amp; Bivariate</h3>
<p>The first step is to look at every variable available. I prefer using the <code>ggplot2</code> framework for all the visuals.</p>
<div id="continuous-variables" class="section level4">
<h4>Continuous Variables</h4>
<ul>
<li><p><code>Age</code> seems to have a bimodal distribution - very young children, and then directly young adults to mid-age persons. The 2nd mode is right skewed with no obvious outliers.</p></li>
<li><p><code>Fare</code> certainly shows many outliers beyond the ~$200 level. A majority of the fares are &lt;$50, which makes sense since a majority of the travelers are bound to be in the 3rd passenger class.</p></li>
</ul>
<pre class="r"><code>p1 &lt;- ggplot(data=train.raw,aes(x=Age))  + geom_histogram(aes(fill=Survived),bins = 40) + coord_flip()
p2 &lt;- ggplot(data=train.raw,aes(x=Fare)) + geom_histogram(aes(fill=Survived),bins = 40) + coord_flip()
grid.arrange(p1,p2,nrow=1)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>As we can see, the median fare is $14.5, the mean is $32, but the max is $512. We’ll investigate winzorising this variable in the latter part. Perhaps a transformation will also help?</p>
<pre class="r"><code>summary(train.raw$Fare)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
##   0.000   7.925  14.500  33.710  31.000 512.329</code></pre>
</div>
<div id="categorical-variables" class="section level4">
<h4>Categorical Variables</h4>
<p>A ggplot command is iterated over for the categorical variables.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Key takeways for the categorical variables:</p>
<ol style="list-style-type: decimal">
<li><code>Pclass</code>: If you were traveling 1st class, you have the highest chance of survival. Could be indicative of preferential treatment to those who paid more, a less politically correct class-stratified society, as well as the fact that the 1st class passengers had cabins at the very top of the ship.</li>
<li><code>Pclass</code>: Persons traveling 3rd class had the highest fatality rate. 3rd class passengers had cabins deep in the ship. With the reasons give in (1), this could have contributed to the low survival rate.</li>
<li><code>Sex</code>: Males have a very high fatality rate. Seems like the ‘women and children’ first policy was followed during evacuation.</li>
<li><code>SibSp</code> &amp; <code>Parch</code>: What’s interesting here is, for both these variables, at level 0, the fatality rate is higher. At levels 1+, the chances of survival are much better. Again, this could point to the ‘women <em>and children</em>’ policy being followed. (Or perhaps there weren’t as many families with children on board!)</li>
<li><code>Embarked</code>: Southampton has a higher fatality rate than Cherbourg or Queenstown. A cross-tabulation between <code>Embarked</code> and <code>Pclass</code> shows that 72% of the 3rd class passengers and 89% of the 2nd class passengers boarded at Southampton. This jives with the observation that 2nd and 3rd class passengers have higher fatality rates.</li>
</ol>
<pre class="r"><code>get_legend&lt;-function(myggplot){
  tmp &lt;- ggplot_gtable(ggplot_build(myggplot))
  leg &lt;- which(sapply(tmp$grobs, function(x) x$name) == &quot;guide-box&quot;)
  legend &lt;- tmp$grobs[[leg]]
  return(legend)
}
p &lt;- lapply(X = c(&#39;Pclass&#39;,&#39;Sex&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Embarked&#39;),
            FUN = function(x) ggplot(data = train.raw)+
                aes_string(x=x,fill=&#39;Survived&#39;)+
                geom_bar(position=&quot;dodge&quot;)+
                theme(legend.position=&quot;none&quot;))
legend &lt;- get_legend(ggplot(data = train.raw,aes(x=Pclass,fill=Survived))+geom_bar())
grid.arrange(p[[1]],p[[2]],p[[3]],p[[4]],p[[5]],
             legend,layout_matrix = cbind(c(1,2,3),
                                          c(4,5,3),
                                          c(6,6,6)),
             widths=c(3,3,1))</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
<div id="multivariate-analyses" class="section level3">
<h3>Multivariate Analyses</h3>
<p>Grouped boxplots are a common method of comparing distributions grouped by categorical variables. I find <a href="https://cran.r-project.org/web/packages/beanplot/beanplot.pdf">beanplots</a> to be excellent complementary plots to boxplots (and in some cases, even better). They’re a bit tricky to read at first - since they are so underutilized - but just through one plot, a wealth of information can be extracted.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>Here is a comparison of the same information between a boxplot and a beanplot. What can we infer from the bean plot better?</p>
<ol style="list-style-type: decimal">
<li>The beanplot allows us to visualize the density function of the parameter, in this case: Age. Furthermore, the length of each beanline is cumulative to the number of datapoints that exist. Rightaway, we can tell that Pclass=3 has the most data in the set, with sparser data at Pclass=1.</li>
<li>The mean values for 1st class is higher than that for 2nd and 3rd class. The distributions of deceased and survived for 1st class are fairly similar.</li>
<li>For 2nd and 3rd class, the survived data shows a bimodal distribution. Bumps at the 0-10 age show that children were evacuated first. This is also the reason the mean values for survived is lower.</li>
<li>For 2nd and 3rd class, the deceased data shows a fairly normal distribution.</li>
<li>The individual measurements (represented by black lines) represent each observation and help identify outliers much more easily than a boxplot does.</li>
</ol>
<pre class="r"><code>ggplot(train.raw,aes(y=Age,x=Pclass))+geom_boxplot(aes(fill=Survived))+theme_bw()
beanplot(Age~Survived*Pclass,side=&#39;b&#39;,train.raw,col=list(&#39;yellow&#39;,&#39;orange&#39;),
         border = c(&#39;yellow2&#39;,&#39;darkorange&#39;),ll = 0.05,boxwex = .5,
         main=&#39;Passenger survival by pclass and Age&#39;,xlab=&#39;Passenger Class&#39;,ylab=&#39;Age&#39;)
legend(&#39;topright&#39;, fill = c(&#39;yellow&#39;,&#39;orange&#39;), legend = c(&quot;Dead&quot;, &quot;Survived&quot;),bty = &#39;n&#39;,cex = .8)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-9-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-9-2.png" width="60%" style="display: block; margin: auto;" /></p>
<p>A look into the <code>SibSp</code> and <code>Parch</code> variables shows something interesting. There are three regions one can identify:</p>
<ul>
<li>The probability of survival is minimal for number of parents/children aboard &gt; 3.</li>
<li>The probability of survival is minimal for number of siblings/spouses aboard &gt; 3.</li>
<li>For <code>SibSp</code>&lt;=3 and <code>Parch</code>&lt;=3, there are better chances for survival.</li>
</ul>
<p>The grouping by <code>Pclass</code> reveals that all the large families were 3rd class travelers. Worse access to help… lowest chance for survival.</p>
<p>These could be simple rules either hard coded during model building: something along the lines of: <em>IF (SibSp&gt;3 OR Parch &gt;3) THEN prediction = 0</em>, or some derived variables can be created.</p>
<pre class="r"><code>ggplot(train.raw,aes(y=SibSp,x=Parch))+
    geom_jitter(aes(color=Survived,shape=Pclass))+
    theme_bw()+
    scale_shape(solid=F)+
    geom_vline(xintercept = 3,color=&#39;darkred&#39;,lty=2)+
    geom_hline(yintercept = 3,color=&#39;red&#39;,lty=2)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Another excellent way to look at multivariate data is using <a href="https://cran.r-project.org/web/packages/alluvial/vignettes/alluvial.html">alluvial</a> plots. Not only are these plots intuitive to read and visually stiking, but they offer a lot of insight into the structure of the multivariate data as well.</p>
<pre class="r"><code>train.raw %&gt;%
    mutate(Age_Group = case_when(
        Age &lt; 18 ~ &#39;Child&#39;,
        Age &gt;= 18 ~ &#39;Adult&#39;
    )) %&gt;%
  group_by(Survived, Sex, Pclass, Age_Group) %&gt;%
  summarise(N = n()) %&gt;%
  ungroup %&gt;%
  na.omit -&gt; alluvial_table

alluvial(alluvial_table[,c(-5)],
         freq = alluvial_table$N,
         cex = 0.8,
         col=ifelse(alluvial_table$Survived == &quot;Survived&quot;, &quot;blue&quot;, &quot;forestgreen&quot;))</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<hr />
</div>
</div>
</div>
<div id="data-preparation" class="section level1">
<h1>Data Preparation</h1>
<div id="missing-values-imputation" class="section level2">
<h2>Missing Values Imputation</h2>
<p>Starting with the easier one first:</p>
<p><strong>Embarked</strong>: The largest portion of the passengers embared at Southhampton. I’m replacing the NAs with the same. First, I create a new imputed training dataset.</p>
<pre class="r"><code>summary(train.raw$Embarked)</code></pre>
<pre><code>##    C    Q    S NA&#39;s
##  120   57  446    2</code></pre>
<pre class="r"><code>train.imp &lt;- train.raw
train.imp$Embarked[is.na(train.imp$Embarked)] &lt;- &#39;S&#39;</code></pre>
<p><strong>Names, Titles &amp; Age</strong>: The names have titles embedded in the strings. I can extract these using regex. Master, Miss, Mr and Mrs are the most popular - no surprise there, with lots of other titles. Here’s the distribution of the titles by age. These can be used to impute the missing age values.</p>
<pre class="r"><code>train.imp$title &lt;- str_extract(pattern = &#39;[a-zA-Z]+(?=\\.)&#39;,string = train.imp$Name)
train.imp$title &lt;- as.factor(train.imp$title)
ggplot(train.imp,aes(x=title,y=Age))+
    geom_jitter(shape=21,alpha=.6,col=&#39;blue&#39;)+
    stat_summary(aes(y = Age,group=1), fun.y=median, colour=&quot;red&quot;, geom=&quot;point&quot;,group=1)+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1),legend.position=&quot;none&quot;)+
    labs(caption=&#39;red points are median values&#39;)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-13-1.png" width="60%" /></p>
<p>Grouping similar titles together, I’ve kept a few titles - Officer, Royalty, Mr, Mrs and Miss.</p>
<pre class="r"><code>train.imp$title &lt;- as.character(train.imp$title)
train.imp$title[train.imp$title %in% c(&#39;Capt&#39;,&#39;Col&#39;,&#39;Major&#39;)] &lt;- &#39;Officer&#39;
train.imp$title[train.imp$title %in% c(&#39;Don&#39;,&#39;Dr&#39;,&#39;Rev&#39;,&#39;Sir&#39;,&#39;Jonkheer&#39;,&#39;Countess&#39;,&#39;Lady&#39;,&#39;Dona&#39;)] &lt;- &#39;Royalty&#39;
train.imp$title[train.imp$title %in% c(&#39;Mrs&#39;,&#39;Mme&#39;)] &lt;- &#39;Mrs&#39;
train.imp$title[train.imp$title %in% c(&#39;Ms&#39;,&#39;Mlle&#39;)] &lt;- &#39;Miss&#39;
train.imp$title &lt;- as.factor(train.imp$title)
ggplot(train.imp,aes(x=title,y=Age))+
    geom_jitter(color=&#39;blue&#39;,shape=21,alpha=.7)+
    stat_summary(aes(y = Age,group=1), fun.y=median, colour=&quot;red&quot;, geom=&quot;point&quot;,group=1)+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    labs(caption=&#39;red points are median values&#39;)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-14-1.png" width="60%" /></p>
<p>Now for the missing Age values. I’m trying out two strategies to impute age, just for kicks. First, a regression tree using the <code>rpart</code> method. 5-repeat 10-fold cross validation across a tuning grid of 20 values of <code>maxdepth</code>. RMSE stablizes at a depth of 14, with a value of 12.2.</p>
<pre class="r"><code>age.predictors &lt;- train.imp %&gt;%
    dplyr::select(-Survived,-Cabin,-Ticket,-Name) %&gt;%
    dplyr::filter(complete.cases(.))
ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5)
rpartGrid &lt;- data.frame(maxdepth = seq(2,10,1))
rpartFit_ageimputation &lt;- train(x=age.predictors[,-3],
                      y=age.predictors$Age,
                      method=&#39;rpart2&#39;,
                      trControl = ctrl,
                      tuneGrid = rpartGrid
                      )
rpartFit_ageimputation</code></pre>
<pre><code>## CART
##
## 508 samples
##   7 predictor
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 457, 457, 457, 457, 457, 457, ...
## Resampling results across tuning parameters:
##
##   maxdepth  RMSE      Rsquared   MAE
##    2        12.02414  0.3171031  9.443687
##    3        11.30498  0.3985131  8.707856
##    4        11.42463  0.3882499  8.782511
##    5        11.27085  0.4038018  8.639549
##    6        11.39825  0.3930011  8.720958
##    7        11.43177  0.3890118  8.744528
##    8        11.47797  0.3851413  8.783542
##    9        11.48005  0.3848860  8.783870
##   10        11.48005  0.3848860  8.783870
##
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was maxdepth = 5.</code></pre>
<pre class="r"><code>plot(rpartFit_ageimputation)
rpart.plot::rpart.plot(rpartFit_ageimputation$finalModel, extra=101, box.palette=&quot;GnBu&quot;)
save(rpartFit_ageimputation,file = &#39;rpartFit_ageimputation&#39;)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-15-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-15-2.png" width="60%" style="display: block; margin: auto;" /></p>
<p>I’m going to use the randomForest model. Using the <code>predict.train()</code> to predict values of age and plug them back into the imputed data. You can see the blue points which are the imputed values of <code>Age</code>. What I noticed is that for all the titles, the imputed Age value seems to be distributed fairly well, except Master. For Master, the three imputed are definitely outliers. I’m going to force these to the median Age.</p>
<pre class="r"><code>missing_age &lt;- is.na(train.imp$Age)
age.predicted &lt;- predict(rpartFit_ageimputation, newdata = train.imp[missing_age,])
train.imp[missing_age,&#39;Age&#39;] &lt;- age.predicted

train.imp %&gt;%
    mutate(Age_Imputed = missing_age) %&gt;%
    ggplot(aes(x=title,y=Age))+
    stat_summary(aes(y = Age,group=1), fun.y=median, colour=&quot;red&quot;, geom=&quot;point&quot;,group=1)+
    geom_jitter(aes(y=Age,col=Age_Imputed,shape=Age_Imputed))+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1),legend.position=&quot;none&quot;)+
    labs(caption=&#39;green points are imputed values&#39;)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-16-1.png" width="60%" /></p>
</div>
<div id="derived-variables" class="section level2">
<h2>Derived Variables</h2>
<p><strong>Child?:</strong> Trying out two engineered variables here - is the passenger a child or not? Using Age=18 as a threshold.</p>
<pre class="r"><code>train.imp$child &lt;- 0
train.imp$child[train.imp$Age&lt;18] &lt;- 1</code></pre>
<p><strong>Really old?:</strong> Really older folks would get priority perhaps. Creating a categorical variables for this conditions.</p>
<pre class="r"><code>train.imp$Seniors &lt;- ifelse(train.imp$Age&gt;60,1,0)</code></pre>
<p><strong>Family related:</strong> Let’s also create some variables that talk about family sizes. What’s the total family size – continous variable <code>TotalFam</code>. Is the person single, part of a couple or a large family? Three categorical variables for these.</p>
<pre class="r"><code>train.imp$TotalFam &lt;- train.imp$SibSp + train.imp$Parch + 1
train.imp$LargeFamily &lt;- ifelse(train.imp$TotalFam&gt;4,1,0)
train.imp$Name &lt;- NULL</code></pre>
<p><strong>Cabin related:</strong> Extracting the cabin alphabet and number from the cabin variable. Since the cabin numbers could be ordered from left to right or top to bottom on the boat, perhaps only the 1st digit is significant. Also, some folks have more than 1 cabin. Wonder if that’s important. Since lots of unknowns in the <code>Cabin</code> variable, all NA values are replaced by ‘U’. Refering to the deck diagram, the topmost decks are A and B, which are closest to the lifeboats. Perhaps that’s important too. Here, I create a bunch of categorical variables based off the original <code>Cabin</code>, and then remove it from the dataset.</p>
<pre class="r"><code>train.imp$CabinMissing &lt;- as.numeric(is.na(train.raw$Cabin))

train.imp$CabinCode &lt;- map_chr(train.raw$Cabin,~str_split(string = .x,pattern = &#39;&#39;)[[1]][1])
train.imp$CabinCode[is.na(train.imp$CabinCode)] &lt;- &#39;U&#39;
train.imp$CabinCode &lt;- as.factor(train.imp$CabinCode)

train.imp$CabinNum &lt;- as.numeric(map_chr(train.raw$Cabin,~str_split(string = .x,pattern = &#39;[a-zA-Z]&#39;)[[1]][2]))
train.imp$CabinNum &lt;- map_int(train.imp$CabinNum, ~as.integer(str_split(.x,pattern = &#39;&#39;,simplify = T)[1][1]))
train.imp$CabinNum[is.na(train.imp$CabinNum)] &lt;- 0

train.imp$TopDeck &lt;- ifelse(train.imp$CabinCode %in% c(&#39;A&#39;,&#39;B&#39;),1,0)
train.imp$MidDeck &lt;- ifelse(train.imp$CabinCode %in% c(&#39;C&#39;,&#39;D&#39;),1,0)
train.imp$LowerDeck &lt;- ifelse(train.imp$TopDeck==0 &amp; train.imp$MidDeck==0 ,1,0)

train.imp$NumberofCabins &lt;- map_int(train.raw$Cabin,~str_split(string = .x,pattern = &#39; &#39;)[[1]] %&gt;% length)
train.imp$Cabin &lt;- NULL</code></pre>
<p><strong>Ticket:</strong> Lastly, the <code>ticket</code> variable. I’m not sure what to make of it, so I’m keeping it for now, after cleaning it up a bit. A majority (80%) of the rows have unique (one) ticket. 14% rows have a duplicate ticket, perhaps indicating a family. A small number of rows have 3+ duplicates of the tickets.</p>
<pre class="r"><code>train.imp$Ticket %&gt;% table() %&gt;% as.numeric() %&gt;% table()</code></pre>
<pre><code>## .
##   1   2   3   4   5   6   7
## 430  60  15   3   1   1   1</code></pre>
<p>There seems to be a bit of a pattern here. Tickets starting with 1 are mostly 1st class, those starting with 2 are 2nd class, and 3 - 3rd class. But, I feel it’s a very loose association.</p>
<pre class="r"><code>train.imp %&gt;% group_by(Pclass) %&gt;% dplyr::select(Ticket,Pclass) %&gt;% sample_n(5)</code></pre>
<pre><code>## # A tibble: 15 x 2
## # Groups:   Pclass [3]
##    Ticket       Pclass
##    &lt;chr&gt;        &lt;fct&gt;
##  1 112059       P1
##  2 PC 17599     P1
##  3 17474        P1
##  4 113503       P1
##  5 36963        P1
##  6 248731       P2
##  7 237668       P2
##  8 W./C. 14263  P2
##  9 S.O.C. 14879 P2
## 10 29011        P2
## 11 LINE         P3
## 12 350047       P3
## 13 348123       P3
## 14 371110       P3
## 15 2627         P3</code></pre>
<p>What I’m going to do is clean up the columns (remove special characters, spaces etc), then split the <code>Ticket</code> column into four: <code>TicketChar</code>, <code>TicketNum</code>,<code>TicketNumLength</code>, <code>TicketNumStart</code>. (Upon running the script a few times, I’ve decided to get rid of <code>TicketNum</code>, but I’m commenting the code for future ref). The <code>TicketChar</code> variable as this distribution:</p>
<pre class="r"><code>train.imp %&lt;&gt;%
    mutate(
        Ticket = str_to_upper(Ticket) %&gt;%
            str_replace_all(pattern = regex(pattern = &#39;[.\\/]&#39;),replacement = &#39;&#39;),
        TicketNum = str_extract(Ticket,pattern = regex(&#39;([0-9]){3,}&#39;)),
        TicketNumStart = map_int(TicketNum,~as.integer(str_split(.x,pattern = &#39;&#39;,simplify = T)[1])),
        TicketNumLen = map_int(TicketNum,~dim(str_split(.x,pattern = &#39;&#39;,simplify = T))[2]),
        TicketChar = str_extract(Ticket,pattern = regex(&#39;^[a-zA-Z/\\.]+&#39;))
        ) %&gt;%
     mutate(
         TicketChar = map_chr(.x=TicketChar,
                              .f=~str_split(string=.x, pattern = &#39;&#39;,simplify = T)[1])
         ) %&gt;%
    mutate(
        TicketChar = ifelse(is.na(TicketChar),&#39;U&#39;,TicketChar),
        TicketNumStart = ifelse(is.na(TicketNumStart),0,TicketNumStart),
        TicketNumLen = ifelse(is.na(TicketNumLen),0,TicketNumLen),
    ) %&gt;%
    mutate(
        TicketChar = as.factor(TicketChar),
        TicketNumStart = factor(TicketNumStart,levels = seq(0,9,1)),
        TicketNumLen = as.factor(TicketNumLen)
    )
train.imp$Ticket &lt;- NULL
train.imp$TicketNum &lt;- NULL
table(train.imp$TicketChar,dnn =&#39;TicketChar&#39;)</code></pre>
<pre><code>## TicketChar
##   A   C   F   L   P   S   U   W
##  15  36   3   3  52  48 461   7</code></pre>
<pre class="r"><code>table(train.imp$TicketNumLen,dnn=&#39;TicketNumLen&#39;)</code></pre>
<pre><code>## TicketNumLen
##   1   3   4   5   6   7
##   5   5 112 171 298  34</code></pre>
<pre class="r"><code>table(train.imp$TicketNumStart,dnn=&#39;TicketNumStart&#39;)</code></pre>
<pre><code>## TicketNumStart
##   0   1   2   3   4   5   6   7   8   9
##   5 171 150 256  11   7   9  12   1   3</code></pre>
<pre class="r"><code>table(train.imp$NumberofCabins)</code></pre>
<pre><code>##
##   1   2   3   4
## 606  12   5   2</code></pre>
<!-- ## Winzoring Variables -->
<!-- The `fare` variable has one massive outlier. Winzorising this variable using the 95th percentile value as the cutoff. -->
<!-- ```{r} -->
<!-- ggplot(train.imp,aes(x=Fare,fill=Pclass))+geom_histogram()+facet_grid(Pclass~.) -->
<!-- quantile(train.imp$Fare[train.imp$Pclass=='P1'],probs = c(.1,.25,.5,.75,.95)) -->
<!-- train.imp$Fare[train.imp$Fare>232] <- 232 -->
<!-- ``` -->
</div>
<div id="final-data-review" class="section level2">
<h2>Final Data Review</h2>
<p>The dataset is now prepared for modeling. Here’s a quick review of the data so far. 23 variables in total.</p>
<pre class="r"><code>train.imp %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 625
## Variables: 23
## $ Survived       &lt;fct&gt; Dead, Survived, Survived, Dead, Dead, Dead, Sur...
## $ Pclass         &lt;fct&gt; P3, P1, P3, P3, P3, P3, P2, P3, P1, P3, P3, P3,...
## $ Sex            &lt;fct&gt; male, female, female, male, male, male, female,...
## $ Age            &lt;dbl&gt; 22.00000, 38.00000, 26.00000, 35.00000, 41.0000...
## $ SibSp          &lt;int&gt; 1, 1, 0, 0, 0, 3, 1, 1, 0, 1, 0, 4, 0, 1, 0, 0,...
## $ Parch          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 5, 0, 1, 0, 0, 0, 0,...
## $ Fare           &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 8.0500, 8.4583, 21.075...
## $ Embarked       &lt;fct&gt; S, C, S, S, Q, S, C, S, S, S, S, Q, S, S, S, Q,...
## $ title          &lt;fct&gt; Mr, Mrs, Miss, Mr, Mr, Master, Mrs, Miss, Miss,...
## $ child          &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,...
## $ Seniors        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ TotalFam       &lt;dbl&gt; 2, 2, 1, 1, 1, 5, 2, 3, 1, 7, 1, 6, 1, 2, 1, 1,...
## $ LargeFamily    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,...
## $ CabinMissing   &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,...
## $ CabinCode      &lt;fct&gt; U, C, U, U, U, U, U, G, C, U, U, U, U, U, U, U,...
## $ CabinNum       &lt;dbl&gt; 0, 8, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0,...
## $ TopDeck        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ MidDeck        &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,...
## $ LowerDeck      &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,...
## $ NumberofCabins &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
## $ TicketNumStart &lt;fct&gt; 2, 1, 3, 3, 3, 3, 2, 9, 1, 3, 3, 3, 2, 3, 2, 3,...
## $ TicketNumLen   &lt;fct&gt; 5, 5, 7, 6, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6,...
## $ TicketChar     &lt;fct&gt; A, P, S, U, U, U, U, P, U, U, U, U, U, U, U, U,...</code></pre>
<hr />
</div>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p>I’m experimenting with a few modeling techniques, mainly <a href="http://xgboost.readthedocs.io/en/latest/">xgboost</a>, penalized models using <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html">kNN</a>, <a href="https://cran.r-project.org/web/packages/kernlab/index.html">SVM with RBF</a>, <a href="https://cran.r-project.org/web/packages/C50/index.html">C5.0</a> and <a href="https://cran.r-project.org/web/packages/nnet/nnet.pdf">Avg NN</a>. I’ve tried to select models with different structures and different underlying architecture. This gives the highest diversity during the initial modeling approach. I’ve implemented all these models using <a href="http://topepo.github.io/caret/index.html">caret</a> which I find an absolutely indispensible toolkit to prep, build, tune and explore numerous models using very few lines of code.</p>
<p>For all models, I’m using a 5-repeat 10-fold cross validation technique on the training dataset. Tuning parameter searches (aka hypertuning) is performed using the <code>tuneGrid</code> parameter in the <code>train()</code> call. The best model is selected using the AUC of the ROC. Here are the models and a few intermediate results for each model. At the end, I’ve compared the performance of all the models together.</p>
<div id="extreme-gradient-boosting" class="section level2">
<h2>Extreme Gradient Boosting</h2>
<p>The renowned and popular <code>xgboost</code> package via <code>caret</code>. <code>ctrl</code> is the training control object which specifies the resampling method selected. The <code>grid</code> object helps perform hyperparameter tuning.</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     # sampling = &#39;down&#39;,
                     savePredictions = T
                     )
xgbGrid &lt;- expand.grid(
    nrounds=seq(14,24,2),
    max_depth=seq(2,8,2),
    eta=c(0.1, 0.2, 0.3),
    gamma=1,
    colsample_bytree=1,
    min_child_weight=1,
    subsample=1
)
xgbFit &lt;- train(
    Survived~.,
    train.imp,
    method = &#39;xgbTree&#39;,
    trControl = ctrl,
    tuneGrid = xgbGrid
)</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>save(xgbFit,file = &#39;xgbFit&#39;)</code></pre>
<p>What we can see:</p>
<ul>
<li>Run times are extremely fast</li>
<li>The final values used for the model were nrounds = 20, max_depth = 2, eta = 0.2. Perhaps increasing the boosting iterations might help, but I’m stopping here.</li>
<li>Feature importance plots show the derived features ‘Senior’ and ‘TopDeck’ add value.</li>
<li>The PDF of AUC ROC shows a bi-modal distribution (slightly). I noticed this for the other models as well. I wonder what that indicates?</li>
<li>Histograms of the probability plots look somewhat as expected, with a strong peak at &lt;0.2 for ‘Survived’ and a strong peak at &gt;0.8 for ‘Dead’. What I don’t like is how it doesn’t seem to tail off at either end, i.e. ~20% of the data in either category is strongly misclassified.</li>
</ul>
<pre class="r"><code>print(xgbFit,details = F)</code></pre>
<pre><code>## eXtreme Gradient Boosting
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 563, 562, 562, 562, 563, 563, ...
## Resampling results across tuning parameters:
##
##   eta  max_depth  nrounds  ROC        Sens       Spec
##   0.1  2          14       0.8735099  0.7158333  0.8722132
##   0.1  2          16       0.8742786  0.7125000  0.8742915
##   0.1  2          18       0.8750402  0.7166667  0.8763698
##   0.1  2          20       0.8745490  0.7208333  0.8800000
##   0.1  2          22       0.8749362  0.7166667  0.8800135
##   0.1  2          24       0.8748937  0.7216667  0.8815655
##   0.1  4          14       0.8808738  0.7475000  0.8903104
##   0.1  4          16       0.8825793  0.7483333  0.8939271
##   0.1  4          18       0.8826153  0.7500000  0.8955196
##   0.1  4          20       0.8826400  0.7500000  0.8986235
##   0.1  4          22       0.8822548  0.7525000  0.8986505
##   0.1  4          24       0.8814086  0.7458333  0.8975978
##   0.1  6          14       0.8788329  0.7383333  0.8773819
##   0.1  6          16       0.8785664  0.7450000  0.8799460
##   0.1  6          18       0.8797321  0.7500000  0.8799730
##   0.1  6          20       0.8799963  0.7483333  0.8815385
##   0.1  6          22       0.8813048  0.7508333  0.8841296
##   0.1  6          24       0.8809042  0.7483333  0.8851687
##   0.1  8          14       0.8777050  0.7416667  0.8852227
##   0.1  8          16       0.8794582  0.7466667  0.8867746
##   0.1  8          18       0.8814659  0.7516667  0.8847099
##   0.1  8          20       0.8821058  0.7525000  0.8846964
##   0.1  8          22       0.8827522  0.7508333  0.8873144
##   0.1  8          24       0.8821356  0.7508333  0.8888799
##   0.2  2          14       0.8766281  0.7116667  0.8800000
##   0.2  2          16       0.8764803  0.7091667  0.8815520
##   0.2  2          18       0.8780986  0.7100000  0.8810661
##   0.2  2          20       0.8786030  0.7125000  0.8805533
##   0.2  2          22       0.8793396  0.7225000  0.8784480
##   0.2  2          24       0.8792088  0.7208333  0.8784615
##   0.2  4          14       0.8801664  0.7458333  0.8949528
##   0.2  4          16       0.8812129  0.7458333  0.8944669
##   0.2  4          18       0.8828281  0.7483333  0.8949798
##   0.2  4          20       0.8818756  0.7508333  0.8949663
##   0.2  4          22       0.8821876  0.7491667  0.8970715
##   0.2  4          24       0.8829254  0.7491667  0.8965317
##   0.2  6          14       0.8793848  0.7391667  0.8877058
##   0.2  6          16       0.8809202  0.7425000  0.8918893
##   0.2  6          18       0.8803273  0.7416667  0.8918893
##   0.2  6          20       0.8814170  0.7400000  0.8924426
##   0.2  6          22       0.8810554  0.7433333  0.8945074
##   0.2  6          24       0.8813939  0.7425000  0.8934413
##   0.2  8          14       0.8802252  0.7441667  0.8893117
##   0.2  8          16       0.8795254  0.7516667  0.8882996
##   0.2  8          18       0.8792850  0.7541667  0.8877868
##   0.2  8          20       0.8800042  0.7500000  0.8882726
##   0.2  8          22       0.8795249  0.7516667  0.8904049
##   0.2  8          24       0.8799564  0.7566667  0.8893387
##   0.3  2          14       0.8777637  0.7216667  0.8769231
##   0.3  2          16       0.8778874  0.7291667  0.8753576
##   0.3  2          18       0.8765267  0.7358333  0.8763833
##   0.3  2          20       0.8767673  0.7366667  0.8747908
##   0.3  2          22       0.8777370  0.7400000  0.8753171
##   0.3  2          24       0.8788827  0.7483333  0.8784480
##   0.3  4          14       0.8811111  0.7500000  0.8960189
##   0.3  4          16       0.8822934  0.7533333  0.8980972
##   0.3  4          18       0.8825172  0.7483333  0.8965182
##   0.3  4          20       0.8824303  0.7516667  0.8929015
##   0.3  4          22       0.8825464  0.7525000  0.8928745
##   0.3  4          24       0.8826560  0.7516667  0.8934143
##   0.3  6          14       0.8790562  0.7416667  0.8929690
##   0.3  6          16       0.8802716  0.7475000  0.8924696
##   0.3  6          18       0.8801054  0.7516667  0.8924561
##   0.3  6          20       0.8796410  0.7500000  0.8914170
##   0.3  6          22       0.8801425  0.7500000  0.8929960
##   0.3  6          24       0.8803093  0.7500000  0.8935223
##   0.3  8          14       0.8820339  0.7566667  0.8862213
##   0.3  8          16       0.8824120  0.7516667  0.8893657
##   0.3  8          18       0.8828990  0.7541667  0.8888664
##   0.3  8          20       0.8821952  0.7558333  0.8878408
##   0.3  8          22       0.8831267  0.7525000  0.8878677
##   0.3  8          24       0.8830929  0.7533333  0.8868151
##
## Tuning parameter &#39;gamma&#39; was held constant at a value of 1
##  1
## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of
##  1
## Tuning parameter &#39;subsample&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 22, max_depth = 8,
##  eta = 0.3, gamma = 1, colsample_bytree = 1, min_child_weight = 1
##  and subsample = 1.</code></pre>
<pre class="r"><code>plot(xgbFit)
xgb.importance(feature_names = colnames(train.imp),
               model = xgbFit$finalModel) %&gt;%
    xgb.ggplot.importance()
densityplot(xgbFit,pch=&#39;|&#39;)
predict(xgbFit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-27-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-27-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-27-3.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-27-4.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="elastinet" class="section level2">
<h2>Elastinet</h2>
<p>Moving on to a mixture model of ridge &amp; lasso.</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = &#39;down&#39;
                     )
glmnetGrid &lt;- expand.grid(.alpha = c(0,.2,.4,.6,.8,1),
                          .lambda = seq(10^-10,10^-1,0.02))
glmnetFit &lt;- train(
    Survived~.,
    train.imp,
    trControl=ctrl,
    method=&#39;glmnet&#39;,
    tuneGrid = glmnetGrid
)
save(glmnetFit,file = &#39;glmnetFit&#39;)</code></pre>
<p>What we can see:</p>
<ul>
<li>The final values used for the model were alpha = 1 and lambda = 0.02. An alpha of 1 indicates this is a pure lasso model</li>
<li>Like before, I don’t like is how it doesn’t seem to tail off at either end, i.e. ~20% of the data in either category is strongly misclassified.</li>
<li>A quick look at the var importance plot does show that the derived variables of title, ticket char, and ticket number are important. Not bad for some quick feature engineering.</li>
</ul>
<pre class="r"><code>glmnetFit</code></pre>
<pre><code>## glmnet
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 563, 562, 563, 563, 562, 563, ...
## Resampling results across tuning parameters:
##
##   alpha  lambda  ROC        Sens       Spec
##   0.0    1e-10   0.8717698  0.7500000  0.8727935
##   0.0    2e-02   0.8717698  0.7500000  0.8727935
##   0.0    4e-02   0.8719369  0.7441667  0.8774629
##   0.0    6e-02   0.8726847  0.7375000  0.8795412
##   0.0    8e-02   0.8727477  0.7308333  0.8826451
##   0.2    1e-10   0.8626538  0.7575000  0.8463023
##   0.2    2e-02   0.8739066  0.7591667  0.8712551
##   0.2    4e-02   0.8738065  0.7450000  0.8753711
##   0.2    6e-02   0.8736283  0.7308333  0.8795951
##   0.2    8e-02   0.8714555  0.7133333  0.8795816
##   0.4    1e-10   0.8616377  0.7550000  0.8463023
##   0.4    2e-02   0.8740992  0.7583333  0.8675978
##   0.4    4e-02   0.8733361  0.7216667  0.8748718
##   0.4    6e-02   0.8700475  0.7108333  0.8758974
##   0.4    8e-02   0.8636204  0.7016667  0.8779487
##   0.6    1e-10   0.8615314  0.7533333  0.8463023
##   0.6    2e-02   0.8741402  0.7525000  0.8696356
##   0.6    4e-02   0.8725290  0.7108333  0.8769366
##   0.6    6e-02   0.8632900  0.7025000  0.8758839
##   0.6    8e-02   0.8630294  0.7041667  0.8691363
##   0.8    1e-10   0.8615742  0.7533333  0.8463023
##   0.8    2e-02   0.8752567  0.7325000  0.8712146
##   0.8    4e-02   0.8691217  0.7125000  0.8790013
##   0.8    6e-02   0.8645069  0.7058333  0.8665452
##   0.8    8e-02   0.8645504  0.7050000  0.8406208
##   1.0    1e-10   0.8614454  0.7541667  0.8463023
##   1.0    2e-02   0.8758111  0.7241667  0.8753711
##   1.0    4e-02   0.8650436  0.7150000  0.8712011
##   1.0    6e-02   0.8633617  0.7066667  0.8456815
##   1.0    8e-02   0.8545662  0.7008333  0.8328340
##
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 1 and lambda = 0.02.</code></pre>
<pre class="r"><code>glmnPlot &lt;- plot(glmnetFit,
                 plotType = &quot;level&quot;,
                 cuts = 15,
                 scales = list(x = list(rot = 90, cex = .65)))
update(glmnPlot,
       xlab = &quot;Mixing Percentage\nRidge &lt;---------&gt; Lasso&quot;,
       sub = &quot;&quot;,
       main = &quot;Area Under the ROC Curve&quot;,
       ylab = &quot;Amount of Regularization&quot;)
densityplot(glmnetFit,pch=&#39;|&#39;)
plot(varImp(glmnetFit),15,main=&#39;Elastinet Model&#39;)
predict(glmnetFit,type = &#39;prob&#39;) -&gt; train.glmnet.Probs
histogram(~Survived+Dead,train.glmnet.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-29-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-29-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-29-3.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-29-4.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="k-nn" class="section level2">
<h2>k-NN</h2>
<p><code>kNN</code> is expected to be the worst of the lot, and it doesn’t prove us wrong.</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = &#39;down&#39;
                     )
knnGrid &lt;- expand.grid(k=seq(3,23,2))
knnFit &lt;- train(
    Survived~.,
    train.imp,
    method = &#39;knn&#39;,
    trControl = ctrl,
    tuneGrid = knnGrid
)</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>save(knnFit,file = &#39;knnFit&#39;)</code></pre>
<p>The probability value histograms show how large the overlap is between the two classes. Clearly a poor predictor model.</p>
<pre class="r"><code>knnFit</code></pre>
<pre><code>## k-Nearest Neighbors
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 563, 562, 563, 562, 562, 563, ...
## Resampling results across tuning parameters:
##
##   k   ROC        Sens       Spec
##    3  0.7587697  0.5908333  0.8027530
##    5  0.7597366  0.5800000  0.8125236
##    7  0.7614229  0.5816667  0.8083941
##    9  0.7668756  0.5975000  0.8047233
##   11  0.7693668  0.5808333  0.8129825
##   13  0.7703925  0.5550000  0.8311876
##   15  0.7754467  0.5416667  0.8239406
##   17  0.7777497  0.5491667  0.8297166
##   19  0.7746016  0.5408333  0.8380027
##   21  0.7708986  0.5516667  0.8452227
##   23  0.7661907  0.5391667  0.8488529
##
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was k = 17.</code></pre>
<pre class="r"><code>plot(knnFit)
densityplot(knnFit,pch=&#39;|&#39;)
predict(knnFit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-31-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-31-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-31-3.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="svm" class="section level2">
<h2>SVM</h2>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = &#39;down&#39;
                     )
svmFit &lt;- train(
    Survived~.,
    train.imp,
    method = &#39;svmRadial&#39;,
    trControl = ctrl,
    tuneGrid = expand.grid(C=c(0.05,0.1,0.2,0.3), sigma=c(0.001,0.005,0.01,0.015))
)
save(svmFit,file = &#39;svmFit&#39;)</code></pre>
<ul>
<li>The SVM shows moderate performance, but notice many wrongs it gets confidently. Large peaks (~20%) in the probability histograms indicate that the predictions this model gets wrong, it gets wrong very confidently.</li>
</ul>
<pre class="r"><code>svmFit</code></pre>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 562, 562, 562, 563, 562, 563, ...
## Resampling results across tuning parameters:
##
##   C     sigma  ROC        Sens       Spec
##   0.05  0.001  0.8112719  0.7275000  0.7859379
##   0.05  0.005  0.8197903  0.7483333  0.7740081
##   0.05  0.010  0.8271795  0.7541667  0.7766667
##   0.05  0.015  0.8308153  0.7575000  0.7854926
##   0.10  0.001  0.8129605  0.7250000  0.7900675
##   0.10  0.005  0.8197886  0.7483333  0.7734953
##   0.10  0.010  0.8288906  0.7366667  0.7927260
##   0.10  0.015  0.8316341  0.7466667  0.7922267
##   0.20  0.001  0.8167904  0.7233333  0.7931579
##   0.20  0.005  0.8407428  0.6858333  0.8363698
##   0.20  0.010  0.8424190  0.6741667  0.8562618
##   0.20  0.015  0.8422610  0.6808333  0.8505668
##   0.30  0.001  0.8190896  0.7275000  0.7962483
##   0.30  0.005  0.8438647  0.6325000  0.8748178
##   0.30  0.010  0.8467133  0.6333333  0.8837517
##   0.30  0.015  0.8447093  0.6633333  0.8666937
##
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were sigma = 0.01 and C = 0.3.</code></pre>
<pre class="r"><code>plot(svmFit)
densityplot(svmFit,pch=&#39;|&#39;)
predict(svmFit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-33-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-33-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-33-3.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="c5.0" class="section level2">
<h2>C5.0</h2>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     )
c5Fit &lt;- train(
    Survived~.,
    train.imp,
    method = &#39;C5.0&#39;,
    trControl = ctrl,
    control = C50::C5.0Control(earlyStopping = FALSE)
)</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>save(c5Fit,file = &#39;c5Fit&#39;)</code></pre>
<ul>
<li>For the C5.0 model, I’ve kept the grid search space the default offered by <code>caret</code>.</li>
<li>We still see the ‘U’ shape on the probability distributions, although more spread out than what we saw in the SVM model.</li>
</ul>
<pre class="r"><code>c5Fit</code></pre>
<pre><code>## C5.0
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 562, 562, 563, 563, 562, 563, ...
## Resampling results across tuning parameters:
##
##   model  winnow  trials  ROC        Sens       Spec
##   rules  FALSE    1      0.8350037  0.7266667  0.8888664
##   rules  FALSE   10      0.8794467  0.7575000  0.8810256
##   rules  FALSE   20      0.8811004  0.7558333  0.8893117
##   rules   TRUE    1      0.8192578  0.7175000  0.8768556
##   rules   TRUE   10      0.8750616  0.7350000  0.8778947
##   rules   TRUE   20      0.8768250  0.7458333  0.8877868
##   tree   FALSE    1      0.8595808  0.7141667  0.8888529
##   tree   FALSE   10      0.8739195  0.7383333  0.8899055
##   tree   FALSE   20      0.8772911  0.7408333  0.8872740
##   tree    TRUE    1      0.8496924  0.7191667  0.8721457
##   tree    TRUE   10      0.8713200  0.7383333  0.8851687
##   tree    TRUE   20      0.8765562  0.7508333  0.8815115
##
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were trials = 20, model = rules
##  and winnow = FALSE.</code></pre>
<pre class="r"><code>plot(c5Fit)
densityplot(c5Fit,pch=&#39;|&#39;)
predict(c5Fit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-35-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-35-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-35-3.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="averaged-neural-networks" class="section level2">
<h2>Averaged Neural Networks</h2>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     )
avNNetgrid &lt;- expand.grid(size=c(2,4,6,8),decay=c(0.1,0.2),bag=c(T,F))
avNnetFit &lt;- train(Survived~.,
                   train.imp,
                   method = &quot;avNNet&quot;,
                   trControl = ctrl,
                   tuneGrid = avNNetgrid,
                   repeats = 15,
                   trace = FALSE)
save(avNnetFit,file = &#39;avNnetFit&#39;)</code></pre>
<p>Averaged neural networks are something I picked up in the 2018 RStudio conference. It fits multiple neural networks to a problem and returns the average value of all the nnets together. Easy to use, pretty spiffy.</p>
<p>There are three tunable parameters here - # of hidden nodes, bagging and weight decay. Upon playing with them a bit, I settled on what you see below. Perhaps increasing the # of hidden units would have made more of a difference, but I’m happy with this so far.</p>
<pre class="r"><code>avNnetFit</code></pre>
<pre><code>## Model Averaged Neural Network
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 562, 563, 563, 562, 562, 562, ...
## Resampling results across tuning parameters:
##
##   size  decay  bag    ROC        Sens       Spec
##   2     0.1    FALSE  0.8741481  0.7283333  0.8956410
##   2     0.1     TRUE  0.8745440  0.7266667  0.8935088
##   2     0.2    FALSE  0.8747430  0.7283333  0.9002834
##   2     0.2     TRUE  0.8760897  0.7300000  0.8924426
##   4     0.1    FALSE  0.8770547  0.7458333  0.8909582
##   4     0.1     TRUE  0.8781877  0.7383333  0.8898516
##   4     0.2    FALSE  0.8776175  0.7383333  0.8956005
##   4     0.2     TRUE  0.8759807  0.7333333  0.8923887
##   6     0.1    FALSE  0.8765132  0.7400000  0.8898785
##   6     0.1     TRUE  0.8779892  0.7450000  0.8857490
##   6     0.2    FALSE  0.8777845  0.7391667  0.8934548
##   6     0.2     TRUE  0.8765373  0.7391667  0.8929150
##   8     0.1    FALSE  0.8754892  0.7416667  0.8851957
##   8     0.1     TRUE  0.8747813  0.7391667  0.8847233
##   8     0.2    FALSE  0.8773043  0.7366667  0.8945614
##   8     0.2     TRUE  0.8783103  0.7350000  0.8935358
##
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were size = 8, decay = 0.2 and bag
##  = TRUE.</code></pre>
<pre class="r"><code>plot(avNnetFit)
densityplot(avNnetFit,pch=&#39;|&#39;)
predict(avNnetFit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-37-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-37-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-37-3.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="conditional-inference-random-forests" class="section level2">
<h2>Conditional Inference Random Forests</h2>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     )
ciRFgrid &lt;- expand.grid(mtry = c(20,25,30,35,40))
ciRFFit &lt;- train(Survived~.,
                   train.imp,
                   method = &quot;cforest&quot;,
                   trControl = ctrl,
                 tuneGrid = ciRFgrid
                 ) </code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>save(ciRFFit,file = &#39;ciRFFit&#39;)</code></pre>
<p>This is another technology I picked up from converstaions at RStudio conference. I believe it’s referenced in the APM book as well. Some information about them is <a href="https://hsequantling.wikispaces.com/file/view/LevshinaCh14CARTRandomForest.pdf">here</a>. To be honest, I haven’t spent any time understanding exactly how these work, so to me they are black box(y) at the moment.</p>
<pre class="r"><code>ciRFFit</code></pre>
<pre><code>## Conditional Inference Random Forest
##
## 625 samples
##  22 predictor
##   2 classes: &#39;Survived&#39;, &#39;Dead&#39;
##
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times)
## Summary of sample sizes: 563, 563, 562, 563, 563, 562, ...
## Resampling results across tuning parameters:
##
##   mtry  ROC        Sens       Spec
##   20    0.8764103  0.7141667  0.8784211
##   25    0.8784511  0.7275000  0.8805533
##   30    0.8801816  0.7375000  0.8815924
##   35    0.8804375  0.7208333  0.8826451
##   40    0.8810473  0.7300000  0.8862753
##
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 40.</code></pre>
<pre class="r"><code>plot(ciRFFit)
densityplot(ciRFFit,pch=&#39;|&#39;)
predict(ciRFFit,type = &#39;prob&#39;) -&gt; train.Probs
histogram(~Survived+Dead,train.Probs)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-39-1.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-39-2.png" width="60%" style="display: block; margin: auto;" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-39-3.png" width="60%" style="display: block; margin: auto;" /></p>
<hr />
</div>
</div>
<div id="compare-models" class="section level1">
<h1>Compare models</h1>
<p>Now the fun part - pitting these models against each other to figure out who’s the strongest. The beauty of using <code>caret</code> to analyze all the models with the same resampling strategy is that it allows one to use the <code>resamples()</code> function to quickly extract all the information in the fits and plot out key performance parameters like ROC, sensitivity and specificity.</p>
<pre class="r"><code>re &lt;-
    resamples(x = list(
    xgb = xgbFit,
    knn = knnFit,
    elastinet = glmnetFit,
    C50 = c5Fit,
    svm = svmFit,
    avgNNet = avNnetFit,
    ciRF = ciRFFit
    ))</code></pre>
<p>This plot shows the point estimates (mean and CI of mean) of all three metrics, as evaluated on each repeated cross-validation on all models run per model-type. All the top 4 models are really alike in AUC ROC, with xgb and C50 taking a slight lead in sensitivity.</p>
<pre class="r"><code>dotplot(re)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-41-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Another way to visualize the data is to plot all the results of each model-type for all it’s cross-validated results on a boxplot. This shows us how much variation we might expect from the model for a true unknown test set. Here, elastinet seems to have the tightest IQR for AUC ROC as well as the tightest IQR for sensitivity. It also shows no outliers.</p>
<pre class="r"><code>bwplot(re)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-42-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>One challenge in the plots above is drawing inferences from the plots above. Too many of the points above are really close to each other. It’s hard to tell if one model is any better than another model.</p>
<p>This is where <code>diff()</code> is a useful function. Basically, for each metric, all pair-wise differences are evaluated. The plot makes this quite intuitive. If the confidence level on the differences includes zero, the models are equal to each other. Similar to assessing a null hypothesis of (beta = 0) in a linear regression analysis.</p>
<p>Now, we can clearly see that if xgb is compared to it’s next 3 rivals, there isn’t really a statistically significant difference between the AUC ROC performance of that model against the others based on the cross-validations alone. No reason to pick over the others.</p>
<pre class="r"><code>difValues &lt;- diff(re)
dotplot(difValues)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<hr />
</div>
<div id="test-set-evaluation" class="section level1">
<h1>Test Set Evaluation</h1>
<p>Of course, any data scientist worth his/her salt knows better than to use training set performances to fully evaluate and select the final model.</p>
<p>So, first re-trace all the steps and prepare the test dataset.</p>
<div id="create-test-set" class="section level2">
<h2>Create test set</h2>
<pre class="r"><code>test.imp &lt;- test.raw

#Embarked
test.imp$Embarked[is.na(test.imp$Embarked)]=&#39;S&#39;

#Title
test.raw$title &lt;- str_extract(pattern = &#39;[a-zA-Z]+(?=\\.)&#39;,string = test.raw$Name)
test.imp$title &lt;- as.character(test.raw$title)
test.imp$title[test.imp$title %in% c(&#39;Capt&#39;,&#39;Col&#39;,&#39;Major&#39;)] &lt;- &#39;Officer&#39;
test.imp$title[test.imp$title %in% c(&#39;Don&#39;,&#39;Dr&#39;,&#39;Rev&#39;,&#39;Sir&#39;,&#39;Jonkheer&#39;,&#39;Countess&#39;,&#39;Lady&#39;,&#39;Dona&#39;)] &lt;- &#39;Royalty&#39;
test.imp$title[test.imp$title %in% c(&#39;Mrs&#39;,&#39;Mme&#39;)] &lt;- &#39;Mrs&#39;
test.imp$title[test.imp$title %in% c(&#39;Ms&#39;,&#39;Mlle&#39;)] &lt;- &#39;Miss&#39;
test.imp$title &lt;- factor(test.imp$title,levels = levels(train.imp$title))

#Missing age
missing.age &lt;- test.imp %&gt;% filter(is.na(Age)) %&gt;% dplyr::select(-Cabin,-Ticket,-Name)
age.predicted &lt;- predict(rpartFit_ageimputation, newdata = missing.age)
test.imp$Age[is.na(test.imp$Age)] &lt;- age.predicted

#Child
test.imp$child &lt;- 0
test.imp$child[test.imp$Age&lt;18] &lt;- 1

#Young/old
test.imp$Seniors &lt;- ifelse(test.imp$Age&gt;60,1,0)

#Family Related
test.imp$TotalFam &lt;- test.imp$SibSp + test.imp$Parch + 1
test.imp$LargeFamily &lt;- ifelse(test.imp$TotalFam&gt;4,1,0)
test.imp$Name &lt;- NULL

#Cabin &amp; Deck
test.imp$CabinMissing &lt;- as.numeric(is.na(test.raw$Cabin))
test.imp$CabinCode &lt;- map_chr(test.raw$Cabin,~str_split(string = .x,pattern = &#39;&#39;)[[1]][1])
test.imp$CabinCode[is.na(test.imp$CabinCode)] &lt;- &#39;U&#39;
test.imp$CabinCode &lt;- factor(test.imp$CabinCode,levels = levels(train.imp$CabinCode))
test.imp$CabinNum &lt;- as.numeric(map_chr(test.raw$Cabin,~str_split(string = .x,pattern = &#39;[a-zA-Z]&#39;)[[1]][2]))
test.imp$CabinNum &lt;- map_int(test.imp$CabinNum, ~as.integer(str_split(.x,pattern = &#39;&#39;,simplify = T)[1][1]))
test.imp$CabinNum[is.na(test.imp$CabinNum)] &lt;- 0

test.imp$TopDeck &lt;- ifelse(test.imp$CabinCode %in% c(&#39;A&#39;,&#39;B&#39;),1,0)
test.imp$MidDeck &lt;- ifelse(test.imp$CabinCode %in% c(&#39;C&#39;,&#39;D&#39;),1,0)
test.imp$LowerDeck &lt;- ifelse(test.imp$TopDeck==0 &amp; test.imp$MidDeck==0 ,1,0)

test.imp$NumberofCabins &lt;- map_int(test.raw$Cabin,~str_split(string = .x,pattern = &#39; &#39;)[[1]] %&gt;% length)
test.imp$Cabin &lt;- NULL

# Ticket
test.imp %&lt;&gt;%
    mutate(
      Ticket = str_to_upper(Ticket) %&gt;%
          str_replace_all(pattern = regex(pattern = &#39;[.\\/]&#39;),replacement = &#39;&#39;),
      TicketNum = str_extract(Ticket,pattern = regex(&#39;([0-9]){3,}&#39;)),
      TicketNumStart = map_int(TicketNum,~as.integer(str_split(.x,pattern = &#39;&#39;,simplify = T)[1])),
      TicketNumLen = map_int(TicketNum,~dim(str_split(.x,pattern = &#39;&#39;,simplify = T))[2]),
      TicketChar = str_extract(Ticket,pattern = regex(&#39;^[a-zA-Z/\\.]+&#39;))
      ) %&gt;%
    mutate(
        TicketChar = map_chr(.x=TicketChar,
                             .f=~str_split(string=.x, pattern = &#39;&#39;,simplify = T)[1])
        ) %&gt;%
    mutate(
      TicketChar = ifelse(is.na(TicketChar),&#39;U&#39;,TicketChar),
      TicketNumStart = ifelse(is.na(TicketNumStart),0,TicketNumStart),
      TicketNumLen = ifelse(is.na(TicketNumLen),0,TicketNumLen),
    ) %&gt;%
    mutate(
        TicketChar = as.factor(TicketChar),
        TicketNumStart = factor(TicketNumStart,levels = seq(0,9,1)),
        TicketNumLen = as.factor(TicketNumLen)
    )
test.imp$Ticket &lt;- NULL
test.imp$TicketNum &lt;- NULL</code></pre>
</div>
<div id="predict-test-results" class="section level2">
<h2>Predict test results</h2>
<p>Now, predict the results using all the modelFits created in the previous section.</p>
<pre class="r"><code>elastinetPred   &lt;- predict(object = glmnetFit, newdata = test.imp)
xgbPred         &lt;- predict(object = xgbFit,    newdata = test.imp)
c5Pred          &lt;- predict(object = c5Fit,     newdata = test.imp)
knnPred         &lt;- predict(object = knnFit,    newdata = test.imp)
svmPred         &lt;- predict(object = svmFit,    newdata = test.imp)
avNNPred        &lt;- predict(object = avNnetFit, newdata = test.imp)
ciRFPred        &lt;- predict(object = ciRFFit,   newdata = test.imp)</code></pre>
<p>Predictions are done; now let’s investigate the results. Extracting confusion matrices is made simple using <code>caret</code>. Comparing all the results is made easy if we leverage the tools in <code>purrr</code>.</p>
<pre class="r"><code>xtab &lt;- table(xgbPred,test.imp$Survived)
xgbCM &lt;- caret::confusionMatrix(xtab)

xtab &lt;- table(elastinetPred,test.imp$Survived)
elastinetCM &lt;- caret::confusionMatrix(xtab)

xtab &lt;- table(c5Pred,test.imp$Survived)
c5CM &lt;- caret::confusionMatrix(xtab)

xtab &lt;- table(knnPred,test.imp$Survived)
knnCM &lt;-caret::confusionMatrix(xtab)

xtab &lt;- table(svmPred,test.imp$Survived)
svmCM &lt;-caret::confusionMatrix(xtab)

xtab &lt;- table(avNNPred,test.imp$Survived)
avNNCM &lt;-caret::confusionMatrix(xtab)

xtab &lt;- table(ciRFPred,test.imp$Survived)
ciRFCM &lt;-caret::confusionMatrix(xtab)

CM_list &lt;- list(xgbCM, elastinetCM, c5CM, knnCM, svmCM, avNNCM, ciRFCM)

compiled_results &lt;- tibble(
    models = c(&#39;xgb&#39;,&#39;elastinet&#39;,&#39;C5.0&#39;,&#39;knn&#39;,&#39;svm&#39;, &#39;avgNN&#39;, &#39;ciRF&#39;),
    accuracy = map_dbl(CM_list,~.x$overall[1]),
    kappa = map_dbl(CM_list,~.x$overall[2]),
    sensitivity = map_dbl(CM_list,~.x$byClass[1]),
    specificity = map_dbl(CM_list,~.x$byClass[2]),
    F1 = map_dbl(CM_list,~.x$byClass[7])
)
compiled_results %&gt;% arrange(accuracy,kappa)</code></pre>
<pre><code>## # A tibble: 7 x 6
##   models    accuracy kappa sensitivity specificity    F1
##   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;
## 1 knn          0.737 0.413       0.520       0.872 0.602
## 2 C5.0         0.808 0.581       0.667       0.896 0.727
## 3 ciRF         0.808 0.585       0.686       0.884 0.733
## 4 xgb          0.816 0.598       0.676       0.902 0.738
## 5 avgNN        0.823 0.616       0.696       0.902 0.751
## 6 svm          0.846 0.665       0.725       0.921 0.783
## 7 elastinet    0.850 0.675       0.745       0.915 0.792</code></pre>
<p>Of course, the easiest way to look at the results is visually. Elastinet and svm are the clear winners here, with 84%+ accuracy. The kappa and F1 scores together also show them are clear winners. This is what’s interesting about using the test datasets. The cross-validation training results had xgb, C50, ciRF, avgNNet and elastinet all almost equally as good. However, the test results show a very clear winner indeed.</p>
<pre class="r"><code>dotplot(reorder(models,accuracy)~accuracy,compiled_results, main = &#39;Accuracy (Test Set Performance)&#39;)
ggplot(compiled_results, aes(F1, accuracy)) +
    geom_point(color = &#39;blue&#39;,shape=1) +
    geom_text_repel(aes(label = models),
                    box.padding=unit(1,&#39;lines&#39;),
                    max.iter=1e2,segment.size=.3,
                    force=1) +
    theme_bw()+
    labs(x=&#39;F1&#39;,y=&#39;kappa&#39;, title=&#39;Kappa vs F1 (Test Set Performance)&#39;)</code></pre>
<p><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-47-1.png" width="50%" /><img src="/post/2018-01-24-yet-another-titanic-solve_files/figure-html/unnamed-chunk-47-2.png" width="50%" /></p>
</div>
</div>
<div id="kaggle-performance" class="section level1">
<h1>Kaggle Performance</h1>
<p>The top few models take me to a 72% - 76% score on Kaggle. It isn’t the top model there of course, but not bad for a 1st Kaggle competiton attempt.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I think this approach depends on the academic background and the industry of the analyst. Prof Srinivasan, and my mentor at work both have strong statistical academic backgrounds, and both believe in thorough EDA of the data. I’ve also noticed this approach from individuals in the banking &amp; insurance industry - perhaps due to regulatory requirements. On the other hand, folks trained in computer science and algorithmic data science tend to underplay the importance of thorough EDA.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>To iterate variable names in ggplot, use <code>ggplot(...)+aes_string(...)</code> in place of <code>ggplot(...,aes(...))</code>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Read more about beanplots here: <a href="https://cran.r-project.org/web/packages/beanplot/vignettes/beanplot.pdf" class="uri">https://cran.r-project.org/web/packages/beanplot/vignettes/beanplot.pdf</a><a href="#fnref3">↩</a></p></li>
</ol>
</div>
