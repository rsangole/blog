[
  {
    "objectID": "astronomy.html",
    "href": "astronomy.html",
    "title": "yHat",
    "section": "",
    "text": "Int’l Observe the Moon Night\n\n\n\n\n\nGlobal Observe the Moon Night hosted by NASA\n\n\n\n\n\n\nOct 6, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nCalstar 2022\n\n\n\n\n\nThe 24th Calstar Party at Lake San Antonio.\n\n\n\n\n\n\nSep 24, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nAL Globular Cluster Observing Challenge\n\n\n\n\n\nJuly through September is a prime time to observe globular clusters. The constellations of Ophiuchus (17 clusters) and Sagittarius (21 clusters) provide 38 globular clusters brighter than magnitude 10.\n\n\n\n\n\n\nJul 1, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "yHat",
    "section": "",
    "text": "Flexmix\n\n\nMixture Modeling\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2017\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFactor Analysis\n\n\nMultivariate\n\n\n\n\nHere, I replicate a paper end to end - survey design, data collection and analysis - by a professor from Brown Univ. The paper applies factor analysis to extract insights on respondant’s personality traits.\n\n\n\n\n\n\nSep 3, 2016\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "yHat",
    "section": "",
    "text": "Hello, my name is Rahul. This is a blog where I write about R and data science.\nI’m a data scientist at Apple where my work focuses on time-series, R in production and Shiny dashboarding. Before Apple, I worked as a data science manager at Cummins where I focused on developing time-series anomaly detection algorithms for engine health monitoring. In my former career, I worked as a mechanical engineer with a focus on computational stress, vibration and fatigue modeling.\nI have a Master’s degree in Predictive Analytics from Northwestern University and another in Mechanical Engineering from the University of Michigan."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "yHat",
    "section": "",
    "text": "Benchmarking\n\n\n\nWhich of the popular data read write methods is faster? Let’s find out.\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nPostgres\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nPostgres\n\n\n\nHow to setup a Docker based workflow for development in RStudio with a local Postgres server, also hosted in Docker\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\nQuick visualizations in command line using {txtplot}\n\n\n\nJul 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\nCorrelation plot for Kepler’s Planets, for day 13 of the 2021 30-day-chart-challenge\n\n\n\nApr 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nIs {fastDummies} any better than {stats} to create dummy variables? Let’s find out.\n\n\n\nDec 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nNotes from the M5 Forecasting Competition keynote speakers.\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nProgramming Practices\n\n\n\nA few ways I ensure my work is reproducible in R\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Practices\n\n\n\nA quick introduction to tryCatch below, followed by three use-cases I use on a regular basis.\n\n\n\nDec 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\nProgramming Practices\n\n\n\nI have 6 methods compete against each other to figure out the fastest way to convert characters to date-time for large datasets.\n\n\n\nApr 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\nA list of Data Science books I reference\n\n\n\nFeb 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nVisualization\n\n\n\nLinear Discriminant Analysis visualized using Shiny\n\n\n\nJan 27, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nHow do the four popular methods of creating dummy variables perform on large datasets? Let’s find out!\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Practices\n\n\n\nYou’ll learn how to use purrr, caret and list-cols to quickly create hundreds of dataset + model combinations, store data & model objects neatly in one tibble, and…\n\n\n\nSep 17, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "astronomy/2022-10-06_intl-moon-observing-day/post.html",
    "href": "astronomy/2022-10-06_intl-moon-observing-day/post.html",
    "title": "Int’l Observe the Moon Night",
    "section": "",
    "text": "I conducted an event with Rajah in the elementary school for the 1st grades. We had ~80 folks show up - excited little ones and parents alike. Rajah and I manned three scopes in all, an 8 inch dob, a 9.25 SCT and a 80mm refractor. Plenty of oohs and aahs as first timers saw the moon, Saturn and Jupiter through these scopes. The seeing was quite excellent, with bands on the surface of Jupyter quite clear even for amateur observers to see. By the time we called it a night, by 9p, it was quite dewy; scopes and equipment were soaking wet.\nThe kids loved the introduction to the night skies; perhaps this will be one of many events at the school."
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "",
    "text": "This page replicates the codes written by Grun & Leish (2007) in ‘FlexMix: An R package for finite mixture modelling’, University of Wollongong, Australia. My intent here was to learn the flexmix package by replicating the results by the authors."
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Quick EDA",
    "text": "Quick EDA\nQuick look at the data itself. The dataframe consists of 2 elements - frequency (numeric vector), and the incidence matrix. There are total of 484 observations.\n\ndata(\"whiskey\")\ndf <- whiskey\nset.seed(1802)\nstr(df)\n\n'data.frame':   484 obs. of  2 variables:\n $ Freq     : int  1 1 10 14 10 23 9 8 1 12 ...\n $ Incidence: num [1:484, 1:21] 1 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:484] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:21] \"Singleton\" \"Knockando\" \"White Horse\" \"Scoresby Rare\" ...\n\n\nThe column names of the df$Incidence matrix are the brands of whiskey.\n\ncolnames(df$Incidence)\n\n [1] \"Singleton\"                  \"Knockando\"                 \n [3] \"White Horse\"                \"Scoresby Rare\"             \n [5] \"Ushers\"                     \"Macallan\"                  \n [7] \"Grant's\"                    \"Passport\"                  \n [9] \"Black & White\"              \"Clan MacGregor\"            \n[11] \"Ballantine\"                 \"Pinch (Haig)\"              \n[13] \"Other brands\"               \"Cutty Sark\"                \n[15] \"Glenfiddich\"                \"Glenlivet\"                 \n[17] \"J&B\"                        \"Dewar's White Label\"       \n[19] \"Johnnie Walker Black Label\" \"Johnnie Walker Red Label\"  \n[21] \"Chivas Regal\"              \n\n\nThe incidence matrix shows a relationship between two classes of variables - in this case: freqencies of the brand of whiskey in the past year, and the brand of whiskey itself. Quick look at a portion of the matrix:\n\ndf$Incidence[sample(x = 1:484,size = 10),sample(1:21,3)]\n\n    Macallan Cutty Sark Black & White\n119        0          1             0\n8          0          0             0\n245        0          0             1\n347        0          0             0\n26         0          1             0\n221        1          0             0\n1          0          0             0\n178        1          0             0\n411        0          0             0\n224        0          1             0\n\n\nThe popularity of the whiskeys can be seen here. Chivas Regal seems to be a favourite, which puts my personal preference in line with a larger population :)\n\nc <- colSums(df$Incidence)\nd1 <- data.frame(Brand=names(c),counts=c,row.names = NULL)\nd1 <- d1 %>% left_join(whiskey_brands) %>% arrange(-counts)\nggplot(d1,aes(reorder(Brand,counts),counts,fill=Type))+geom_bar(stat='identity')+coord_flip()+labs(y='Counts',x='Whiskey Brand')"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Model building",
    "text": "Model building\nThe first model in the paper is a stepped Flexmix model, specific for binary variables using the FLXMcvbinary() model. Since the objective is to cluster the model based on the Incidence counts and Frequencies, the formula used is Incidence ~ 1. The frequencies themselves are input as weights in the formula.\n\nwh_mix <- stepFlexmix(Incidence ~ 1,\n                      weights = ~ Freq, \n                      data = df,\n                      model = FLXMCmvbinary(truncated = TRUE),\n                      control = list(minprior = 0.005),\n                      k=1:7,\n                      nrep=5)\n\n1 : * * * * *\n2 : * * * * *\n3 : * * * * *\n4 : * * * * *\n5 : * * * * *\n6 : * * * * *\n7 : * * * * *\n\nsummary(wh_mix)\n\n     Length       Class        Mode \n          1 stepFlexmix          S4 \n\n\nA top model can be selecting using BIC or AIC criteria. The BIC criteria selects a model with 5 clusters.\n\nplot(BIC(wh_mix),type='b',ylab='BIC')\npoints(x = which.min(BIC(wh_mix)),min(BIC(wh_mix)),col='red',pch=20)\n\n\n\nwh_best <- getModel(wh_mix,'BIC')\nprint(wh_best)\n\n\nCall:\nstepFlexmix(Incidence ~ 1, weights = ~Freq, data = df, model = FLXMCmvbinary(truncated = TRUE), \n    control = list(minprior = 0.005), k = 5, nrep = 5)\n\nCluster sizes:\n  1   2   3   4   5 \n 24 262 794 161 977 \n\nconvergence after 127 iterations\n\n\nThe proportions of the observations in each cluster are shown here:\n\nround(prop.table(table(wh_best@cluster)),2)\n\n\n   1    2    3    4    5 \n0.04 0.25 0.19 0.23 0.29 \n\n\nThe parameter estimates plotted for model with k=5 is shown below graphically. Component 3 (4% of households) contain the largest number of different brands. Component 1 (25% of households) seen to prefer single malt whiskeys. Component 4 (23% of households) are across the board with Brands, but perhaps show lesser of an interest in single malts, just like Component 5 (29% of the households).\n\n# wh_best.prior <- prior(wh_best)\nwh_best.param <- parameters(wh_best)\nwh_best.param <- data.frame(Brand=stringr::str_replace(rownames(wh_best.param),pattern = 'center.',replacement = ''),\nwh_best.param,row.names = NULL) \nwh_best.param <- wh_best.param %>% gather(Components,Value,Comp.1:Comp.5)\nwh_best.param <- wh_best.param %>% left_join(y = whiskey_brands,by = 'Brand')\nggplot(wh_best.param,aes(y=Value,x=Brand,fill=Type))+\n    geom_bar(stat='identity')+\n    coord_flip()+\n    facet_grid(.~Components)"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda-1",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda-1",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Quick EDA",
    "text": "Quick EDA\nThe dependant variable here is Patents. Independant variable is lgRD which is the log of R&D spending. The objective in this exercise is to try and find how many may clusters exist within this bi-variate dataset. When I started this exercise, it seemed quite moot to me, since visually, I couldn’t really tell any distict clusters. But, the results show otherwise.\n\ndata(\"patent\")\ndf_patent <- tbl_df(patent)\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nPlease use `tibble::as_tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\ndf_patent\n\n# A tibble: 70 × 4\n   Company                                    Patents    RDS   lgRD\n   <chr>                                        <int>  <dbl>  <dbl>\n 1 \"ABBOTT LABORATORIES                     \"      42 0.0549  4.09 \n 2 \"AFFILIATED HOSPITAL PRDS                \"       1 0.0032 -2.08 \n 3 \"ALBERTO-CULVER CO                       \"       3 0.0078  0.119\n 4 \"ALCON LABORATORIES                      \"       2 0.0803  1.88 \n 5 \"ALLERGAN PHARMACEUTICALS INC            \"       3 0.0686  1.10 \n 6 \"ALZA CORP-CL A                          \"      40 3.33    2.08 \n 7 \"AMERICAN HOME PRODUCTS CORP             \"      60 0.0243  4.10 \n 8 \"AMERICAN HOSPITAL SUPPLY                \"      30 0.0128  2.83 \n 9 \"AMERICAN STERILIZER CO                  \"       7 0.0252  1.39 \n10 \"AVON PRODUCTS                           \"       3 0.0094  2.60 \n# … with 60 more rows\n\nplot(Patents~lgRD,df_patent)"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building-1",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building-1",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Model Building",
    "text": "Model Building\nThe paper mentions that Wang et al. (1998) chose a finite mixture of three Poisson regression models to represent the data. The FLXMRglm() is used for the Poisson model with a concomitant variable modeled using FLXPmultinom().\n\npat_mix <- flexmix(Patents ~ lgRD, k = 3, data = df_patent, model = FLXMRglm(family = \"poisson\"), concomitant = FLXPmultinom(~RDS))\npat_mix\n\n\nCall:\nflexmix(formula = Patents ~ lgRD, data = df_patent, k = 3, model = FLXMRglm(family = \"poisson\"), \n    concomitant = FLXPmultinom(~RDS))\n\nCluster sizes:\n 1  2  3 \n12 13 45 \n\nconvergence after 33 iterations\n\n\nThe clusters obtained from the analysis are given by a cluster() function.\n\nclusters(pat_mix)\n\n [1] 1 3 3 1 3 2 3 3 3 1 3 2 3 3 1 3 1 2 3 3 1 1 2 3 2 3 3 3 3 1 3 3 3 3 2 3 3 1\n[39] 2 3 3 3 3 2 3 3 1 3 2 3 3 1 3 3 3 3 3 1 3 3 3 3 2 3 2 2 3 3 2 3"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#results",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#results",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Results",
    "text": "Results\nThe data is replotted but with colors for the clusters and additional splines. As we can see, the model beautifully models three lines through three clusters in the data.\n\nComponents <- factor(clusters(pat_mix))\nxyplot(Patents~lgRD,groups = Components,df_patent,type=c('p','spline'))"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#further-investigation",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#further-investigation",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Further investigation",
    "text": "Further investigation\nThe flexmix package has a function to plot rootograms of the posterior probabilities of observations. Observations where the a-posteriori probability is large for component #1 and #3 are indicated. As we can see where component #1 has highest probabilities indicated in the 1st bucket, they are lowest in #2 and #3 buckets.\n\nplot(pat_mix,mark=1)\n\n\n\nplot(pat_mix,mark=3)\n\n\n\n\nA summary of the mixture model results show the estimated priors, number of observations within each cluster (size), number of observations with p>10^-4 (post>0), and a ratio of the two. The rations of 0.58, 0.42 and 0.18 indicate big overlaps of the clusters. This can also be observed by the large portion of values in the mid-section of the rootogram above.\n\nsummary(pat_mix)\n\n\nCall:\nflexmix(formula = Patents ~ lgRD, data = df_patent, k = 3, model = FLXMRglm(family = \"poisson\"), \n    concomitant = FLXPmultinom(~RDS))\n\n       prior size post>0 ratio\nComp.1 0.201   12     48 0.250\nComp.2 0.184   13     47 0.277\nComp.3 0.615   45     63 0.714\n\n'log Lik.' -197.6752 (df=10)\nAIC: 415.3505   BIC: 437.8354 \n\n\nTests of significance of the coefficients are obtained by the refit(). In each cluster the intercept and lgRD are both statistically significant at the 0.05 level. The black bars in the plot are 95% CI over the point estimates.\n\nrm <- refit(pat_mix)\nsummary(rm)\n\n$Comp.1\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) -2.63638    0.62672 -4.2067 2.592e-05 ***\nlgRD         1.58644    0.13393 11.8454 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.2\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) 1.962173   0.176485  11.118 < 2.2e-16 ***\nlgRD        0.671908   0.045639  14.722 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.3\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) 0.508194   0.160927  3.1579  0.001589 ** \nlgRD        0.879702   0.040249 21.8564 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(rm,bycluster=F)\n\n\n\n\nThe authors note that “estimates vary between all components even though the co- efficients for lgRD are similar for the first and third component”."
  }
]