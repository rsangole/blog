[
  {
    "objectID": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html",
    "href": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html",
    "title": "Performance Benchmarking Data Read Write",
    "section": "",
    "text": "In a recent conversation, I was asked how to speed up execution of scripts dealing with computation on big data sets (biggish i.e. in-memory). The scripts had some data read and write features with some ETL sandwiched in between. Upon reviewing the code base, I found some immediate improvements by moving a bunch of ETL code from {dplyr} to {data.table}, which has been my defacto go to for large data sets.\nOn the data read/write side of the equation, I used to be an ardent {qs} user with intermittent forays into {data.table::fread/fwrite} functions. However, recently, I’ve switched majority of my work to {arrow} which has proven itself a strong ally in the war against slow data transaction times.\nThe question remains - which one works better? Particularly, which one works better on the machines I’m using as well as the server I have access to? My configuration has changed from my last performance review posting. I’m now running:\n\nMacBook Pro, running MacOS 12.5\n2.4 GHz 8-Core Intel Core i9\n64 GB 2667 MHz DDR4\nR version 4.2.0 on x86_64-apple-darwin17.0 (64-bit)"
  },
  {
    "objectID": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#data",
    "href": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#data",
    "title": "Performance Benchmarking Data Read Write",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n \n  \n    cat_A \n    cat_B \n    date \n    num_1 \n    num_2 \n    num_3 \n    num_4 \n    chr_1 \n    chr_2 \n    chr_3 \n    chr_4 \n  \n \n\n  \n    Alabama \n    B \n    2023-03-01 \n    0.67 \n    0.13 \n    0.83 \n    0.04 \n    h \n    p \n    i \n    u \n  \n  \n    Alabama \n    B \n    2022-10-18 \n    0.52 \n    0.63 \n    0.29 \n    0.14 \n    s \n    o \n    h \n    a \n  \n  \n    Alaska \n    A \n    2023-03-20 \n    0.42 \n    0.24 \n    0.45 \n    0.21 \n    n \n    g \n    l \n    n \n  \n  \n    Arkansas \n    B \n    2023-08-07 \n    0.60 \n    0.12 \n    0.46 \n    1.00 \n    c \n    p \n    r \n    b \n  \n\n\n\n\nI created some fake data with categoricals, numerics and date-time vars. I’ve used ‘long’ data here, since it represented the use case I typically have, although I wonder if the results would change if the data were wider. After a few iterations, I settled on number of rows: 100k, 1M, and 5M. The results for number of rows = 1M+ did not change, to be honest."
  },
  {
    "objectID": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#approaches",
    "href": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#approaches",
    "title": "Performance Benchmarking Data Read Write",
    "section": "Approaches",
    "text": "Approaches\nI tested the packages I tend to use the most, along with a new one vroom and the traditional RDS format from R. All in all, I tested:\n\ndata.table - both the traditional CSV, and yaml = TRUE formulations\narrow - two configurations: parquet and arrow_csv\nqs\nvroom\nRDS\nfeather\nfst\n\nEach approach is shown by a green square in the middle here. Each square represents a read and write using {microbenchmark} to measure the execution times. In total, this pipeline runs {10 read/write approaches x 3 data sets x 10 repeats} for a total of 300 runs."
  },
  {
    "objectID": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#speeds",
    "href": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#speeds",
    "title": "Performance Benchmarking Data Read Write",
    "section": "Speeds",
    "text": "Speeds\nThe plots below show the absolute read & write speeds on the left (measured in seconds), with relative speed decrease as compared to the best performer on the right.\ntldr\n\narrow-parquet are the clear winner when it comes to read-speed. The next best is vroom which is 2x slower. Almost every other option is 5x-7x slower than arrow.\nqs performs the best for write-speeds. arrow and data.table are the next best at 2-3x slower speeds than qs.\nFor data.table , the yaml = TRUE setting doesn’t make any difference to read-speeds. This one surprised me, since I thought it would speed it up at least a bit.\nrds is the slowest, unsurprisingly, at 20-30x slower than arrow"
  },
  {
    "objectID": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#file-sizes",
    "href": "posts/2022-09-14_data-read-write-performance/data-read-write-perf.html#file-sizes",
    "title": "Performance Benchmarking Data Read Write",
    "section": "File Sizes",
    "text": "File Sizes\nThis one is a doozey! For large 5M row data sets, there’s a marked level shift between the compressed and uncompressed file formats!\n\n\n\n\n\nI’m curious to see if this result varies for wider data sets, or data sets with more/less characters and numerics."
  },
  {
    "objectID": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html",
    "href": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html",
    "title": "Pur(r)ify Your Carets",
    "section": "",
    "text": "I want to write a quick blogpost on two phenomenal pieces of code written by Kuhn et al, and Wickham et al, namely - purrr, caret. These play so well with the tidyverse that they have become an indispensible part of my repertoire.\nIn any datascience project, I want to investigate the effect of various combinations of:\n\nVariable transformations\nVariable selection\nGrouped vs ungrouped categorical variables\nModels of different types\nDifferent hyperparameter tuning methods\n\nFor each of the various combinations possible, I want to quantify model performance using common performance metrics like AIC or SBC. Commonly, I’ll select the model that has the ‘best’ possible performance among all such models.\nTraditionally, I end up with many R objects: one for each new combination of transformation-model_type-tuning_method. For example, boostFit, xgbFit, glmFit, elastinetFit for untransformed variables. If I have any transformations, I might also have boostFit.xform, xgbFit.xform, glmFit.xform etc. Add to that, investigation of grouped vs ungrouped variables… boostFit.xform.grouped, xgbFit.xform.ungrouped etc. You get the idea.\nThe challenge with this approach is that the data and the models remain separated, there’s a lot of repeat code for object management, manipulation and plotting, and in order to compare all the models together, we have to somehow stitch the results together. (For the last point, resamples() in caret works beautifully, but requires the same number of resamples in each model.)\nThe approach I’m presenting below is a combination of a few approaches I learnt through the APM book, the caret documentation, grammar and verbage in tidyverse, as well as a couple of useful talks in the 2017 R Studio conferenence in Orlando [Notably ones on purrr and list-cols]. What you’ll also see is that the code is extremely succint, which is simply a joy to write and read."
  },
  {
    "objectID": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html#an-example-using-bostonhousing-data",
    "href": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html#an-example-using-bostonhousing-data",
    "title": "Pur(r)ify Your Carets",
    "section": "An example using BostonHousing data",
    "text": "An example using BostonHousing data\n\nLoad libs & data\nThe libraries I’m using here are tidyr, tibble, dplyr, magrittr, purrr, and caret. The dataset is from mlbench.\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(caret)\nlibrary(mlbench)\nlibrary(xgboost)\ndata(\"BostonHousing\")\n\n\n\nTransformations on Xs\nFor the purposes of this demonstration, I’ll simply create two new sets variables using a Box-Cox transformation - caret’s preProcess() makes this easy - and the squared values of the originals. Save each new variable-set in a new character vector which follows the naming convention preds.xxxx.1\n\n# The originals\nresponse <- 'medv'\npreds.original <- colnames(BostonHousing[,1:13])\n\n# Box-Cox transformation\nprepTrain <- preProcess(x = BostonHousing[,preds.original], method = c('BoxCox'))\nboxcoxed <- predict(prepTrain,newdata = BostonHousing[,preds.original])\ncolnames(boxcoxed) <- paste0(colnames(boxcoxed),'.boxed')\npreds.boxcoxed <- colnames(boxcoxed)\n\n# Squaring\nsquared <- (BostonHousing[,c(1:3,5:13)])^2\ncolnames(squared) <- paste0(colnames(squared),'.sq')\npreds.sq <- colnames(squared)\n\n# All together now...\nBostonHousing %<>% \n  cbind(boxcoxed,squared)\n\n# Make sure everything is a numerical (for xgboost to work), and also NOT a tibble (some caret functions have trouble with tibbles)\nBostonHousing %<>% \n  map_df(.f = ~as.numeric(.x)) %>% as.data.frame()\n\nstr(BostonHousing)\n\n'data.frame':   506 obs. of  39 variables:\n $ crim         : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn           : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus        : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas         : num  1 1 1 1 1 1 1 1 1 1 ...\n $ nox          : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm           : num  6.58 6.42 7.18 7 7.15 ...\n $ age          : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis          : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad          : num  1 2 2 3 3 3 5 5 5 5 ...\n $ tax          : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio      : num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b            : num  397 397 393 395 397 ...\n $ lstat        : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv         : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ crim.boxed   : num  -5.06 -3.6 -3.6 -3.43 -2.67 ...\n $ zn.boxed     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus.boxed  : num  0.994 2.966 2.966 0.914 0.914 ...\n $ chas.boxed   : num  1 1 1 1 1 1 1 1 1 1 ...\n $ nox.boxed    : num  -0.83 -1.09 -1.09 -1.13 -1.13 ...\n $ rm.boxed     : num  2.81 2.76 3 2.94 2.99 ...\n $ age.boxed    : num  175 224 161 110 137 ...\n $ dis.boxed    : num  1.41 1.6 1.6 1.8 1.8 ...\n $ rad.boxed    : num  0 0.693 0.693 1.099 1.099 ...\n $ tax.boxed    : num  1.88 1.87 1.87 1.87 1.87 ...\n $ ptratio.boxed: num  117 158 158 174 174 ...\n $ b.boxed      : num  78764 78764 77157 77866 78764 ...\n $ lstat.boxed  : num  1.89 2.78 1.61 1.2 1.99 ...\n $ crim.sq      : num  3.99e-05 7.46e-04 7.45e-04 1.05e-03 4.77e-03 ...\n $ zn.sq        : num  324 0 0 0 0 ...\n $ indus.sq     : num  5.34 49.98 49.98 4.75 4.75 ...\n $ nox.sq       : num  0.289 0.22 0.22 0.21 0.21 ...\n $ rm.sq        : num  43.2 41.2 51.6 49 51.1 ...\n $ age.sq       : num  4251 6225 3733 2098 2938 ...\n $ dis.sq       : num  16.7 24.7 24.7 36.8 36.8 ...\n $ rad.sq       : num  1 4 4 9 9 9 25 25 25 25 ...\n $ tax.sq       : num  87616 58564 58564 49284 49284 ...\n $ ptratio.sq   : num  234 317 317 350 350 ...\n $ b.sq         : num  157530 157530 154315 155733 157530 ...\n $ lstat.sq     : num  24.8 83.54 16.24 8.64 28.41 ...\n\n\nHere’s our new predictor variable sets:\n\npred_varsets <- ls(pattern = 'preds')\npred_varsets\n\n[1] \"preds.boxcoxed\" \"preds.original\" \"preds.sq\"      \n\n\n\n\n\n\nCreate a starter dataframe\nI first create a starter dataframe where the input data is repeated as many times as the number of predictor variable sets. enframe() allows us to embed objects a dataframe column.\n\nnum_var_select <- length(pred_varsets)\nlist(BostonHousing) %>% \n    rep(num_var_select) %>% \n    enframe(name = 'id', value = 'rawdata') %>% \n    mutate(pred_varsets = pred_varsets) -> starter_df\nstarter_df\n\n# A tibble: 3 × 3\n     id rawdata         pred_varsets  \n  <int> <list>          <chr>         \n1     1 <df [506 × 39]> preds.boxcoxed\n2     2 <df [506 × 39]> preds.original\n3     3 <df [506 × 39]> preds.sq      \n\n\nNow, I split the raw data into train.X column which houses data only for those predictor variables identified in the pred_varsets column. map2 is a great function which allows a mapping to be done over two variables and passed to a function.\nI also create a train.Y for the response variable here.\n\n# Function to select columns in the raw data\nfilterColumns <- function(x,y){\n    x[,(colnames(x) %in% eval(parse(text=y)))]\n}\n\n# Create X and Y columns\nstarter_df %<>% \n  transmute(\n  id,\n  pred_varsets,\n  train.X = map2(rawdata, pred_varsets,  ~ filterColumns(.x, .y)),\n  train.Y = map(rawdata, ~ .x$medv)\n  )\n\nstarter_df\n\n# A tibble: 3 × 4\n     id pred_varsets   train.X         train.Y    \n  <int> <chr>          <list>          <list>     \n1     1 preds.boxcoxed <df [506 × 13]> <dbl [506]>\n2     2 preds.original <df [506 × 13]> <dbl [506]>\n3     3 preds.sq       <df [506 × 12]> <dbl [506]>\n\n\n\n\n\nSelect the models\nThis is where I can select which models I want in the analysis. Each model should be in a function of this style:\nmodelName <- function(X, Y){\n    ctrl <- trainControl(\n        ...\n    )\n    train(\n        x = X,\n        y = Y,\n        trContrl = ctrl,\n        method = '## modelname ##',\n        ...\n    )\n}\nI’m using caret exclusively, so each function needs a trainControl() and a train(). Learn more about caret here.\n\nrpartModel <- function(X, Y) {\n    ctrl <- trainControl(\n        ## 5-fold CV\n        method = \"repeatedcv\",\n        number = 5\n    )\n    train(\n        x = X,\n        y = Y,\n        method = 'rpart2',\n        trControl = ctrl,\n        tuneGrid = data.frame(maxdepth=c(2,3,4,5)),\n        preProc = c('center', 'scale')\n    )\n}\nxgbTreeModel <- function(X,Y){\n    ctrl <- trainControl(\n        ## 5-fold CV\n        method = \"repeatedcv\",\n        number = 5\n    )\n    train(\n        x=X,\n        y=Y,\n        method = 'xgbTree',\n        trControl = ctrl,\n        tuneGrid = expand.grid(nrounds = c(100,300,500), \n                              max_depth = c(2,4,6) ,\n                              eta = 0.1,\n                              gamma = 1, \n                              colsample_bytree = 1, \n                              min_child_weight = 1, \n                              subsample = 1),\n        preProc = c('center', 'scale')\n    )\n}\n\nOnce these functions are setup, enframe these into a dataframe.\n\nmodel_list <- list(rpartModel=rpartModel,\n                   xgbModel=xgbTreeModel) %>%\n    enframe(name = 'modelName',value = 'model')\n\nmodel_list\n\n# A tibble: 2 × 2\n  modelName  model \n  <chr>      <list>\n1 rpartModel <fn>  \n2 xgbModel   <fn>  \n\n\n\n\n\nCreate data-model combinations\nNow, we’re ready to combine the two together. train_df has all the predictor varset combinations, model_list has the list of all models. I’m assuming I want to run each combination of the two; so if I have 3 variable sets, and 2 models, I have a total of 6 models to run. This code sets that up:\n\ntrain_df <-\n    starter_df[rep(1:nrow(starter_df),nrow(model_list)),]\n\ntrain_df %<>%\n    bind_cols(\n        model_list[rep(1:nrow(model_list),nrow(starter_df)),] %>% arrange(modelName)\n    ) %>%\n    mutate(id=1:nrow(.))\ntrain_df\n\n# A tibble: 6 × 6\n     id pred_varsets   train.X         train.Y     modelName  model \n  <int> <chr>          <list>          <list>      <chr>      <list>\n1     1 preds.boxcoxed <df [506 × 13]> <dbl [506]> rpartModel <fn>  \n2     2 preds.original <df [506 × 13]> <dbl [506]> rpartModel <fn>  \n3     3 preds.sq       <df [506 × 12]> <dbl [506]> rpartModel <fn>  \n4     4 preds.boxcoxed <df [506 × 13]> <dbl [506]> xgbModel   <fn>  \n5     5 preds.original <df [506 × 13]> <dbl [506]> xgbModel   <fn>  \n6     6 preds.sq       <df [506 × 12]> <dbl [506]> xgbModel   <fn>  \n\n\n\n\n\nSolve the models\nThe data is almost all setup now. invoke_map() is a function which can call functions and pass it arguments. Since we need to pass both train.X and train.Y together, there’s an intermediate call to map2() to “listify” these first into params.\nAll them models solve, and their results (the model object itself) is stored in modelFits.\n\ntrain_df %<>%\n  mutate(params = map2(train.X, train.Y,  ~ list(X = .x, Y = .y)),\n                 modelFits=invoke_map(model,params)\n         )\n\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n\ntrain_df %>% dplyr::select(pred_varsets,modelName,params,modelFits)\n\n# A tibble: 6 × 4\n  pred_varsets   modelName  params           modelFits\n  <chr>          <chr>      <list>           <list>   \n1 preds.boxcoxed rpartModel <named list [2]> <train>  \n2 preds.original rpartModel <named list [2]> <train>  \n3 preds.sq       rpartModel <named list [2]> <train>  \n4 preds.boxcoxed xgbModel   <named list [2]> <train>  \n5 preds.original xgbModel   <named list [2]> <train>  \n6 preds.sq       xgbModel   <named list [2]> <train>  \n\n\n\n\n\nExtract results\nNow, I can extract pretty much any model performance or hypertuning parameter using purrr. Since caret is so lovingly standardized, it doesn’t matter if I’m using a glm, xgboost, rpart2, or ann - the code remains the same.\n\ntrain_df %<>% \n    mutate(\n        RMSE=map_dbl(modelFits,~max(.x$results$RMSE)),\n        RMSESD=map_dbl(modelFits,~max(.x$results$RMSESD)),\n        Rsq=map_dbl(modelFits,~max(.x$results$Rsquared)),\n        bestTune=map(modelFits,~.x$bestTune)\n    )\ntrain_df %>% dplyr::select(-train.X,-train.Y,-params,-modelFits)\n\n# A tibble: 6 × 8\n     id pred_varsets   modelName  model   RMSE RMSESD   Rsq bestTune    \n  <int> <chr>          <chr>      <list> <dbl>  <dbl> <dbl> <list>      \n1     1 preds.boxcoxed rpartModel <fn>    6.15  0.698 0.711 <df [1 × 1]>\n2     2 preds.original rpartModel <fn>    5.87  1.15  0.742 <df [1 × 1]>\n3     3 preds.sq       rpartModel <fn>    6.28  0.950 0.719 <df [1 × 1]>\n4     4 preds.boxcoxed xgbModel   <fn>    3.44  0.532 0.887 <df [1 × 7]>\n5     5 preds.original xgbModel   <fn>    3.27  0.493 0.902 <df [1 × 7]>\n6     6 preds.sq       xgbModel   <fn>    3.31  0.683 0.895 <df [1 × 7]>\n\n\nThis allows us to very quickly visualize the results using lattice or ggplot across all models.\n\nlattice::dotplot(Rsq~pred_varsets|modelName,train_df)\n\n\n\ntrain_df %>% \n    ggplot(aes(x=pred_varsets,color=modelName))+\n    geom_point(aes(y=RMSE),size=2)+\n    geom_errorbar(aes(ymin = RMSE-RMSESD,ymax= RMSE+RMSESD),size=.5,width=.15)\n\n\n\n\nSince the model fit objects themselves are embedded, I can still look at each model’s internals. For example, to plot the results of the 5-fold CV on the grid search for the xgboost model:\n\nplot(train_df$modelFits[train_df$modelName=='xgbModel' & train_df$pred_varsets=='preds.original'][[1]])"
  },
  {
    "objectID": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html#in-conclusion",
    "href": "posts/2017-09-17-pur-r-ify-your-carets/2017-09-17-pur-r-ify-your-carets.html#in-conclusion",
    "title": "Pur(r)ify Your Carets",
    "section": "In conclusion",
    "text": "In conclusion\nThese packages make investigating a very large number of datasets and models easy. With just a few lines of code, I ran a total of 6 different models - and for each model: a 5-fold cross validation, and for the xgboost models: a grid search across two tuning parameters - to select the best model on any number of performance criteria. Yet, everything remains neatly arranged in one dataframe, which can be saved as .RData and retrived later.\nAlso remember that the raw data replicated in the data column of starter_df doesn’t have to be the exact same dataset for each row either. so you could leverage this methodology for a train-validate-test approach, or for resampled training sets, where each row has completely different datasets embedded within. Really depends on your creativity and how you write subsequent code. You’re definitely going to find more material online on this topic, be sure to check r-bloggers."
  },
  {
    "objectID": "posts/2021-01-11-tidytuesday-transit-costs/2021-01-11-tidytuesday-transit-costs.html",
    "href": "posts/2021-01-11-tidytuesday-transit-costs/2021-01-11-tidytuesday-transit-costs.html",
    "title": "TidyTuesday - Transit Costs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(ggdark)\nlibrary(ggtext)\n\ntransit_cost <- data.table::fread(\"transit_cost.csv\")\n\ntransit_cost <- transit_cost %>%\n  mutate(tunnel_per = as.numeric(str_replace(tunnel_per, \"%\", \"\")),\n         real_cost = as.numeric(real_cost),\n         country = ifelse(is.na(country), \"Unk\", country),\n         cc_id = paste(country,city))\n\ndat <- transit_cost %>%\n  group_by(country, city) %>%\n  summarise(\n    total_projects = n(),\n    total_stations = sum(stations, na.rm = TRUE),\n    total_tunnel_len = sum(tunnel, na.rm = TRUE),\n    total_len = sum(length, na.rm = TRUE),\n    tunnel_pc = total_tunnel_len/total_len,\n    total_cost = sum(real_cost, na.rm = TRUE) / 1e3, #now in Billions\n    avg_stations = mean(stations, na.rm = TRUE),\n    avg_tunnel_len = mean(tunnel, na.rm = TRUE),\n    avg_len = mean(length, na.rm = TRUE),\n    tunnel_pc = avg_tunnel_len/avg_len,\n    avg_cost = mean(real_cost, na.rm = TRUE)\n  ) %>%\n  filter(total_len < 10000,\n         total_projects > 1,\n         country %in% c(\"CN\", \"IN\")) %>%\n  mutate(cc_id = paste(country,city),\n         country = ifelse(country == \"CN\", \"China\", \"India\"))\nglimpse(dat)\n\nRows: 35\nColumns: 13\nGroups: country [2]\n$ country          <chr> \"China\", \"China\", \"China\", \"China\", \"China\", \"China\",…\n$ city             <chr> \"Beijing\", \"Changchun\", \"Changsha\", \"Chengdu\", \"Chong…\n$ total_projects   <int> 27, 7, 13, 11, 11, 7, 3, 10, 5, 11, 10, 8, 2, 5, 12, …\n$ total_stations   <int> 376, 81, 152, 152, 163, 66, 61, 121, 90, 154, 192, 10…\n$ total_tunnel_len <dbl> 450.0686, 95.5000, 164.5400, 225.1000, 156.3370, 151.…\n$ total_len        <dbl> 721.973, 116.000, 216.860, 252.950, 273.220, 166.940,…\n$ tunnel_pc        <dbl> 0.7650659, 0.8232759, 0.8219666, 0.8898992, 0.7867776…\n$ total_cost       <dbl> 138.86373, 16.92127, 38.33016, 44.59756, 41.87035, 25…\n$ avg_stations     <dbl> 13.925926, 11.571429, 11.692308, 13.818182, 14.818182…\n$ avg_tunnel_len   <dbl> 20.45766, 13.64286, 13.71167, 20.46364, 19.54213, 21.…\n$ avg_len          <dbl> 26.73974, 16.57143, 16.68154, 22.99545, 24.83818, 23.…\n$ avg_cost         <dbl> 5143.101, 2417.324, 2948.474, 4054.324, 3806.395, 364…\n$ cc_id            <chr> \"CN Beijing\", \"CN Changchun\", \"CN Changsha\", \"CN Chen…\n\nlabel_dat_india <- dat %>%\n  filter(country %in% \"India\",\n         city != \"Gurgaon\")\nlabel_dat_china <- dat %>%\n  filter(city %in% c(\"Shanghai\", \"Beijing\"))\n\nto_plot <- transit_cost %>%\n  tidyr::drop_na() %>%\n  filter(cc_id %in% dat$cc_id,\n         country %in% c(\"CN\", \"IN\"),\n         length < 100) %>%\n  mutate(country = ifelse(country == \"CN\", \"China\", \"India\"))\n\n\nin_color <- \"#2a9d8f\"\ncn_color <- \"#fca311\"\n\nchennai <- dat %>% filter(city == \"Chennai\")\nchennai_stations <- chennai$total_stations\nchennai_projects <- chennai$total_projects\nchennai_x <- chennai$avg_len\nchennai_y <- chennai$avg_stations\nchennai_cost <- chennai$total_cost\n\nto_plot %>%\n  ggplot(aes(length, stations)) +\n  geom_point(color = \"#8d99ae\", size = 0.8, show.legend = FALSE, alpha = 0.3) +\n  geom_smooth(data = dat %>% filter(city!=\"Wenzhou\"),\n              aes(avg_len, avg_stations, color = country),\n              se=FALSE, linetype=\"dashed\", size=0.3, method = \"lm\", span = 4) +\n  geom_point(data = dat,\n             aes(avg_len, avg_stations, color = country, size = total_cost^1.3),\n             pch = 19, alpha = 0.7) +\n  geom_text_repel(data = label_dat_india,\n                   aes(avg_len, avg_stations,\n                                   label = city,\n                                   color = country),\n                                   min.segment.length = 1,\n                   box.padding = unit(0.5, \"line\"),\n                   nudge_x = -1,\n                   show.legend = FALSE) +\n  annotate(geom = \"curve\",\n           xend = label_dat_china$avg_len[1], yend = label_dat_china$avg_stations[1],\n           x = label_dat_china$avg_len[1] + 6, y = label_dat_china$avg_stations[1] - 6,\n           curvature = .3, arrow = arrow(length = unit(0, \"mm\")), color = \"#fca311\") +\n  annotate(geom = \"text\",\n           x = label_dat_china$avg_len[1] + 5.5, y = label_dat_china$avg_stations[1] - 7,\n           label = \"Shanghai & Beijing\", hjust = \"left\", size = 3.4, color = \"#fca311\") +\n  annotate(geom = \"curve\",\n           xend = 2, yend = 0,\n           x = 2 + 7, y = 0 - 11,\n           curvature = -.3, arrow = arrow(length = unit(2, \"mm\")), color = \"#8d99ae\", alpha = 0.6) +\n  annotate(geom = \"text\",\n           x = 2 + 7.5, y = 0 - 11.5,\n           label = \"Each point is a transit line\", hjust = \"left\", size = 3.4, color = \"#8d99ae\") +\n  annotate(geom = \"curve\",\n           xend = chennai_x, yend = chennai_y + 2,\n           x = chennai_x, y = chennai_y + 16,\n           curvature = 0, arrow = arrow(length = unit(0, \"mm\")), color = in_color, alpha = 0.6) +\n  annotate(geom = \"text\",\n           x = chennai_x + 0.5, y = chennai_y + 13,\n           label = glue::glue(\n             \"{cost} over {stations} stations\n             in {chennai_projects} lines\",\n             x = scales::label_number(accuracy = 1, suffix = \" km\")(chennai_x),\n             y = chennai_y,\n             stations = chennai_stations,\n             chennai_projects = chennai_projects,\n             cost = scales::label_dollar(accuracy = 1, suffix = \"M\")(chennai_cost)\n             ),\n           hjust = \"left\", size = 3.4, color = in_color) +\n  dark_theme_minimal() +\n  scale_x_continuous(breaks = seq(10, 90, 20)) +\n  scale_y_continuous(breaks = seq(10, 70, 20)) +\n  scale_size(name = \"Total City Cost\",\n             breaks = c(20^1.3, 50^1.3, 80^1.3),\n             labels = c(\"$20M\", \"$50M\",\"$80M\"),\n             range = c(1,10)\n             ) +\n  coord_cartesian(ylim = c(0,90), clip = \"off\") +\n  labs(\n    title = \"The Cost of Transit in the 21<sup>st</sup> Century\",\n    subtitle = glue::glue(\"<span style='color:{cn_color};font-family:Inter-Medium;'>China: </span>253 projects in 28 cities, totaling $1T since 1998<br /><span style='color:{in_color};font-family:Inter-Medium;'>India: </span> 29 projects in 7 cities, totaling $2B since 2011<br /><span style='color:{in_color};font-family:Inter-Medium;'>India</span> has longer transit lines with more stations than <span style='color:{cn_color};font-family:Inter-Medium;'>China</span>, driving up costs <br />for each city.\"),\n    x = \"Average Length\",\n    y = \"Average Stations\",\n    caption = \"#tidytuesday\\n@rsangole\"\n    ) +\n  theme(\n    legend.title = element_text(size = 10, color = \"gray60\"),\n    legend.text =  element_text(size = 10, color = \"gray60\"),\n    axis.ticks.x.bottom = element_line(colour = \"gray30\",size = 0.5),\n    axis.ticks.y.left = element_line(colour = \"gray30\"),\n    axis.title.y = element_text(hjust = .9, size = 10, face = \"italic\", color = \"gray60\"),\n    axis.title.x = element_text(hjust = .9, size = 10, face = \"italic\", color = \"gray60\"),\n    plot.title = element_markdown(family = \"Inter-Medium\", color = \"#f8f8f2\", size = 22,\n                                  margin = margin(0, 0, 0.5, 0, unit = \"line\")),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(color = \"#f8f8f2\", size = 12, lineheight = 1.2,\n                                     margin = margin(0, 0, 1, 0, unit = \"line\")),\n    plot.margin = margin(1.5, 1.5, 1, 1.5, unit = \"line\"),\n    legend.position = c(0.9,0.1)\n    ) +\n  scale_discrete_manual(aesthetics = \"color\",\n                        values = c(\"India\" = in_color, \"China\" = cn_color),\n                        guide = F)"
  },
  {
    "objectID": "posts/2021-01-19-tidytuesday-the-tate-collection/2021-01-19-tidytuesday-the-tate-collection.html",
    "href": "posts/2021-01-19-tidytuesday-the-tate-collection/2021-01-19-tidytuesday-the-tate-collection.html",
    "title": "TidyTuesday - The Tate Collection",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidyr)\nlibrary(data.table)\nlibrary(beeswarm)\nlibrary(extrafont)\n# extrafont::font_import(paths = \".\", prompt = F)\n\nartists <- data.table::fread(\"artists.csv\") %>% \n  mutate(gender = ifelse(is.na(gender), \"Unknown\", gender),\n         gender = factor(gender,levels = c(\"Male\",\"Female\", \"Unknown\"), ordered = TRUE),\n         life_yr = yearOfDeath - yearOfBirth,\n         pre_1850 = yearOfBirth < 1850,\n         name_len = stringr::str_length(name) - 2) %>% \n  separate(col = \"placeOfBirth\", sep = \",\", into = c(\"birth_city\", \"birth_country\"), remove = F) %>% \n  separate(col = \"placeOfDeath\", sep = \",\", into = c(\"death_city\", \"death_country\"), remove = F) %>% \n  mutate(moved_countries = birth_country != death_country,\n         birth_country = ifelse(is.na(birth_country), \"Unknown\", birth_country),\n         death_country = ifelse(is.na(death_country), \"Unknown\", death_country))\nartwork <-\n    data.table::fread(\"artwork.csv\") %>%\n    mutate(\n        artistRole = as.factor(artistRole),\n        medium = as.factor(medium),\n        units = as.factor(units),\n        area = width * height,\n        title_len = stringr::str_length(title)\n    ) %>%\n    left_join(\n        y = artists %>% select(name, gender, yearOfBirth, birth_city, birth_country, life_yr),\n        by = c(\"artist\" = \"name\")\n    )\n\n\ncolor_pallete <- c(\"#005780\",\n                   \"#3e487a\",\n                   \"#955196\",\n                   \"#dd5182\",\n                   \"#ff6e54\",\n                   \"#ffa600\")\n\nartwork[,\n        medium_cleaned := case_when(\n            grepl(pattern = \"Graphite\", x = medium) ~ \"Graphite\",\n            grepl(pattern = \"Oil paint\", x = medium) ~ \"Oil Paint\",\n            grepl(pattern = \"Screenprint\", x = medium) ~ \"Screenprint\",\n            grepl(pattern = \"Watercolour\", x = medium) ~ \"Watercolour\",\n            grepl(pattern = \"photograph|Photograph\", x = medium) ~ \"Photograph\",\n            grepl(pattern = \"chalk|Chalk\", x = medium) ~ \"Chalk\"\n        )]\nartwork[,\n        color := case_when(\n            medium_cleaned %like% \"Graphite\" ~ color_pallete[1],\n            medium_cleaned %like% \"Paint\" ~ color_pallete[2],\n            medium_cleaned %like% \"Screenprint\" ~ color_pallete[6],\n            medium_cleaned %like% \"Watercolour\" ~ color_pallete[4],\n            medium_cleaned %like% \"Photograph\" ~ color_pallete[5],\n            medium_cleaned %like% \"Chalk\" ~ color_pallete[3]\n        )]\nmedium_dat_2 <- artwork[, .(year, medium_cleaned, color)]\n\nbees_plot <- medium_dat_2 %>%\n    filter(!is.na(medium_cleaned), !is.na(year)) %>%\n    arrange(year)\nbees_plot[, cutpts_numeric := cut(year, breaks = seq(1500, 2015, 5), labels = F)]\nbees_plot[, cutpts := cut(year, breaks = seq(1500, 2015, 5))]\nbees_plot[, xaxis := as.numeric(substr(cutpts, 2, 5))]\n\nbees_plot_reduced <-\n    bees_plot[, .N, .(xaxis, medium_cleaned, color, cutpts_numeric)]\nbees_plot_reduced[, num_pts := ceiling(N / 100)]\n\ndatlist <- list()\nfor (i in 1:nrow(bees_plot_reduced)) {\n    .nrows = bees_plot_reduced[i, num_pts]\n    .dlist <- list()\n    for (j in 1:.nrows) {\n        .dlist[[j]] <-\n            bees_plot_reduced[i, .(xaxis, medium_cleaned, color, cutpts_numeric)]\n    }\n    datlist[[i]] <- rbindlist(.dlist)\n}\n\nto_plot <- rbindlist(datlist)\nglimpse(to_plot)\n\nRows: 624\nColumns: 4\n$ xaxis          <dbl> 1540, 1555, 1560, 1565, 1570, 1575, 1585, 1590, 1595, 1…\n$ medium_cleaned <chr> \"Oil Paint\", \"Oil Paint\", \"Oil Paint\", \"Oil Paint\", \"Oi…\n$ color          <chr> \"#3e487a\", \"#3e487a\", \"#3e487a\", \"#3e487a\", \"#3e487a\", …\n$ cutpts_numeric <int> 9, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, …\n\n\n\nmake_plot <- function(to_plot) {\n    categories <- unique(to_plot[, .(medium_cleaned, color)])\n    xleg <- c(1.0,  2.8,  4.6,  6.8,  8.2, 10.0)\n    beeswarm(\n        xleg,\n        pwcol = categories$color,\n        horizontal = TRUE,\n        method = \"center\",\n        cex = 1.3,\n        pch = 19,\n        xlim = c(0, 12),\n        axes = FALSE\n    )\n    text(\n        x = xleg + 0.05,\n        y = 1,\n        labels = toupper(categories$medium_cleaned),\n        col = \"gray80\",\n        pos = 4,\n        cex = 0.9\n    )\n}\nmake_plot_2 <- function(to_plot) {\n    beeswarm(\n        x = to_plot$xaxis,\n        pwcol = to_plot$color,\n        horizontal = TRUE,\n        method = \"hex\",\n        spacing = 1.1,\n        cex = 1.5,\n        pch = 19,\n        xlim = c(1550, 2010),\n        axes = FALSE\n    )\n    x <- seq(from = 1550, to = 2000, by = 50)\n    axis(\n        side = 1,\n        labels = x,\n        at = x,\n        col = \"white\",\n        col.ticks = \"gray80\",\n        col.axis = \"gray60\"\n    )\n}\npar(\n    fig = c(0, 1, 0.7, 1),\n    new = TRUE,\n    bg = \"#292929\"\n    # family = \"Monoid\"\n)\nmake_plot(to_plot = to_plot)\npar(fig = c(0, 1, 0, 0.9), new = TRUE)\nmake_plot_2(to_plot = to_plot)\nmtext(\n    text = \"The Tate\",\n    side = 3,\n    cex = 2,\n    col = \"gray80\",\n    padj = -5\n)\nmtext(\n    text = \"THE TOP 6 MEDIUMS FOR 40,000 PIECES OF ART. EACH DOT REPRESENTS UPTO 100 ARTWORKS.\",\n    side = 3,\n    cex = 1,\n    col = \"gray80\",\n    padj = -5.5\n)\nmtext(\n    text = \"Image by @rsangole\",\n    side = 1,\n    cex = 0.9,\n    col = \"#3e487a\",\n    padj = 6,\n    adj = 1\n)"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "",
    "text": "The M5 Forecasting Competiton recently finished, this past Jun 2020. On the 29th of Oct, M Open Forecasting Center (MOFC) held a virtual awards ceremony. I jotted down some notes for myself from the key note speakers."
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#spyros-makridakis-founder",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#spyros-makridakis-founder",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Spyros Makridakis / Founder",
    "text": "Spyros Makridakis / Founder\n\nHistory of forecasting competitions\n\nInitial forecasting work focused on simpler statistical models and valued human judgement (albeit with minimal improvement in forecasting accuracy). The participants for the first many forecasting competitions were largely statistians and the like.\nM3 competition was huge push forward, ~10% accuracy improvement\nM4 onwards, lots more machine learning solutions\n\n\n\n\nM5 Competition\n\nBest method is 22.4% more accurate than best statistical benchmark\nHigher levels of hierarchy show better improvement than lower levels\nMiddle of CI distribution shows most improvement; marginal/worse at tails\n\n\n\nWhat is the value of experience?\n\nWinner is an undergraduate student, has no experience in forecasting, and little/no experience in data science!\nWinner beat 7000+ masters & grand masters!\nWinning method LightGBM used by top 50 competitors\nEvidence that understanding the computational algorithms instead of forecasting itself is paying off"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#chris-fry-google",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#chris-fry-google",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Chris Fry / Google",
    "text": "Chris Fry / Google\n\nChris got involved by analyzing M4, it’s applicability to real world data sets. His critique and subsequent issues addressed in the M5 competition:\n\n\n\nSpoke of the challenge of single time-window submission:\n\nShort period of 28 days\nResults are subject to ‘uniqueness’ of test window\n@google, they do something a bit different:\n\nlarge sample for evaluation\nyear long window will cover all holidays & other events\nshould provide better stability in measurement of algorithm performance\n\n\n\n\n\n75% of score is across top 9 levels. Bottom three are disaggregated intermittent noizy series.\n\n\n\nPrice weighting : 90% of the weight came from 11% of the total series. Competiton winners can focus on left sliver and win… but businesses do care about the right (inventory, spoilage etc)."
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#brian-seaman-walmart",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#brian-seaman-walmart",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Brian Seaman / Walmart",
    "text": "Brian Seaman / Walmart\nWalmart sponsored the M5 competiton and offered their real world data for the competition.\n\n\nWhy Walmart cares about forecasts?\n\nInventory management, capacity planning, labor planning, financials, real estate, network capacity etc\n\nWalmart challenges\n\n10k Stores x 100k Items, 2m associates!\nVaried - Top sellers, Long Tails, Seasonal/Steady series\nNew stores - cold start challenges\n\nOpportunities\n\nScaling & better reliability by relying on Cloud based solutions\nDevelop relationships (product hierarchies), (product obsoluetlece, superceding)\nEvents like holidays\n\nM5 competition\n\nTried to keep datasets large without getting overwhelming\nIntermittent demand, short histories and discontinued items; but no cold-starts since you need to develop some relationship models to previous like-items\nItem relationships - product taxonomy + geography\nOthers - Events, pricing, weather"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#addison-howard-kaggle",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#addison-howard-kaggle",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Addison Howard / Kaggle",
    "text": "Addison Howard / Kaggle\n\nOverview of how Kaggle works\nBalance of expertize vs industry specific knowledge?\nYet again, Addison spoke about the need for expertize in a domain vs computational prowess given the development in AI/ML.\n\nThis as a very eye-opening revalition!\n\n\n\n\nHe spoke about how competitions are an excellent platform for pushing our collective wisdom to solve complex problems. Look at these error bars for the same problem Martin solved, when picked up by teams!\n\n - Some insights into the M5 competition:"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#len-tashman-editor-of-foresight",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#len-tashman-editor-of-foresight",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Len Tashman / Editor of Foresight",
    "text": "Len Tashman / Editor of Foresight\nComments about takeaways of M5:\n\nGood\n\nM5 was based on real data\nData were hierarchical, and dirty (intermittent, noisy)\nML is proving extremely valuable\n\nBut…\n\nML methods are still quite black-boxy. Business don’t inherently accept these models.\nFirms need to access how these methods improve their operational performance\nSpoke of firms which do not use any methodical forecasting at all. How can they be brought into the fold?\nHow can we find the balance between traditional statistical approaches and these new ML approches in this field?"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#tldr",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#tldr",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "tl;dr",
    "text": "tl;dr\n\n{stats} continues to dominate the speed tests\n{fastDummies} had similar speeds only for dataframes with rows ~1M\n{dummy} and {dummies} are the slowest"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#motivation",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#motivation",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Motivation",
    "text": "Motivation\nIn 2017, I compared the performance of four packages {stats}, {dummies}, {dummy} and {caret} to create dummy variables in this post.\nJacob Kaplan of UPenn has created a new package {fastdummies} which claims to be faster than other existing packages.\nLet’s test it out."
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#machine",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#machine",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Machine",
    "text": "Machine\nI’m running these tests on a 2019 MacBook Pro running macOS Catalina (10.15.7) on a 2.4 GHz 8-Core Intel i9 with 32 MB 2400 MHz DDR4, in a docker container running:\nplatform       x86_64-pc-linux-gnu         \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          4                           \nminor          0.0                         \nyear           2020                        \nmonth          04                          \nday            24                          \nsvn rev        78286                       \nlanguage       R                           \nversion.string R version 4.0.0 (2020-04-24)\nnickname       Arbor Day"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#perf-testing",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#perf-testing",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Perf Testing",
    "text": "Perf Testing\n\nA quick test\nCreate a test dataset…\n\nNROW  <- 1e4\nfac_levels <- c(4, 4, 5, 5, 7, 7, 9, 9)\ninput_data <- tibble::tibble(\n    facVar_1 = as.factor(sample(LETTERS[1:fac_levels[1]], size = NROW, replace = TRUE)),\n    facVar_2 = as.factor(sample(LETTERS[1:fac_levels[2]], size = NROW, replace = TRUE)),\n    facVar_3 = as.factor(sample(LETTERS[1:fac_levels[3]], size = NROW, replace = TRUE)),\n    facVar_4 = as.factor(sample(LETTERS[1:fac_levels[4]], size = NROW, replace = TRUE)),\n    facVar_5 = as.factor(sample(LETTERS[1:fac_levels[5]], size = NROW, replace = TRUE)),\n    facVar_6 = as.factor(sample(LETTERS[1:fac_levels[6]], size = NROW, replace = TRUE)),\n    facVar_7 = as.factor(sample(LETTERS[1:fac_levels[7]], size = NROW, replace = TRUE)),\n    facVar_8 = as.factor(sample(LETTERS[1:fac_levels[8]], size = NROW, replace = TRUE))\n)\nstr(input_data)\n\ntibble [10,000 × 8] (S3: tbl_df/tbl/data.frame)\n $ facVar_1: Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 4 4 3 3 2 4 1 3 1 ...\n $ facVar_2: Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 4 1 4 4 4 4 4 3 3 4 ...\n $ facVar_3: Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 5 1 5 5 5 4 4 4 3 2 ...\n $ facVar_4: Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 2 3 1 1 5 5 1 4 2 3 ...\n $ facVar_5: Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 3 5 6 3 5 2 5 2 7 6 ...\n $ facVar_6: Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 5 2 1 3 3 5 5 6 2 2 ...\n $ facVar_7: Factor w/ 9 levels \"A\",\"B\",\"C\",\"D\",..: 3 6 7 5 8 8 8 8 3 8 ...\n $ facVar_8: Factor w/ 9 levels \"A\",\"B\",\"C\",\"D\",..: 5 8 7 3 6 2 7 8 8 2 ...\n\n\nRun microbenchmark…\n\nstats_fn <- function(dat) stats::model.matrix(~.-1,dat)\ndummies_fn <- function(dat) dummies::dummy.data.frame(as.data.frame(dat))\ndummy_fn <- function(dat) dummy::dummy(dat)\ncaret_fn <- function(dat) {caret::dummyVars(formula = ~.,data = dat) %>% predict(newdata = dat)}\nfastDummies_fn <- function(dat) fastDummies::dummy_cols(dat)\n\nmicrobenchmark::microbenchmark(\n    stats =       stats_fn(input_data),\n    dummies =     dummies_fn(input_data),\n    dummy =       dummy_fn(input_data),\n    caret =       caret_fn(input_data),\n    fastDummies = fastDummies_fn(input_data),\n    times = 10L\n    ) %>% autoplot()\n\n\n\n\nstats is still clearly the fastest of all the packages, for this moderately sized dataset.\n\n\nDig a bit deeper\nHow does the performance vary when rows, columns, or number of factors are scaled?\nFirst, make some functions to create dataframes with varying rows/cols/levels per variable, run benchmarks & extract median execution times.\n\nmake_data <- function(NROW = 10, NCOL = 5, NFAC = 5){\n    sapply(1:NCOL, \n           function(x) sample(LETTERS[1:NFAC], \n                              size = NROW, \n                              replace = TRUE)) %>% \n        as_tibble()\n    \n}\nrun_benchmark <- function(dat){\n    microbenchmark::microbenchmark(\n    stats =       stats_fn(dat),\n    dummies =     dummies_fn(dat),\n    dummy =       dummy_fn(dat),\n    caret =       caret_fn(dat),\n    fastDummies = fastDummies_fn(dat),\n    times = 10L\n    )\n}\nextract_median_time <- function(benchmarks){\n    as_tibble(benchmarks) %>% \n        dplyr::group_by(expr) %>% \n        summarize(median_ms = median(time) * 1e-6)\n}\n\nmake_data makes a pretty simple tibble:\n\nmake_data(NROW = 5, NCOL = 6, NFAC = 3)\n\n# A tibble: 5 × 6\n  V1    V2    V3    V4    V5    V6   \n  <chr> <chr> <chr> <chr> <chr> <chr>\n1 A     C     A     C     A     B    \n2 A     A     C     A     A     C    \n3 A     C     C     A     A     A    \n4 C     A     C     B     B     A    \n5 C     A     C     A     A     B    \n\n\n\n\nHow does performance scale by number of rows?\nstats still rocks. With very large datasets, fastDummies approaches similar speed.\n\nexperiment_rows <- tibble::tibble(\n    nrows = 10^(1:6)\n    ) %>% \n    dplyr::mutate(input_data = purrr::map(nrows, ~make_data(NROW = .x, NCOL = 5, NFAC = 5)),\n                  benchmarks = purrr::map(input_data, ~run_benchmark(.x)),\n                  median_times = purrr::map(benchmarks, ~extract_median_time(.x)))\nexperiment_rows %>% \n    dplyr::select(nrows, median_times) %>%\n    tidyr::unnest(cols = c(median_times)) %>%\n    dplyr::rename(Package = expr) %>% \n    tidyr::pivot_wider(names_from = Package, values_from = median_ms) %>% \n    dplyr::mutate(\n        dummies = dummies/stats,\n        dummy = dummy/stats,\n        caret = caret/stats,\n        fastDummies = fastDummies/stats,\n        stats = 1\n    ) %>%\n    tidyr::pivot_longer(-nrows) %>% \n    ggplot(aes(nrows, value, color = name)) +\n    geom_line() +\n    geom_point(aes(text = glue::glue(\"<b>{title}</b> {verb} {y}x\", \n                                     title = name, \n                                     verb = ifelse(name == \"stats\", \":\", \"slower by\"), \n                                     y = ifelse(value > 2,\n                                            round(value),\n                                            round(value, digits = 1))))) +\n    scale_y_log10(labels = scales::label_number(accuracy = 1, suffix = \"x\")) +\n    scale_x_log10(breaks = 10^(1:6), labels = scales::label_number_si()) +\n    labs(x = \"Number of Rows\", y = \"Relative Execution Rates\", \n         title = \"Row Performance (log-log scale)\") -> p\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\nHow does performance scale by number of columns?\nstats is the clear winner here.\n\n\n\n\n\n\n\n\nHow does performance scale by number of levels?\nInterestingly, number of levels per factor have little/no impact on performance for stats, caret and dummies. fastDummies & dummies show a positive correlation to levels."
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#conclusion",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#conclusion",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Conclusion",
    "text": "Conclusion\nSee tl;dr"
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "",
    "text": "Very recently, at work, we got into a discussion about creation of dummy variables in R code. We were dealing with a fairly large dataset of roughly 500,000 observations for roughly 120 predictor variables. Almost all of them were categorical variables, many of them with a fairly large number of factor levels (think 20-100). The types of models we needed to investigate required creation of dummy variables (think xgboost). There are a few ways to convert categoricals into dummy variables in R. However, I did not find any comparison of performance for large datasets.\nSo here it goes."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#why-do-we-need-dummy-variables",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#why-do-we-need-dummy-variables",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Why do we need dummy variables?",
    "text": "Why do we need dummy variables?\nI won’t say any more here. Plenty of good resources on the web: here, here, and here."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#ways-to-create-dummy-variables-in-r",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#ways-to-create-dummy-variables-in-r",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Ways to create dummy variables in R",
    "text": "Ways to create dummy variables in R\nThese are the methods I’ve found to create dummy variables in R. I’ve explored each of these\n\nstats::model.matrix()\ndummies::dummy.data.frame()\ndummy::dummy()\n\ncaret::dummyVars()\n\nPrepping some data to try these out. Using the HairEyeColor dataset as an example. It consists of 3 categorical vars and 1 numerical var. Perfect to try things out. Adding a response variable Y too.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(magrittr)\ndata(\"HairEyeColor\")\nHairEyeColor %<>% tbl_df()\nHairEyeColor$Y = sample(c(0,1),dim(HairEyeColor)[1],replace = T) %>% factor(levels = c(0,1),labels = c('No','Yes'))\nglimpse(HairEyeColor)\n\nRows: 32\nColumns: 5\n$ Hair <chr> \"Black\", \"Brown\", \"Red\", \"Blond\", \"Black\", \"Brown\", \"Red\", \"Blond…\n$ Eye  <chr> \"Brown\", \"Brown\", \"Brown\", \"Brown\", \"Blue\", \"Blue\", \"Blue\", \"Blue…\n$ Sex  <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ n    <dbl> 32, 53, 10, 3, 11, 50, 10, 30, 10, 25, 7, 5, 3, 15, 7, 8, 36, 66,…\n$ Y    <fct> Yes, Yes, Yes, No, Yes, No, Yes, No, Yes, No, No, Yes, No, No, Ye…\n\n\nLet’s look at each package:\n\nstats package\nThe stats package has a function called model.matrix which converts factor variables to dummy variables. It also drops the response variable.\nSome pros\n\nWorks with tibbles\nReally fast\nRetains numerical columns as is\nFormula interface allows one to specify what Y is\n\nSome cons\n\nNeed to add the response Y back into the mix, if we need it\n\n\nhead(model.matrix(Y~.-1,HairEyeColor),3)\n\n  HairBlack HairBlond HairBrown HairRed EyeBrown EyeGreen EyeHazel SexMale  n\n1         1         0         0       0        1        0        0       1 32\n2         0         0         1       0        1        0        0       1 53\n3         0         0         0       1        1        0        0       1 10\n\n\n\n\ndummies package\ndummies has a command called dummy.data.frame which does the needful.\nSome pros\n\nRetains numerical columns as is\nCan create based dummy variables for numeric columns too\n\nSome cons\n\nDoesn’t work with tibbles\nDoesn’t have a formula interface to specify what Y is. Need to manually remove response variable from dataframe\n\n\nlibrary(dummies)\nhead(dummy.data.frame(data = as.data.frame(HairEyeColor),sep=\".\"),3)\n\n  Hair.Black Hair.Blond Hair.Brown Hair.Red Eye.Blue Eye.Brown Eye.Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye.Hazel Sex.Female Sex.Male  n Y.No Y.Yes\n1         0          0        1 32    0     1\n2         0          0        1 53    0     1\n3         0          0        1 10    0     1\n\n\n\n\ndummy package\ndummy creates dummy variables of all the factors and character vectors in a data frame. It also supports settings in which the user only wants to compute dummies for the categorical values that were present in another data set. This is especially useful in the context of predictive modeling, in which the new (test) data has more or other categories than the training data. 1\nSome pros\n\nWorks with tibbles\nRetains numerical columns as is\nCan create based dummy variables for numeric columns too\np parameter can select terms in terms of frequency\nCan grab only those variables in a separate dataframe\nCan create based dummy variables for numeric columns too\n\nSome cons\n\nDoesn’t have a formula interface to specify what Y is. Need to manually remove response variable from dataframe\n\n\nlibrary(dummy)\nhead(dummy(HairEyeColor),3)\n\n  Hair_Black Hair_Blond Hair_Brown Hair_Red Eye_Blue Eye_Brown Eye_Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye_Hazel Sex_Female Sex_Male Y_No Y_Yes\n1         0          0        1    0     1\n2         0          0        1    0     1\n3         0          0        1    0     1\n\n\nSide note: there’s a useful feature to grab all the categories in a factor variable.\n\ncategories(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Blond\" \"Brown\" \"Red\"  \n\n$Eye\n[1] \"Blue\"  \"Brown\" \"Green\" \"Hazel\"\n\n$Sex\n[1] \"Female\" \"Male\"  \n\n$Y\n[1] \"No\"  \"Yes\"\n\n\n\n\ncaret package\nLastly, there’s the caret package’s dummyVars(). This follows a different paradigm. First, we create reciepe of sorts, which just creates an object that specifies how the dataframe gets dummy-fied. Then, use the predict() to make the actual conversions.\nSome pros\n\nWorks on creating full rank & less than full rank matrix post-conversion\nHas a feature to keep only the level names in the final dummy columns\nCan directly create a sparse matrix\nRetains numerical columns as is\n\nSome cons\n\nY needs a factor\nIf the cateogical variables aren’t factors, you can’t use the sep=' ' feature\n\n\nlibrary(caret)\nHairEyeColor$Hair <- as.factor(HairEyeColor$Hair)\nHairEyeColor$Eye <- as.factor(HairEyeColor$Eye)\nHairEyeColor$Sex <- as.factor(HairEyeColor$Sex)\ndV <- dummyVars(formula = Y~.,data = HairEyeColor)\ndV\n\nDummy Variable Object\n\nFormula: Y ~ .\n5 variables, 4 factors\nVariables and levels will be separated by '.'\nA less than full rank encoding is used\n\n\n\nhead(predict(object = dV, newdata = HairEyeColor),3)\n\n  Hair.Black Hair.Blond Hair.Brown Hair.Red Eye.Blue Eye.Brown Eye.Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye.Hazel Sex.Female Sex.Male  n\n1         0          0        1 32\n2         0          0        1 53\n3         0          0        1 10"
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#performance-comparison",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#performance-comparison",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Performance comparison",
    "text": "Performance comparison\nI’ve run these benchmarks on my Macbook Pro with these specs:\n\nProcessor Name: Intel Core i5\nProcessor Speed: 2.4 GHz\nNumber of Processors: 1\nTotal Number of Cores: 2\nL2 Cache (per Core): 256 KB\nL3 Cache: 3 MB\nMemory: 8 GB\n\n\nSmaller datasets\nThe first dataset used is the HairEyeColor. 32 rows, 1 numeric var, 3 categorical var. All the resulting dataframes are as similar as possible… they all retain the Y variable at the end.\n\nlibrary(microbenchmark)\nHairEyeColor_df <- as.data.frame(HairEyeColor)\n\nstats_fn <- function(D){\n    stats::model.matrix(Y~.-1,D) %>% \n        cbind(D$Y)\n}\n\ndummies_fn <- function(D){\n    dummies::dummy.data.frame(D[,-5]) %>% \n        cbind(D$Y)\n}\n\ndummy_fn <- function(D){\n    dummy::dummy(D[,-5]) %>% \n        cbind(D$Y)\n}\n\ncaret_fn <- function(D){\n    dV <- caret::dummyVars(formula = Y~.,data = D)\n    predict(object = dV, newdata = D) %>% \n        cbind(D$Y)\n    }\n\nmicrobenchmark::microbenchmark(\n    stats = stats_fn(D = HairEyeColor),\n    dummies = dummies_fn(D = HairEyeColor_df),\n    dummy = dummy_fn(D = HairEyeColor),\n    caret = caret_fn(D = HairEyeColor),\n    times = 1000L,\n    control = list(order = 'block'),\n    unit = 's'\n    ) -> benchmarks\n\nautoplot(benchmarks)\n\n\n\n\nThe results speak for themself. The stats is clearly the fastest with dummies and caret being a more distant 2nd & 3rd.\n\n\nLarge datasets\nTo leverage a large dataset for this analysis, I’m using the Accident & Traffic Flow dataset, which is fairly big - 570,011 rows and 33 columns. I’ve narrowed down to 7 categorical variables to test the packages, and I’ve created a fake response variable as well.\n\ndata <- read_csv('~/github/github.com/blog-large-data/accidents_2005_to_2007.csv',progress = F)\ndata %<>%\n    transmute(\n        Day_of_Week = as.factor(Day_of_Week),\n        Road_Type = Road_Type %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        Weather = Weather_Conditions %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        RoadSurface = Road_Surface_Conditions %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        PedHC =  `Pedestrian_Crossing-Human_Control` %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        PedPF =  `Pedestrian_Crossing-Physical_Facilities` %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        Year =  as.factor(Year)\n    ) %>% \n    mutate(\n        Y = sample(c(0,1),dim(data)[1],replace = T) %>% factor(levels = c(0,1),labels = c('No','Yes'))\n    )\ndim(data)\n\n[1] 570011      8\n\n\nIn total, there will be 39 dummy variable columns created for these 7 factor variables, as we can see here:\n\nmap_int(data,~length(levels(.x)))\n\nDay_of_Week   Road_Type     Weather RoadSurface       PedHC       PedPF \n          7           6           9           5           3           6 \n       Year           Y \n          3           2 \n\n\nNow for the benchmarks:\n\ndata_df <- as.data.frame(data)\nstats_fn <- function(D){\n    stats::model.matrix(Y~.-1,D) %>% \n        cbind(D$Y)\n}\n\ndummies_fn <- function(D){\n    dummies::dummy.data.frame(D[,-8]) %>% \n        cbind(D$Y)\n}\n\ndummy_fn <- function(D){\n    dummy::dummy(D[,-8]) %>% \n        cbind(D$Y)\n}\n\ncaret_fn <- function(D){\n    dV <- caret::dummyVars(formula = Y~.,data = D)\n    predict(object = dV, newdata = D) %>% \n        cbind(D$Y)\n    }\n\nmicrobenchmark::microbenchmark(\n    stats = stats_fn(D = data),\n    dummies = dummies_fn(D = data_df),\n    dummy = dummy_fn(D = data),\n    caret = caret_fn(D = data),\n    times = 30L,\n    control = list(order = 'block')\n    ) -> benchmarks\n\nautoplot(benchmarks)\n\n\n\n\nJust like before, stats is clerly the fastest."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#conclusion",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#conclusion",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Conclusion",
    "text": "Conclusion\n\nStick to stats::model.matrix(). It works with tibbles, it’s fast, and it takes a formula.\nIf you like the caret package and it’s interface, it’s the 2nd best choice.\ndummy or dummies doesn’t seem to offer any advantages to these packages."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#qs",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#qs",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Qs",
    "text": "Qs\n\nAre there other packages you recommend for dummy variable creation? If yes, please let me know in the comments.\nCould you run the bench marks on more powerful machines and larger datasets, and share your results? I’d like to append them here."
  },
  {
    "objectID": "posts/2021-04-13-30-day-chart-challenge/2021-04-13-30-day-chart-challenge.html",
    "href": "posts/2021-04-13-30-day-chart-challenge/2021-04-13-30-day-chart-challenge.html",
    "title": "Visualizing Correlations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(ggdark)\nlibrary(ggtext)\nlibrary(ggforce)\n\n# https://data.world/markmarkoh/kepler-confirmed-planets\ndat <- data.table::fread(\"planets.csv\")\n\nhull_a <- dat %>% \n  select(st_rad, st_mass) %>% \n  filter(st_rad > 0.335 & st_rad < 2.95 & st_mass < 2) %>% \n  tidyr::drop_na()\ncor_a <- scales::label_number(accuracy = 0.01)(cor(hull_a)[1, 2])\n\nhull_b <- dat %>% \n  select(st_rad, st_mass) %>% \n  filter(st_rad > 2.95) %>% \n  tidyr::drop_na()\ncor_b <- scales::label_number(accuracy = 0.01)(cor(hull_b)[1,2])\n\n# https://coolors.co/1c1f35-151728-44243e-723054-68838c-4e636a-4ab6d3\npt_color <- \"#6BC3DB\"\nhull_color <- \"#68838C\"\naxis_text_color <- \"#68838C\"\ncaption_color <- \"#4E636A\"\ngrid_color <- \"#1C1F35\"\nplot_title_color <- \"#903C6A\"\nbg_color <- \"#151728\"\n\ndat %>%\n  ggplot(aes(x = st_rad, y = st_mass)) +\n  geom_point(\n    colour = pt_color,\n    pch = 21,\n    size = 1,\n    alpha = 0.3\n  ) +\n  geom_mark_hull(\n    data = hull_a,\n    aes(label = glue::glue(\"Group A\\nCorr: {cor_a}\")),\n    concavity = 10,\n    color = hull_color,\n    size = 0.3,\n    radius = .02,\n    con.cap = 0,\n    con.colour = hull_color,\n    con.border = \"none\",\n    label.fill = bg_color,\n    label.colour = hull_color,\n    label.margin = margin(1, 1, 1, 1, \"mm\")\n  ) +\n  geom_mark_hull(\n    data = hull_b,\n    aes(label = glue::glue(\"Group B\\nCorr: {cor_b}\")),\n    concavity = 10,\n    color = hull_color,\n    size = 0.3,\n    radius = .02,\n    con.cap = 0,\n    con.colour = hull_color,\n    con.border = \"none\",\n    label.fill = bg_color,\n    label.colour = hull_color,\n    label.margin = margin(1, 1, 1, 1, \"mm\")\n  ) +\n  annotate(\n    geom = \"curve\",\n    xend = 0.09,\n    yend = 0.06,\n    x = 0.09 - 0.01,\n    y = 0.06 + 0.1,\n    curvature = -.3,\n    arrow = arrow(length = unit(0, \"mm\")),\n    color = hull_color\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 0.09 - 0.01,\n    y = 0.06 + 0.1,\n    label = \"Each point\\nis a star\",\n    hjust = \"right\",\n    size = 3.4,\n    color = hull_color\n  ) +\n  scale_x_continuous(trans = \"log10\",\n                     labels =\n                       scales::label_number(suffix = \"x\",\n                                            accuracy = 1,)) +\n  scale_y_continuous(labels =\n                       scales::label_number(suffix = \"x\",\n                                            accuracy = 1,)) +\n  dark_theme_minimal() +\n  coord_cartesian(ylim = c(0, 3), clip = \"off\") +\n  theme(\n    plot.background = element_rect(color = bg_color, \n                                   fill = bg_color),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = grid_color),\n    axis.text = element_text(size = 12, color = axis_text_color),\n    axis.title.y = element_text(size = 14, color = axis_text_color),\n    axis.title.x = element_text(size = 14, color = axis_text_color),\n    plot.title = element_markdown(\n      family = \"Inter-Medium\",\n      color = plot_title_color,\n      size = 22,\n      margin = margin(0, 0, 0.5, 0, unit = \"line\")\n    ),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(\n      color = plot_title_color,\n      size = 12,\n      lineheight = 1.2,\n      margin = margin(0, 0, 1, 0, unit = \"line\")\n    ),\n    plot.margin = margin(1.5, 1.5, 1, 1.5, unit = \"line\"),\n    legend.position = c(0.9, 0.1),\n    plot.caption = element_text(colour = caption_color, size = 10)\n  ) +\n  labs(\n    x = \"Radius compared to the Sun\",\n    y = \"Mass compared to the Sun\",\n    title = \"Stars of the Kepler K2 Mission\",\n    subtitle = glue::glue(\"The Kepler K2 mission was a 5-year long expedition searching for Earth-like or smaller planets orbiting stars in our galaxy.<br />Kepler was a special-purpose spacecraft that precisely measured the light variations from thousands of distant stars, looking for planetary transits.<br />This graphic shows the relationship between the stellar mass & stellar radii of {n} stars, with a potential {pl_n} orbiting exoplanets.<br />It's fascinating to see two distinct clusters: stars with very high correlation between mass and radius, and those with radii > 3x our sun, <br /> with near-zero correlation between mass & radii.\",\n                          pl_n = scales::label_comma(accuracy = 10)(dat[,sum(pl_pnum)]),\n                          n = scales::label_comma(accuracy = 10)(dat %>% count(pl_hostname) %>% nrow())),\n    caption = \"DAY 13 - #30DayChartChallenge\\nDATASET BY @markmarkoh\\nPLOT BY @rsangole\"\n  )"
  },
  {
    "objectID": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html",
    "href": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html",
    "title": "Performance Benchmarking for Date-Time conversions",
    "section": "",
    "text": "Once more, there’s was an opportunity at work to optimize code and reduce run-time. The last time was for dummy-variable creation. Upon querying large data from our hive tables, the returned dataframe contains values of class character. Thus, everything has to be first type converted before any processing can be done.\nThe most time consuming of these has been character to date-time conversion for which I traditionally used base::as.POSIXct.\nUpon searching for some options myself, some help from Twitter, I’ve compared the performance of 6 functions on a 1e7 size character vector."
  },
  {
    "objectID": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#performance-comparison",
    "href": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#performance-comparison",
    "title": "Performance Benchmarking for Date-Time conversions",
    "section": "Performance comparison",
    "text": "Performance comparison\nI’ve run these benchmarks on my Macbook Pro:\n\nProcessor Name: Intel Core i5\nProcessor Speed: 2.4 GHz\nNumber of Processors: 1\nTotal Number of Cores: 2\nL2 Cache (per Core): 256 KB\nL3 Cache: 3 MB\nMemory: 8 GB"
  },
  {
    "objectID": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#packages-compared",
    "href": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#packages-compared",
    "title": "Performance Benchmarking for Date-Time conversions",
    "section": "Packages compared",
    "text": "Packages compared\n\nbase::strptime\nbase::as.POSIXct\nlubridate::parse_date_time\nlubridate::parse_date_time2 [fast C parser]\nlubridate::fast_strptime [fast C parser]\nfasttime::fastPOSIXct [fast C parser]\n\n\ntvec <- rep(as.character(Sys.time()+runif(1,-1e9,1e9)),1e7)\n\nstrp_fn <- function(tvec) strptime(tvec, format = '%Y-%m-%d %H:%M:%S', tz = 'UTC')\nPOSIX_fn <- function(tvec) as.POSIXct(tvec, format = '%Y-%m-%d %H:%M:%S', tz = 'UTC')\npdt_fn <- function(tvec) lubridate::parse_date_time(tvec, orders = 'Ymd H M S', tz = 'UTC')\npdt2_fn <- function(tvec) lubridate::parse_date_time2(tvec, orders = 'Ymd H M S', tz = 'UTC')\nfaststrp_fn <- function(tvec) lubridate::fast_strptime(tvec, format = '%Y-%m-%d %H:%M:%OS', tz = 'UTC')\nfasttime_fn <- function(tvec) fasttime::fastPOSIXct(x = tvec, tz = 'UTC', required.components = 6)\n\nbenchmarks <- microbenchmark::microbenchmark(\n    strptime = strp_fn(tvec),          asPOSIXct = POSIX_fn(tvec),\n    parse_date_time = pdt_fn(tvec),    parse_date_time2 = pdt2_fn(tvec),\n    fast_strptime = faststrp_fn(tvec), fastPOSIXct = fasttime_fn(tvec),\n    times = 30L, unit = 's')\n\nggplot2::autoplot(benchmarks, log = F)\nprint(benchmarks) ; print(benchmarks, unit = 'relative')"
  },
  {
    "objectID": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#results",
    "href": "posts/2018-04-12-performance-benchmarking-for-date-time-conversions/2018-04-12-performance-benchmarking-for-date-time-conversions.html#results",
    "title": "Performance Benchmarking for Date-Time conversions",
    "section": "Results",
    "text": "Results\n\n\n\n\n\nThe results are quite amazing. fastPOSIXct wins by a massive margin. as.POSIXct is 10x slower than fastPOSIXct and has a wider spread too.\n\n\nUnit: relative\n             expr       min        lq     mean    median       uq      max\n         strptime  7.140213  6.995050 6.587170  6.931937 6.243467 5.545282\n        asPOSIXct 10.888603 10.668598 9.886067 10.338859 9.127702 8.170841\n  parse_date_time  3.658794  3.700901 3.607782  3.840411 3.530341 3.136295\n parse_date_time2  1.973176  1.957485 1.864200  1.924281 1.753341 1.701600\n    fast_strptime  1.795452  1.753926 1.717556  1.772935 1.605809 1.588742\n      fastPOSIXct  1.000000  1.000000 1.000000  1.000000 1.000000 1.000000\n neval\n    30\n    30\n    30\n    30\n    30\n    30\n\n\nIf you run these bench marks on more powerful machines or larger datasets, share your results too. It’ll be interesting to see if this result scales.\nThanks Henning for the winning package suggestion! I owe you some beer!"
  },
  {
    "objectID": "posts/2021-01-06-tidytuesday-big-mac-index/2021-01-06-tidytuesday-big-mac-index.html",
    "href": "posts/2021-01-06-tidytuesday-big-mac-index/2021-01-06-tidytuesday-big-mac-index.html",
    "title": "TidyTuesday - Big Mac Index",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(ggdark)\nlibrary(ggtext)\n\nbigmac <- data.table::fread(\"big-mac.csv\")\n\nbigmac_us_dat <- bigmac %>%\n  dplyr::filter(currency_code == \"USD\") %>%\n  dplyr::select(date, bigmac_us_price_USD = local_price)\n\nbigmac_value_dat <- bigmac %>%\n  dplyr::select(date,\n                currency_code,\n                name,\n                bigmac_local_price = local_price,\n                dollar_ex) %>%\n  dplyr::left_join(\n    y = bigmac_us_dat,\n    on = date\n  )\n\nbigmac_value_dat <- bigmac_value_dat %>%\n  dplyr::mutate(\n    bigmac_ex_rate = bigmac_local_price / bigmac_us_price_USD,\n    valuation =  1 - bigmac_ex_rate / dollar_ex\n    )\n\nbigmac_value_dat %>%\n  dplyr::select(date, name, valuation) %>%\n  dplyr::filter(date %in% as.Date(c(\"2020-07-01\", \"2015-07-01\"))) %>%\n  tidyr::pivot_wider(names_from = date, values_from = valuation) %>%\n  janitor::clean_names() %>%\n  dplyr::mutate(movement = x2020_07_01 - x2015_07_01,\n                bw = ifelse(x2020_07_01 > x2015_07_01, \"Valuation worse in 5 years\", \"Valuation better in 5 years\"),\n                bw = ifelse(name == \"United States\", \"ref\", bw),\n                bw = factor(bw, levels = c(\"Valuation worse in 5 years\", \"Valuation better in 5 years\", \"ref\")),\n                name = forcats::fct_reorder(name, x2020_07_01),\n                label_pos = ifelse(bw == \"Valuation worse in 5 years\",\n                                   x2020_07_01 + 0.02,\n                                   x2020_07_01 - 0.02)) %>%\n  dplyr::arrange(movement)  %>%\n  tidyr::drop_na()-> to_plot\n\nglimpse(to_plot)\n\nRows: 42\nColumns: 6\n$ name        <fct> Sri Lanka, Czech Republic, India, Ukraine, Thailand, Singa…\n$ x2015_07_01 <dbl> 0.45409867, 0.40840293, 0.61738434, 0.67662319, 0.33855599…\n$ x2020_07_01 <dbl> 0.359218095, 0.334283358, 0.557499169, 0.619139345, 0.2857…\n$ movement    <dbl> -0.094880571, -0.074119572, -0.059885175, -0.057483840, -0…\n$ bw          <fct> Valuation better in 5 years, Valuation better in 5 years, …\n$ label_pos   <dbl> 0.33921810, 0.31428336, 0.53749917, 0.59913935, 0.26574757…\n\n\n\nto_plot %>%\n  ggplot(aes(y = name, color = bw)) +\n  geom_hline(yintercept = 3,\n             color = \"gray80\",\n             size = 0.3) +\n  geom_vline(xintercept = 0,\n             color = \"gray90\",\n             size = 0.2) +\n  geom_point(aes(x = x2015_07_01), color = \"gray70\", size = 1.5) +\n  geom_segment(aes(x = x2020_07_01, xend = x2015_07_01, yend = name), color = \"gray70\") +\n  geom_point(\n    data = to_plot %>% filter(name != \"United States\"),\n    mapping = aes(x = x2020_07_01),\n    size = 2\n  ) +\n  geom_text(\n    data = to_plot %>% filter(bw == \"Valuation worse in 5 years\"),\n    mapping = aes(x = label_pos, y = name, label = name),\n    size = 3.5,\n    hjust = 0,\n    show.legend = FALSE\n  ) +\n  geom_text(\n    data = to_plot %>% filter(bw == \"Valuation better in 5 years\"),\n    mapping = aes(x = label_pos, y = name, label = name),\n    size = 3.5,\n    hjust = 1,\n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = data.frame(\n      x = c(-0.5, -0.25, 0, 0.25, 0.50, 0.75),\n      y = c(3, 3, 3, 3, 3, 3)\n    ),\n    mapping = aes(\n      x = x,\n      y = y,\n      label = scales::label_percent()(x)\n    ),\n    size = 2.8,\n    color = \"gray30\",\n    hjust = 0.5,\n    label.size = 0,\n    label.padding = unit(0.2, \"lines\")\n  ) +\n  annotate(\n    geom = \"curve\",\n    xend = 0.674,\n    yend = 42.5,\n    x = 0.674 + 0.04,\n    y = 44,\n    curvature = .3,\n    arrow = arrow(length = unit(2, \"mm\")),\n    color = \"#e76f51\"\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 0.674 + 0.04 + 0.006,\n    y = 44,\n    label = \"2020\",\n    hjust = \"left\",\n    size = 3.4,\n    color = \"#e76f51\"\n  ) +\n  annotate(\n    geom = \"curve\",\n    xend = 0.563,\n    yend = 42.5,\n    x = 0.563 + 0.04,\n    y = 44,\n    curvature = .3,\n    arrow = arrow(length = unit(2, \"mm\")),\n    color = \"gray70\"\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 0.563 + 0.04 + 0.006,\n    y = 44,\n    label = \"2015\",\n    hjust = \"left\",\n    size = 3.4,\n    color = \"gray70\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(color = \"#264653\")\n  ) +\n  theme(\n    legend.position = c(0.14, 0.2),\n    legend.title = element_blank(),\n    legend.text = element_text(color = \"#264653\", size = 9.5)\n  ) +\n  scale_color_manual(values = c(\"#e76f51\", \"#2a9d8f\")) +\n  scale_x_continuous(\n    labels = scales::label_percent(),\n    breaks = c(-0.5, -0.25, 0, 0.25, 0.5, 0.75),\n    expand = c(0, 0.1)\n  ) +\n  scale_y_discrete(expand = c(-0.1, 7)) +\n  annotate(\n    geom = \"text\",\n    x = -0.3,\n    y = 44,\n    label = \"Over Valued\",\n    size = 3.4,\n    color = \"#264653\"\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 0.3,\n    y = 44,\n    label = \"Under Valued\",\n    size = 3.4,\n    color = \"#264653\"\n  ) +\n  labs(x = \"Big Mac Exchange Rate Valuation\", y = \"\")"
  },
  {
    "objectID": "posts/2021-07-19_enhance-etl-text-plots/2021-07-19-enhance-etl-text-plots.html",
    "href": "posts/2021-07-19_enhance-etl-text-plots/2021-07-19-enhance-etl-text-plots.html",
    "title": "Enhance ETL pipeline monitoring with text plots",
    "section": "",
    "text": "Will I use them to visualize my data? No. Are they useful tokeep an eye on your pipelines and quickly diagnose issues? Yes, I’ve been able to diagnose more than a few data quality spills very quickly because I had these rudimentary plots in my logfiles.\nSome examples…\n\nWhat’s the distribution of a variable?\n\ndat <- dplyr::starwars %>% \n  mutate(across(where(is.character), forcats::fct_infreq))\n\ndat %>%\n  tidyr::drop_na(height) %>% \n  pull(height) %>% \n  txtdensity(., xlab = \"starwars: height distribution\", width = 70, height = 12, pch = \"o\")\n\n      +-----------+--------------+-------oo----+-------------+-------+\n 0.02 +                                oooooo                        +\n      |                               oo    oo                       |\n0.015 +                              oo      o                       +\n      |                             oo        o                      |\n 0.01 +                             o         oo                     +\n      |                            o           oo                    |\n0.005 +         ooo              oo              ooo                 +\n      |  oooooooo oooooooooooooooo                 ooooooooo   ooo   |\n    0 +-----------+--------------+-------------+-----------ooooo-----+\n                 100            150           200           250       \n                       starwars: height distribution                  \n\n\n\n\nCounts of a factor?\n\ntxtbarchart(dat$sex, width = 70, height = 12, pch = \"x\")\n\n   +--x--------------+--------------+--------------+--------------+--+\n60 +  x                                                              +\n   |  x                                                              |\n   |  x                                                              |\n40 +  x                                                              +\n   |  x                                                              |\n20 +  x              x                                               +\n   |  x              x                                               |\n   |  x              x              x              x              x  |\n 0 +--x--------------x--------------x--------------x--------------x--+\n      1              2              3              4              5   \nLegend: 1=male, 2=female, 3=none, 4=hermaphroditic, 5=NA's\n\n\n\n\nBoxplots\n\ntxtboxplot(dat$height, dat$mass, width = 70, height = 12)\n\n 0            50            100           150           200           \n |-------------+-------------+-------------+-------------+-----------|\n                                                +---+--+              \n1                                       --------|   |  |---------     \n                                                +---+--+              \n                 +-----+-+                                            \n2     -----------|     | |----------                                  \n                 +-----+-+                                            \n     +                                                                \n3    |                                                                \n     +                                                                \nLegend: 1=dat$height, 2=dat$mass, 3=12\n\n\n\n\nTime series plots\n\ntxtplot(y = LakeHuron, x = 1875:1972, width = 70, height = 12, xlab = \"year\", ylab = \"level\")\n\n  582 +--*--+----------+-----------+----------+-----------+----------+\n      |      ****                                                    |\n  581 +  *****  *                       *            **              +\nl 580 +    *     ** *     ***    **                  * *       ***   +\ne 579 +           **  ****   * * ***     *      ****   *  *     *    +\nv     |             ** *     **    **   *       *       *  *  **     |\ne 578 +                       * *    * *     **     *   *  *  *      +\nl 577 +                               *  ** ** *         ** **       +\n      |                                    *                         |\n  576 +-----+----------+-----------+----------+-----------+-*--------+\n          1880       1900        1920       1940        1960          \n                                   year                               \n\n\nACF plots, though I’ve not used these in production yet.\n\ntxtacf(sunspot.year, width = 70, height = 12)\n\n  1 +--*--------------+-------------+--------------+--------------+--+\n    |  *                                                             |\n    |  *  *                                                          |\n    |  *  *                         *  *                             |\n0.5 +  *  *  *                   *  *  *  *                          +\n    |  *  *  *                   *  *  *  *                       *  |\n    |  *  *  *                 * *  *  *  *  *                    *  |\n  0 +  *  *  *  *  *  *  *  *  * *  *  *  *  *  *  *  *  *  *  *  *  +\n    |              *  *  *  *                   *  *  *  *  *        |\n    |              *  *  *                         *  *  *           |\n    +--+--------------*-------------+--------------+--------------+--+\n       0              5            10             15             20"
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#mran-time-machine",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#mran-time-machine",
    "title": "Reproducible Work in R",
    "section": "1. MRAN Time Machine",
    "text": "1. MRAN Time Machine\nMicrosoft R Application Network (MRAN) offers a “time machine”. This service takes a daily snapshot of the CRAN repository repository - going as far back as Sep 2014. You can browse the snapshots on their page:\n\nMRAN snapshots help us lock-down package versions using a date as the ‘index’. For example, running:\ninstall.packages(\"lattice\", \n                 repos = \"https://mran.microsoft.com/snapshot/2020-10-01\")\nwill install the version of {lattice} as of 1st Oct, 2020.\nNow, this approach doesn’t make it easier to choose specific versions of packages released over time, but instead allows you to lock down a date and get only those versions available on the selected date. What that means is that running “Update Packages” on any date after 1st of Oct won’t change your package configuration.\n> options(repos = \"https://mran.microsoft.com/snapshot/2020-10-01\")\n> getOption(\"repos\")\n[1] \"https://mran.microsoft.com/snapshot/2020-10-01\""
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-image",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-image",
    "title": "Reproducible Work in R",
    "section": "2. Docker Image",
    "text": "2. Docker Image\n\nThe Dockerfile\nThe Dockerfile holds the definition of how a docker image is built. The Dockerfile which I use to maintain this blog is saved here. It’s hosted on hub.docker.com too.\nHere’s a quick explanation of the file. For a deeper dive into Dockerfiles, there are lots of resources online 1, 2, 3.\n\nFROM\nI’m using the rocker/tidyverse:4.0.0 image, which offers a great starting point. It has R version 4.0.0 and the tidyverse packages preinstalled.\nFROM rocker/tidyverse:4.0.0\n\n\nRUN\nThis installs many of the linux libraries needed for the subsequent R packages to work. I’ve also installed some useful utility packages like curl, jq and vim.\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        libgit2-dev \\\n        libxml2-dev \\\n        ... \\\n        ... \\\n        curl \\\n        tree \\\n        jq \\\n        htop \\\n        texinfo \\\n        vim \\\n        man-db \\\n        less\n\n\nENV + R PKG INSTALL\nHere’s where I set the MRAN Build Date and then install the R packages I need using install2.r with the -r argument to point to MRAN Time Machine instead of CRAN.\nENV MRAN_BUILD_DATE=2020-09-01\n\n# Install Basic Utility R Packages\nRUN install2.r -r https://cran.microsoft.com/snapshot/${MRAN_BUILD_DATE} \\\n    --error \\\n    rgl \\\n    data.table \\\n    reprex \\\n    # ~ 30 more R Packages\n    ... \\\n    ... \\\n\n\nBuild & Push\nBuild the docker image and push it to hub.docker.com.\ndocker build . -t hatmatrix/blog:latest\ndocker push hatmatrix/blog:latest\nYour docker image is now avaiable online for anyone running your project to pull."
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#develop-in-docker",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#develop-in-docker",
    "title": "Reproducible Work in R",
    "section": "3. Develop in Docker",
    "text": "3. Develop in Docker\nNow I have a stable docker image to use for this blog. I can run the image with this shell cmd:\ndocker run \n    -d \n    -e PASSWORD=1234 \n    -v ~/github/:/home/rstudio/projects/ \n    -p 3838:3838 \n    -p 8787:8787 \n    hatmatrix/blog:latest\nThe components of this command are:\n\ndocker run : Run a docker image…\n-d : in detached mode, i.e. once the image is run in the background, you get your shell prompt\n-e PASSWORD=1234 : -e are additional arguments. Here, we’ve set the Rstudio password to 1234\n-v : this maps ~/github/ on my local machine to ~/home/rstudio/projects/ within the docker container\n-p : these args map ports from my local machine to ports within docker. We need one for rstudio (8787) and one for any shiny apps we launch from within rstudio (3838)\nhatmatrix/blog:latest : this is the name of the docker image\n\nThe importance of -v: Without -v you won’t have access to any of your local files within the docker container. Remember, docker containers are fully isolated from your local machine. Also, since containers are ephemeral (i.e. short-lived & temporary), once the container is shutdown, you will lose any data stored within it permanently. Mapping to a local folder allows you to work on projects stored locally within the container."
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-images-accompany-r-projects",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-images-accompany-r-projects",
    "title": "Reproducible Work in R",
    "section": "4. Docker images accompany R projects",
    "text": "4. Docker images accompany R projects\nJust create a /docker folder in your working project directory, and save your Dockerfile. Here’s my example for this blog: example docker folder. Optionally, create a docker-build.sh to save on some typing down the line."
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "",
    "text": "This is part one of the two part post related to Docker, PostgreSQL databases and Anomaly data-sets."
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#connecting-via-r",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#connecting-via-r",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "Connecting via R",
    "text": "Connecting via R\nUse postgres.R to test your connection. Run your DBI:: commands you would normally, except for one key difference.\nWhile making the connection, make sure the name of the host is the name of the database service you’ve chosen in docker-compose.yml. (Outside docker, you would have typically used localhost to connect to a local PostgreSQL server).\ncon <- DBI::dbConnect(\n  drv = RPostgres::Postgres(),\n  dbname = \"anomaly\",\n  host = \"db\", # this needs to be the name of the postgres service\n               # (line 3 in docker-compose.yml)\n  user = \"rahul\",\n  password = \"pass\",\n  port = 5432\n)\nThat’s it! You’re off to the races now. Use the DB as you normally would using {DBI}.\ncon %>% DBI::dbListTables()\ncon %>% dplyr::tbl(\"table_name\")"
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#to-stop-services",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#to-stop-services",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "To Stop Services",
    "text": "To Stop Services\nYou have two options here:\n\ndocker-compose stop will stop the services, which you can restart using docker-compose start.\ndocker-compose down will and remove containers as well. Run docker-compose up to get going once again."
  },
  {
    "objectID": "posts/2018-02-13-books-i-reference/2018-02-13-books-i-reference.html",
    "href": "posts/2018-02-13-books-i-reference/2018-02-13-books-i-reference.html",
    "title": "Books I Reference",
    "section": "",
    "text": "The full list of the books in my shelf is on my Goodreads account 1. The ones I refer to the most are listed here:\n\n\n\n \n  \n     \n     \n  \n \n\n  Deep Learning\n\n    Deep Learning with R \n    Francois Chollet \n  \n  \n    Handbook Of Neural Computing Applications \n    Alianna J Maren \n  \n  \n    Deep Learning \n    Ian Goodfellow \n  \n  \n    LSTM with Python \n    Jason Brownlee \n  \n  GLM\n\n    Generalized Additive Models: An Introduction with R, Second Edition \n    Simon Wood \n  \n  \n    Applied Regression Modeling \n    Iain Pardoe \n  \n  \n    Generalized Linear Models \n    John P. Hoffmann \n  \n  \n    Introduction to Linear Regression Analysis \n    Douglas Montgomery \n  \n  ML\n\n    Hands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems \n    Aurelien Geron \n  \n  \n    Flexible Imputation of Missing Data \n    Stef van Buuren \n  \n  \n    Applied Predictive Modeling \n    Max Kuhn \n  \n  \n    An Introduction to Statistical Learning: With Applications in R \n    Trevor Hastie \n  \n  Python\n\n    Python for Data Analysis \n    Wes McKinney \n  \n  \n    Introducing Python: Modern Computing in Simple Packages \n    Bill Lubanovic \n  \n  \n    Python Cookbook \n    David Beazley \n  \n  Quality\n\n    Multivariate Statistical Quality Control Using R \n    Edgar Santos \n  \n  \n    Statistics for Experimenters: Design, Innovation, and Discovery \n    George Box \n  \n  \n    Quality Control with R: An ISO Standards Approach \n    Emilio L Cano \n  \n  \n    Design and Analysis of Experiments with R \n    John Lawson \n  \n  R\n\n    R for Data Science: Import, Tidy, Transform, Visualize, and Model Data \n    Hadley Wickham \n  \n  \n    Testing R Code \n    Richard Cotton \n  \n  \n    R Packages \n    Hadley Wickham \n  \n  \n    Advanced R \n    Hadley Wickham \n  \n  Stats\n\n    The Essentials of Probability \n    Richard Durrett \n  \n  \n    Probability and Statistical Inference \n    Robert Hogg \n  \n  \n    Applied Multivariate Statistical Analysis \n    Richard Johnson \n  \n  \n    Multivariate Statistical Methods \n    Donald Morrison \n  \n  \n    Mathematical Statistics and Data Analysis \n    John Rice \n  \n  \n    Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data \n    Jason Osborne \n  \n  \n    Multivariable Modeling and Multivariate Analysis for the Behavioral Sciences \n    Brian Everitt \n  \n  Text\n\n    Natural Language Processing with Python \n    Steven Bird \n  \n  \n    Text Mining with R \n    Julia Silge \n  \n  Visualization\n\n    Information Dashboard Design: Displaying Data for At-a-Glance Monitoring \n    Stephen Few \n  \n  \n    Lattice: Multivariate Data Visualization with R \n    Deepayan Sarkar \n  \n  \n    ggplot2: Elegant Graphics for Data Analysis \n    Hadley Wickham \n  \n  \n    Data Visualisation: A Handbook for Data Driven Design \n    Andy Kirk \n  \n\n\n\n\n\n\n\n\nFootnotes\n\n\nMost of them are a result of my MSPA coursework. Others are from colleagues. I’m always searching for good literature to study from. If you have suggestions, please drop me a note.↩︎"
  },
  {
    "objectID": "posts/2021-08-18-making-an-anomaly-database/2021-08-18-making-an-anomaly-database.html",
    "href": "posts/2021-08-18-making-an-anomaly-database/2021-08-18-making-an-anomaly-database.html",
    "title": "Making the Anomaly Database",
    "section": "",
    "text": "This post describes how you populate the anomaly database built in Part 1.\n\nMotivation\nContinuing the theme of end-to-end reproducible workflows, I want to be able to recreate my raw database programmatically as well.\nAt the end of this activity, I’m able to quickly load and manage ~6G of data for my personal use.\n\nThe entire codebase for populating the database is in my GitHub repo.\n\n\nWho should read this?\nIf you’re comfortable using download.file(), system commands, arff file formats, and {DBI} you won’t learn much here. Read on if you’re curious about my approach.\n\n\nSteps\nOnly three files here:\n\nInitial Setup (00-execute.R)\nDownload data from the web (01-download-data.R)\nLoad data into anomaly database in Postgres (02-load-data-to-postgres.R)\n\n\nInitial Setup\nA list helps be keep track of the data sources, and helps me turn any downloads off to save on space/time (the Monash one is a ~2G download, for example).\ndatasets <- list(\n  ionosphere = TRUE,\n  nab = TRUE,\n  monash = TRUE, # 2G download, 6G uncompressed\n  ucr = TRUE\n)\nSome simple housekeeping to ensure directories are setup correctly. Furthermore, if the folder is git controlled, the directory which will house the datasets large_data needs to be in .gitignore. I check for this.\n# create large_data/ if does not exist\nif (!fs::dir_exists(here::here(\"large_data\"))) {\n  cli::cli_alert(\"{here::here('large_data')} does not exist\")\n  resp <-\n    usethis::ui_yeah(\n      \"Create {here::here('large_data')}?\",\n      yes = \"Y\",\n      no = \"N\",\n      shuffle = F\n    )\n  if (!resp)\n    stop()\n  fs::dir_create(here::here(\"large_data\"))\n}\n\n# git but no gitignore?\nif (fs::dir_exists(here::here(\".git\")) &\n    !fs::file_exists(here::here(\".gitignore\"))) {\n  cli::cli_alert_danger(\n    \"You have a git project, but no .gitignore. You must add {here::here('large_data')} to .gitignore since the data are massive.\"\n  )\n  stop()\n}\n\n# gitignore but large_data missing?\nif (fs::file_exists(here::here(\".gitignore\")) &\n    !any(grepl(\"large_data\", readLines(here::here(\".gitignore\"))))) {\n  cli::cli_alert_danger(\n    \"Your .gitignore does not have `large_data` specified. Add this to continue, since the data are massive.\"\n  )\n  stop()\n}\n\n\nDownload Data\nNow, for those datasets in the list above, simply download the data using download.file() for the selected datasets and move/unzip them to the large_data folder. I’m also checking if the folder already exists, and I’d like to overwrite it.\nHere’s an example for the UCR dataset. The code for the rest of the datasets is pretty similar.\nif(datasets$ucr){\n  DIR <- here::here(\"large_data/UCRArchive_2018\")\n  resp <- T\n  if(fs::dir_exists(DIR)){\n    resp <- usethis::ui_yeah(\"{DIR} already exists. Re-download data?\", \"Y\", \"N\", shuffle = F)\n    fs::dir_delete(here::here(\"large_data/UCRArchive_2018\"))\n  }\n  if(resp){\n    download.file(url = \"https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/UCRArchive_2018.zip\",\n                  destfile = here::here(\"large_data/UCRArchive_2018.zip\"))\n    system(command = glue::glue('unzip -P someone {here::here(\"large_data/UCRArchive_2018.zip\")} -d {here::here(\"large_data\")}'))\n    fs::file_delete(here::here(\"large_data/UCRArchive_2018.zip\"))\n  }\n}\n\n\nLoad Data\nNow, it’s as easy as:\n\nConnect to the postgres database using DBI::dbConnect\nRead a dataset from large_data/\nSimple cleanup (janitor::clean_names, all timestamp cols are called time etc)\nUse DBI::dbWriteTable to load the data into postgres\n\nHere’s an example codebase:\n# DB Connection ----\ncon <- DBI::dbConnect(\n  drv = RPostgres::Postgres(),\n  dbname = \"anomaly\",\n  host = \"db\",\n  user = \"rahul\",\n  password = \"pass\",\n  port = 5432\n)\n\nif (datasets$ionosphere) {\n  dat <-\n    read_csv(\"large_data/ionosphere/ionosphere.data\", col_names = F) %>%\n    rename(class = X35)\n  DBI::dbWriteTable(con, \"ionosphere\", dat)\n\n# Quick check\n  con %>% dplyr::tbl(\"ionosphere\")\n  \n}\nFor the monash dataset, you do need to use foreign::read.arff().\n\n\n\n\nTips\nLarge file downloads will timeout within the default timeout-window of 1 min. Handle this before calling download.file().\ntimeout.existing <- getOption(\"timeout\")\non.exit(options(timeout = timeout.existing))\noptions(timeout = 60*60)\nData dictionaries can be stored directly in the DB too. I store the contents of each README.md in the UCR_Archive2018/* folder in a data dictionary table called ucr_00_meta. This allows me to programatically call the dictionary in downstream development.\n\nShiny can be effectively used for quick exploration. Here’s an example of something I’m building for myself. The dashboard pulls data from PostgreSQL directly. UCR metadata is also pulled from the db rendered at the top of each page, making it quick to browse through the datasets. As I add more datasets, I keep expanding this dashboard."
  },
  {
    "objectID": "posts/2018-01-27-visualize-lda/2018-01-27-visualize-lda.html",
    "href": "posts/2018-01-27-visualize-lda/2018-01-27-visualize-lda.html",
    "title": "Visualising Linear Discriminant Analyses",
    "section": "",
    "text": "Quick visualization I’ve created as my 1st foray into R Shiny. Nothing earth shattering, but was helpful to learn the tool.\n\nHosted on shinyapps for free, at link\nGithub code here"
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html",
    "title": "Using tryCatch for robust R scripts",
    "section": "",
    "text": "Using tryCatch to write robust R code can be a bit confusing. I found the help file dry to read. There are some resources which explore tryCatch, linked below. Over the years, I have developed a few programming paradigms which I’ve repeatedly found useful. A quick introduction to tryCatch below, followed by three use-cases I use on a regular basis."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#hello-world-example",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#hello-world-example",
    "title": "Using tryCatch for robust R scripts",
    "section": "Hello World example",
    "text": "Hello World example\nThis is a toy example showing how a function can use tryCatch to handle execution.\n\nlog_calculator <- function(x){\n    tryCatch(\n        expr = {\n            message(log(x))\n            message(\"Successfully executed the log(x) call.\")\n        },\n        error = function(e){\n            message('Caught an error!')\n            print(e)\n        },\n        warning = function(w){\n            message('Caught an warning!')\n            print(w)\n        },\n        finally = {\n            message('All done, quitting.')\n        }\n    )    \n}\n\nIf x is a valid number, expr and finally are executed:\n\nlog_calculator(10)\n## 2.30258509299405\n## Successfully executed the log(x) call.\n## All done, quitting.\n\nIf x is an invalid number (negative, zero, NA), expr is attempted, and warning and finally are executed:\n\nlog_calculator(-10)\n## Caught an warning!\n## <simpleWarning in log(x): NaNs produced>\n## All done, quitting.\n\nIf x is an invalid entry which raises an error, expr is attempted, and error and finally are executed:\n\nlog_calculator(\"log_me\")\n## Caught an error!\n## <simpleError in log(x): non-numeric argument to mathematical function>\n## All done, quitting."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-within-loops",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-within-loops",
    "title": "Using tryCatch for robust R scripts",
    "section": "Use tryCatch within loops",
    "text": "Use tryCatch within loops\nThere are cases at work where I have quite large datasets to pre-process before model building can begin. The sources of these data can be varied and thus the quality of these data can vary. While each dataset should conform to our data quality standards (datatypes, data dictionaries, other domain-specific constraints), very often these isn’t the case. As a result, common data preprocessing functions might fail on few datasets. We can use tryCatch within the for loop to catch errors without breaking the loop.\nAnother toy example: Say, we have a nested dataframe of the mtcars data, nested on the cylinder numbers, and say, we had a few character values in mpg which is our response variable.\n\n# Example nested dataframe\ndf_nested <- split(mtcars, mtcars$cyl)\n\ndf_nested[[2]][c(4,5),\"mpg\"] <- \"a\"\ndf_nested\n## $`4`\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Toyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n## \n## $`6`\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4        21   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag    21   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Valiant           a   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Merc 280          a   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## \n## $`8`\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\nWe wish to run a few custom preprocessors, including taking the log of mpg.\n\nconvert_gear_to_factors <-\n  function(df) {\n    df %>% \n      mutate(gear = factor(gear, levels = 1:5, labels = paste0(\"Gear_\", 1:5)))\n  }\ntransform_response_to_log <-\n  function(df) {\n    df %>% mutate(log_mpg = log(mpg)) %>% select(-mpg)\n  }\n\nHow do we run our preprocessors over all the rows without error-ing out?\n\nfor (indx in 1:length(df_nested)) {\n    tryCatch(\n        expr = {\n            df_nested[[indx]] <-  df_nested[[indx]] %>% \n                convert_gear_to_factors() %>% \n                transform_response_to_log()\n            message(\"Iteration \", indx, \" successful.\")\n        },\n        error = function(e){\n            message(\"* Caught an error on itertion \", indx)\n            print(e)\n        }\n    )\n}\n## Iteration 1 successful.\n## * Caught an error on itertion 2\n## <error/dplyr:::mutate_error>\n## Error in `mutate()`:\n## ! Problem while computing `log_mpg = log(mpg)`.\n## Caused by error in `log()`:\n## ! non-numeric argument to mathematical function\n## ---\n## Backtrace:\n##   1. base::tryCatch(...)\n##  10. dplyr:::mutate.data.frame(., log_mpg = log(mpg))\n##  11. dplyr:::mutate_cols(.data, dplyr_quosures(...), caller_env = caller_env())\n##  13. mask$eval_all_mutate(quo)\n## Iteration 3 successful.\n\nWe’re able to handle the error on iteration 2, let the user know, and run the remaining iterations."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#catch-issues-early-log-progress-often",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#catch-issues-early-log-progress-often",
    "title": "Using tryCatch for robust R scripts",
    "section": "Catch issues early, log progress often",
    "text": "Catch issues early, log progress often\nAn important component of preparing ‘development’ code to be ‘production’ ready is implementation of good defensive programming and logging practices. I won’t go into details of either here, except to showcase the style of programs I have been writing to prepare code before it goes to our production cluster.\n\npreprocess_data <- function(df, x, b, ...){\n    message(\"-- Within preprocessor\")\n    df %>% \n        assertive::assert_is_data.frame() %>% \n        assertive::assert_is_non_empty()\n    x %>% \n        assertive::assert_is_numeric() %>% \n        assertive::assert_all_are_greater_than(3.14)\n    b %>% \n        assertive::assert_is_a_bool()\n    \n    # Code here...\n    # ....\n    # ....\n    \n    return(df)\n}\nbuild_model <- function(...){message(\"-- Building model...\")}\neval_model  <- function(...) {message(\"-- Evaluating model...\")}\nsave_model  <- function(...) {message(\"-- Saving model...\")}\n\nmain_executor <- function(...){\n    tryCatch(\n        expr = {\n            preprocess_data(df, x, b, more_args,...) %>% \n                build_model() %>% \n                eval_model() %>% \n                save_model()\n        },\n        error = function(e){\n            message('** ERR at ', Sys.time(), \" **\")\n            print(e)\n            write_to_log_file(e, logger_level = \"ERR\") #Custom logging function\n        },\n        warning = function(w){\n            message('** WARN at ', Sys.time(), \" **\")\n            print(w)\n            write_to_log_file(w, logger_level = \"WARN\") #Custom logging function\n        },\n        finally = {\n            message(\"--- Main Executor Complete ---\")\n        }\n    )\n}\n\nEach utility function starts with checking arguments. There are plenty of packages which allow run-time testing. My favorite one is assertive. It’s easy to read the code, and it’s pipe-able. Errors and warnings are handled using tryCatch - they are printed to the console if running in interactive mode, and then written to log files as well. I have written my own custom logging functions, but there are packages like logging and log4r which work perfectly fine."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-while-model-building",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-while-model-building",
    "title": "Using tryCatch for robust R scripts",
    "section": "Use tryCatch while model building",
    "text": "Use tryCatch while model building\ntryCatch is quite invaluable during model building. This is an actual piece of code I wrote for a kaggle competition as part of my midterm work at school. Github link here. The details of what’s going on isn’t important. At a high level, I was fitting stlf models using forecast for each shop, among 60 unique shop-ID numbers. For various reasons, for some shops, an stlf model could not be be fit, in which case a default seasonal naive model using snaive was to be used. tryCatch is a perfect way to handle such exceptions as shown below. I used a similar approach while building models at an “item” level: the number of unique items was in the 1000s; manually debugging one at a time is impossible. tryCatch allows us to programatically handle such situations.\n\nstlf_yhats <- vector(mode = 'list', length = length(unique_shops))\nfor (i in seq_along(unique_shops)) {\n    cat('\\nProcessing shop', unique_shops[i])\n    tr_data <- c6_tr %>% filter(shop_id == unique_shops[i])\n    tr_data_ts <-\n        dcast(\n          formula = yw ~ shop_id,\n          data = tr_data,\n          fun.aggregate = sum,\n          value.var = 'total_sales',\n          fill = 0\n        )\n    tr_data_ts <- ts(tr_data_ts[, -1], frequency = 52)\n\n    ##################\n    # <--Look here -->\n    fit <- tryCatch(\n      expr = {tr_data_ts %>% stlf(lambda = 'auto')},\n      error = function(e) { tr_data_ts %>% snaive()}\n      )\n    ##################\n  \n    fc <- fit %>% forecast(h = h)\n    stlf_yhats[[i]] <- as.numeric(fc$mean)\n    stlf_yhats[[i]] <- ifelse(stlf_yhats[[i]] < 0, 0, stlf_yhats[[i]])\n}\n\nHope this is useful to others learning tryCatch. Cheers."
  },
  {
    "objectID": "projects/2016-09-03-factor-analysis-of-personality-traits/2016-09-03-factor-analysis-of-personality-traits.html",
    "href": "projects/2016-09-03-factor-analysis-of-personality-traits/2016-09-03-factor-analysis-of-personality-traits.html",
    "title": "Factor Analysis of Personality Traits",
    "section": "",
    "text": "Objective\nThe objective of the original study, as stated by the article: ‘… it’s unlikely there are actually 32 different dimensions of personality; rather people probably vary along a small number of dimensions…’.\nMy objective:\n\nRecreate this study and see for myself: Can I replicate the results?\nHow many factors will I need to account for most of the variation?\nHow would I interpret the factors I find? How would these align to my expectations about overall personality traits and align ?\n\n\n\nDuplication of the Survey\nI re-created a version of Prof Malle’s survey on survey monkey. The survey is still open, so feel free to take it. At the point of writing this post, I had a total of 61 responses from my Facebook friends. It would be nice to match Prof Malle’s sample size of 240.\nThe survey asks:\n\n“How strongly do the following personality traits describe you?\nDistant, Talkative, Careless, Hardworking, Anxious, Agreeable, Tense, Kind, Relaxed, Disorganised, Outgoing, Approving, Shy, Disciplined, Harsh, Perseverant, Friendly, Worrying, Responsible, Contradictory, Sociable, Lazy, Cooperative, Quiet, Organized, Critical, Lax, Laidback, Withdrawn, Give up easily, Easy going.”\n\n\n\nResults\n\nHow do the traits relate to one another?\nA correlation plot with the 32 traits ordered using hierarchical clustering using Ward’s minimum variance agglomeration method. The corrplot package in R implements this method.\nThe X and Y axes of this plot are the 32 traits sorted according to Ward’s method. The sorting results in very intelligent grouping. The intensity of the colour and size of the colored square in each cell indicates the strength of Pearson’s correlation coefficient between each pair.\nI have highlighted 8 black rectangles which show grouping between similar traits. For example, the first box contains Worrying, Anxious, and Tense, traits very similar to one another. The third box contains Disciplined, Organized, Hardworking, and Responsible.\nThree red boxes highlight dissimilar or opposite traits. Take the first red box (top mid). On the X axis, we have Easy Going, Relaxed and Laidback, while on the Y axis, we have Worrying, Anxious, and Tense.\n\nThe groupings in the correlation plot & their magnitudes agree very well with Prof Malle’s findings.\n\n\n\nWhat does the factor analysis tell us?\nAgain, the objective here is: are there a few “factors” that explain personalities well as measured by the 32 personality traits? After varimax rotations on the factors, the factor plots show wonderful results!\nTo read the graphs, read the personality trait labels at the very extremes of the X and Y axes. These represent the traits with the highest loadings for each factor. The rotated factor plots were surprisingly intuitive to read.\nFactor 1 is Introvert/Extrovert. Factor 2 is the propensity to worry, or Anxiety Level.\n\nFactor 3 seems to be Friendliness (or Approachable vs Reserved).\n\nFactor 4 clearly is Organized vs Disorganized.\n\nFactor 5 can be defined as Conscientiousness, or the willingness to be perseverant, to do hard work.\n\n\n\nConclusion\n\nWe have been able to reduce 32 personality traits into 5 common traits:\n\nIntrovert/Extrovert\nPropensity for Anxiety\nFriendliness\nOrganized/Disorganized\nConscientiousness\n\nI was pleased to find that my result agreed very well with Prof Malle’s study. All in all, the factors extracted make intuitive sense when one thinks about overarching personality traits.\n\nHow exciting is factor analysis? :D\n\n\nHow many factors to select?\nThere are a couple of methods to select the number of factors to keep. A scree plot plots the eigenvalues of the correlation matrix of the manifest variables against the factor number. The nscree package can help get Scree plots.\n\n\nKaiser rule (select factors with eigenvalues > 1 for corr matrices) results in 9 factors to be selected.\nVisual inspection of the scree plot: The knee in the plot is at 4 factors. This is numerically computed as the Acceleration Factor (AF).\nOptimal Coordinates (OC) results in selection of 5 factors.\n\nAnother way to test the number of factors required is to look at the results of the chi-square test for the hypothesis test:\nH0: ‘n’ factors are sufficient Ha: More factors are needed\nFor the 5 factor model, the chi square statistic is 403.6 for 320 degrees of freedom. The p-value is 0.00104, so we can strongly reject the null hypothesis that 5 factors are sufficient. Increasing to the 9 factor model, the chi square statistic is 241 on 222 degrees of freedom. P-value is 0.18, so we fail to reject the null hypothesis.\nI’ve tested using 5 factors as well as 9 factors. The order of the factors and their interpretation change depending on how many factors are selected. This is unlike a PCA. Also, for the 9 factor case, the plots really do not reveal anything interesting beyond 5 factors.\n\n\nWhere’s the R code?\nThe raw data and the R code is stored on Github."
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "",
    "text": "This page replicates the codes written by Grun & Leish (2007) in ‘FlexMix: An R package for finite mixture modelling’, University of Wollongong, Australia. My intent here was to learn the flexmix package by replicating the results by the authors."
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Quick EDA",
    "text": "Quick EDA\nQuick look at the data itself. The dataframe consists of 2 elements - frequency (numeric vector), and the incidence matrix. There are total of 484 observations.\n\ndata(\"whiskey\")\ndf <- whiskey\nset.seed(1802)\nstr(df)\n\n'data.frame':   484 obs. of  2 variables:\n $ Freq     : int  1 1 10 14 10 23 9 8 1 12 ...\n $ Incidence: num [1:484, 1:21] 1 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:484] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:21] \"Singleton\" \"Knockando\" \"White Horse\" \"Scoresby Rare\" ...\n\n\nThe column names of the df$Incidence matrix are the brands of whiskey.\n\ncolnames(df$Incidence)\n\n [1] \"Singleton\"                  \"Knockando\"                 \n [3] \"White Horse\"                \"Scoresby Rare\"             \n [5] \"Ushers\"                     \"Macallan\"                  \n [7] \"Grant's\"                    \"Passport\"                  \n [9] \"Black & White\"              \"Clan MacGregor\"            \n[11] \"Ballantine\"                 \"Pinch (Haig)\"              \n[13] \"Other brands\"               \"Cutty Sark\"                \n[15] \"Glenfiddich\"                \"Glenlivet\"                 \n[17] \"J&B\"                        \"Dewar's White Label\"       \n[19] \"Johnnie Walker Black Label\" \"Johnnie Walker Red Label\"  \n[21] \"Chivas Regal\"              \n\n\nThe incidence matrix shows a relationship between two classes of variables - in this case: freqencies of the brand of whiskey in the past year, and the brand of whiskey itself. Quick look at a portion of the matrix:\n\ndf$Incidence[sample(x = 1:484,size = 10),sample(1:21,3)]\n\n    Macallan Cutty Sark Black & White\n119        0          1             0\n8          0          0             0\n245        0          0             1\n347        0          0             0\n26         0          1             0\n221        1          0             0\n1          0          0             0\n178        1          0             0\n411        0          0             0\n224        0          1             0\n\n\nThe popularity of the whiskeys can be seen here. Chivas Regal seems to be a favourite, which puts my personal preference in line with a larger population :)\n\nc <- colSums(df$Incidence)\nd1 <- data.frame(Brand=names(c),counts=c,row.names = NULL)\nd1 <- d1 %>% left_join(whiskey_brands) %>% arrange(-counts)\nggplot(d1,aes(reorder(Brand,counts),counts,fill=Type))+geom_bar(stat='identity')+coord_flip()+labs(y='Counts',x='Whiskey Brand')"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Model building",
    "text": "Model building\nThe first model in the paper is a stepped Flexmix model, specific for binary variables using the FLXMcvbinary() model. Since the objective is to cluster the model based on the Incidence counts and Frequencies, the formula used is Incidence ~ 1. The frequencies themselves are input as weights in the formula.\n\nwh_mix <- stepFlexmix(Incidence ~ 1,\n                      weights = ~ Freq, \n                      data = df,\n                      model = FLXMCmvbinary(truncated = TRUE),\n                      control = list(minprior = 0.005),\n                      k=1:7,\n                      nrep=5)\n\n1 : * * * * *\n2 : * * * * *\n3 : * * * * *\n4 : * * * * *\n5 : * * * * *\n6 : * * * * *\n7 : * * * * *\n\nsummary(wh_mix)\n\n     Length       Class        Mode \n          1 stepFlexmix          S4 \n\n\nA top model can be selecting using BIC or AIC criteria. The BIC criteria selects a model with 5 clusters.\n\nplot(BIC(wh_mix),type='b',ylab='BIC')\npoints(x = which.min(BIC(wh_mix)),min(BIC(wh_mix)),col='red',pch=20)\n\n\n\nwh_best <- getModel(wh_mix,'BIC')\nprint(wh_best)\n\n\nCall:\nstepFlexmix(Incidence ~ 1, weights = ~Freq, data = df, model = FLXMCmvbinary(truncated = TRUE), \n    control = list(minprior = 0.005), k = 5, nrep = 5)\n\nCluster sizes:\n  1   2   3   4   5 \n 24 262 794 161 977 \n\nconvergence after 127 iterations\n\n\nThe proportions of the observations in each cluster are shown here:\n\nround(prop.table(table(wh_best@cluster)),2)\n\n\n   1    2    3    4    5 \n0.04 0.25 0.19 0.23 0.29 \n\n\nThe parameter estimates plotted for model with k=5 is shown below graphically. Component 3 (4% of households) contain the largest number of different brands. Component 1 (25% of households) seen to prefer single malt whiskeys. Component 4 (23% of households) are across the board with Brands, but perhaps show lesser of an interest in single malts, just like Component 5 (29% of the households).\n\n# wh_best.prior <- prior(wh_best)\nwh_best.param <- parameters(wh_best)\nwh_best.param <- data.frame(Brand=stringr::str_replace(rownames(wh_best.param),pattern = 'center.',replacement = ''),\nwh_best.param,row.names = NULL) \nwh_best.param <- wh_best.param %>% gather(Components,Value,Comp.1:Comp.5)\nwh_best.param <- wh_best.param %>% left_join(y = whiskey_brands,by = 'Brand')\nggplot(wh_best.param,aes(y=Value,x=Brand,fill=Type))+\n    geom_bar(stat='identity')+\n    coord_flip()+\n    facet_grid(.~Components)"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda-1",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#quick-eda-1",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Quick EDA",
    "text": "Quick EDA\nThe dependant variable here is Patents. Independant variable is lgRD which is the log of R&D spending. The objective in this exercise is to try and find how many may clusters exist within this bi-variate dataset. When I started this exercise, it seemed quite moot to me, since visually, I couldn’t really tell any distict clusters. But, the results show otherwise.\n\ndata(\"patent\")\ndf_patent <- tbl_df(patent)\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nPlease use `tibble::as_tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\ndf_patent\n\n# A tibble: 70 × 4\n   Company                                    Patents    RDS   lgRD\n   <chr>                                        <int>  <dbl>  <dbl>\n 1 \"ABBOTT LABORATORIES                     \"      42 0.0549  4.09 \n 2 \"AFFILIATED HOSPITAL PRDS                \"       1 0.0032 -2.08 \n 3 \"ALBERTO-CULVER CO                       \"       3 0.0078  0.119\n 4 \"ALCON LABORATORIES                      \"       2 0.0803  1.88 \n 5 \"ALLERGAN PHARMACEUTICALS INC            \"       3 0.0686  1.10 \n 6 \"ALZA CORP-CL A                          \"      40 3.33    2.08 \n 7 \"AMERICAN HOME PRODUCTS CORP             \"      60 0.0243  4.10 \n 8 \"AMERICAN HOSPITAL SUPPLY                \"      30 0.0128  2.83 \n 9 \"AMERICAN STERILIZER CO                  \"       7 0.0252  1.39 \n10 \"AVON PRODUCTS                           \"       3 0.0094  2.60 \n# … with 60 more rows\n\nplot(Patents~lgRD,df_patent)"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building-1",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#model-building-1",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Model Building",
    "text": "Model Building\nThe paper mentions that Wang et al. (1998) chose a finite mixture of three Poisson regression models to represent the data. The FLXMRglm() is used for the Poisson model with a concomitant variable modeled using FLXPmultinom().\n\npat_mix <- flexmix(Patents ~ lgRD, k = 3, data = df_patent, model = FLXMRglm(family = \"poisson\"), concomitant = FLXPmultinom(~RDS))\npat_mix\n\n\nCall:\nflexmix(formula = Patents ~ lgRD, data = df_patent, k = 3, model = FLXMRglm(family = \"poisson\"), \n    concomitant = FLXPmultinom(~RDS))\n\nCluster sizes:\n 1  2  3 \n12 13 45 \n\nconvergence after 33 iterations\n\n\nThe clusters obtained from the analysis are given by a cluster() function.\n\nclusters(pat_mix)\n\n [1] 1 3 3 1 3 2 3 3 3 1 3 2 3 3 1 3 1 2 3 3 1 1 2 3 2 3 3 3 3 1 3 3 3 3 2 3 3 1\n[39] 2 3 3 3 3 2 3 3 1 3 2 3 3 1 3 3 3 3 3 1 3 3 3 3 2 3 2 2 3 3 2 3"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#results",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#results",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Results",
    "text": "Results\nThe data is replotted but with colors for the clusters and additional splines. As we can see, the model beautifully models three lines through three clusters in the data.\n\nComponents <- factor(clusters(pat_mix))\nxyplot(Patents~lgRD,groups = Components,df_patent,type=c('p','spline'))"
  },
  {
    "objectID": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#further-investigation",
    "href": "projects/2017-02-01-finite-mixture-modeling-using-flexmix/2017-02-01-finite-mixture-modeling-using-flexmix.html#further-investigation",
    "title": "Finite Mixture Modeling using Flexmix",
    "section": "Further investigation",
    "text": "Further investigation\nThe flexmix package has a function to plot rootograms of the posterior probabilities of observations. Observations where the a-posteriori probability is large for component #1 and #3 are indicated. As we can see where component #1 has highest probabilities indicated in the 1st bucket, they are lowest in #2 and #3 buckets.\n\nplot(pat_mix,mark=1)\n\n\n\nplot(pat_mix,mark=3)\n\n\n\n\nA summary of the mixture model results show the estimated priors, number of observations within each cluster (size), number of observations with p>10^-4 (post>0), and a ratio of the two. The rations of 0.58, 0.42 and 0.18 indicate big overlaps of the clusters. This can also be observed by the large portion of values in the mid-section of the rootogram above.\n\nsummary(pat_mix)\n\n\nCall:\nflexmix(formula = Patents ~ lgRD, data = df_patent, k = 3, model = FLXMRglm(family = \"poisson\"), \n    concomitant = FLXPmultinom(~RDS))\n\n       prior size post>0 ratio\nComp.1 0.201   12     48 0.250\nComp.2 0.184   13     47 0.277\nComp.3 0.615   45     63 0.714\n\n'log Lik.' -197.6752 (df=10)\nAIC: 415.3505   BIC: 437.8354 \n\n\nTests of significance of the coefficients are obtained by the refit(). In each cluster the intercept and lgRD are both statistically significant at the 0.05 level. The black bars in the plot are 95% CI over the point estimates.\n\nrm <- refit(pat_mix)\nsummary(rm)\n\n$Comp.1\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) -2.63638    0.62672 -4.2067 2.592e-05 ***\nlgRD         1.58644    0.13393 11.8454 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.2\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) 1.962173   0.176485  11.118 < 2.2e-16 ***\nlgRD        0.671908   0.045639  14.722 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.3\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) 0.508194   0.160927  3.1579  0.001589 ** \nlgRD        0.879702   0.040249 21.8564 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(rm,bycluster=F)\n\n\n\n\nThe authors note that “estimates vary between all components even though the co- efficients for lgRD are similar for the first and third component”."
  },
  {
    "objectID": "astronomy.html",
    "href": "astronomy.html",
    "title": "yHat",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "yHat",
    "section": "",
    "text": "Flexmix\n\n\nMixture Modeling\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2017\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFactor Analysis\n\n\nMultivariate\n\n\n\n\nHere, I replicate a paper end to end - survey design, data collection and analysis - by a professor from Brown Univ. The paper applies factor analysis to extract insights on respondant’s personality traits.\n\n\n\n\n\n\nSep 3, 2016\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "yHat",
    "section": "",
    "text": "Hello, my name is Rahul. This is a blog where I write about R and data science.\nI’m a data scientist at Apple where my work focuses on time-series, R in production and Shiny dashboarding. Before Apple, I worked as a data science manager at Cummins where I focused on developing time-series anomaly detection algorithms for engine health monitoring. In my former career, I worked as a mechanical engineer with a focus on computational stress, vibration and fatigue modeling.\nI have a Master’s degree in Predictive Analytics from Northwestern University and another in Mechanical Engineering from the University of Michigan."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "yHat",
    "section": "",
    "text": "Benchmarking\n\n\n\nWhich of the popular data read write methods is faster? Let’s find out.\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nPostgres\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nPostgres\n\n\n\nHow to setup a Docker based workflow for development in RStudio with a local Postgres server, also hosted in Docker\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\nQuick visualizations in command line using {txtplot}\n\n\n\nJul 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\nCorrelation plot for Kepler’s Planets, for day 13 of the 2021 30-day-chart-challenge\n\n\n\nApr 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nIs {fastDummies} any better than {stats} to create dummy variables? Let’s find out.\n\n\n\nDec 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nNotes from the M5 Forecasting Competition keynote speakers.\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nProgramming Practices\n\n\n\nA few ways I ensure my work is reproducible in R\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Practices\n\n\n\nA quick introduction to tryCatch below, followed by three use-cases I use on a regular basis.\n\n\n\nDec 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\nProgramming Practices\n\n\n\nI have 6 methods compete against each other to figure out the fastest way to convert characters to date-time for large datasets.\n\n\n\nApr 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\nA list of Data Science books I reference\n\n\n\nFeb 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nVisualization\n\n\n\nLinear Discriminant Analysis visualized using Shiny\n\n\n\nJan 27, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nHow do the four popular methods of creating dummy variables perform on large datasets? Let’s find out!\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Practices\n\n\n\nYou’ll learn how to use purrr, caret and list-cols to quickly create hundreds of dataset + model combinations, store data & model objects neatly in one tibble, and…\n\n\n\nSep 17, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  }
]