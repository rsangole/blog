[
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "",
    "text": "The M5 Forecasting Competiton recently finished, this past Jun 2020. On the 29th of Oct, M Open Forecasting Center (MOFC) held a virtual awards ceremony. I jotted down some notes for myself from the key note speakers."
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#spyros-makridakis-founder",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#spyros-makridakis-founder",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Spyros Makridakis / Founder",
    "text": "Spyros Makridakis / Founder\n\nHistory of forecasting competitions\n\nInitial forecasting work focused on simpler statistical models and valued human judgement (albeit with minimal improvement in forecasting accuracy). The participants for the first many forecasting competitions were largely statistians and the like.\nM3 competition was huge push forward, ~10% accuracy improvement\nM4 onwards, lots more machine learning solutions\n\n\n\n\nM5 Competition\n\nBest method is 22.4% more accurate than best statistical benchmark\nHigher levels of hierarchy show better improvement than lower levels\nMiddle of CI distribution shows most improvement; marginal/worse at tails\n\n\n\nWhat is the value of experience?\n\nWinner is an undergraduate student, has no experience in forecasting, and little/no experience in data science!\nWinner beat 7000+ masters & grand masters!\nWinning method LightGBM used by top 50 competitors\nEvidence that understanding the computational algorithms instead of forecasting itself is paying off"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#chris-fry-google",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#chris-fry-google",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Chris Fry / Google",
    "text": "Chris Fry / Google\n\nChris got involved by analyzing M4, it’s applicability to real world data sets. His critique and subsequent issues addressed in the M5 competition:\n\n\n\nSpoke of the challenge of single time-window submission:\n\nShort period of 28 days\nResults are subject to ‘uniqueness’ of test window\n@google, they do something a bit different:\n\nlarge sample for evaluation\nyear long window will cover all holidays & other events\nshould provide better stability in measurement of algorithm performance\n\n\n\n\n\n75% of score is across top 9 levels. Bottom three are disaggregated intermittent noizy series.\n\n\n\nPrice weighting : 90% of the weight came from 11% of the total series. Competiton winners can focus on left sliver and win… but businesses do care about the right (inventory, spoilage etc)."
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#brian-seaman-walmart",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#brian-seaman-walmart",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Brian Seaman / Walmart",
    "text": "Brian Seaman / Walmart\nWalmart sponsored the M5 competiton and offered their real world data for the competition.\n\n\nWhy Walmart cares about forecasts?\n\nInventory management, capacity planning, labor planning, financials, real estate, network capacity etc\n\nWalmart challenges\n\n10k Stores x 100k Items, 2m associates!\nVaried - Top sellers, Long Tails, Seasonal/Steady series\nNew stores - cold start challenges\n\nOpportunities\n\nScaling & better reliability by relying on Cloud based solutions\nDevelop relationships (product hierarchies), (product obsoluetlece, superceding)\nEvents like holidays\n\nM5 competition\n\nTried to keep datasets large without getting overwhelming\nIntermittent demand, short histories and discontinued items; but no cold-starts since you need to develop some relationship models to previous like-items\nItem relationships - product taxonomy + geography\nOthers - Events, pricing, weather"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#addison-howard-kaggle",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#addison-howard-kaggle",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Addison Howard / Kaggle",
    "text": "Addison Howard / Kaggle\n\nOverview of how Kaggle works\nBalance of expertize vs industry specific knowledge?\nYet again, Addison spoke about the need for expertize in a domain vs computational prowess given the development in AI/ML.\n\nThis as a very eye-opening revalition!\n\n\n\n\nHe spoke about how competitions are an excellent platform for pushing our collective wisdom to solve complex problems. Look at these error bars for the same problem Martin solved, when picked up by teams!\n\n - Some insights into the M5 competition:"
  },
  {
    "objectID": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#len-tashman-editor-of-foresight",
    "href": "posts/2020-10-29_m5-competition-notes/2020-10-29-m5-competition-notes.html#len-tashman-editor-of-foresight",
    "title": "M5 Competition Virtual Awards Ceremony",
    "section": "Len Tashman / Editor of Foresight",
    "text": "Len Tashman / Editor of Foresight\nComments about takeaways of M5:\n\nGood\n\nM5 was based on real data\nData were hierarchical, and dirty (intermittent, noisy)\nML is proving extremely valuable\n\nBut…\n\nML methods are still quite black-boxy. Business don’t inherently accept these models.\nFirms need to access how these methods improve their operational performance\nSpoke of firms which do not use any methodical forecasting at all. How can they be brought into the fold?\nHow can we find the balance between traditional statistical approaches and these new ML approches in this field?"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#tldr",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#tldr",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "tl;dr",
    "text": "tl;dr\n\n{stats} continues to dominate the speed tests\n{fastDummies} had similar speeds only for dataframes with rows ~1M\n{dummy} and {dummies} are the slowest"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#motivation",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#motivation",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Motivation",
    "text": "Motivation\nIn 2017, I compared the performance of four packages {stats}, {dummies}, {dummy} and {caret} to create dummy variables in this post.\nJacob Kaplan of UPenn has created a new package {fastdummies} which claims to be faster than other existing packages.\nLet’s test it out."
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#machine",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#machine",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Machine",
    "text": "Machine\nI’m running these tests on a 2019 MacBook Pro running macOS Catalina (10.15.7) on a 2.4 GHz 8-Core Intel i9 with 32 MB 2400 MHz DDR4, in a docker container running:\nplatform       x86_64-pc-linux-gnu         \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          4                           \nminor          0.0                         \nyear           2020                        \nmonth          04                          \nday            24                          \nsvn rev        78286                       \nlanguage       R                           \nversion.string R version 4.0.0 (2020-04-24)\nnickname       Arbor Day"
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#perf-testing",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#perf-testing",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Perf Testing",
    "text": "Perf Testing\n\nA quick test\nCreate a test dataset…\n\nNROW  <- 1e4\nfac_levels <- c(4, 4, 5, 5, 7, 7, 9, 9)\ninput_data <- tibble::tibble(\n    facVar_1 = as.factor(sample(LETTERS[1:fac_levels[1]], size = NROW, replace = TRUE)),\n    facVar_2 = as.factor(sample(LETTERS[1:fac_levels[2]], size = NROW, replace = TRUE)),\n    facVar_3 = as.factor(sample(LETTERS[1:fac_levels[3]], size = NROW, replace = TRUE)),\n    facVar_4 = as.factor(sample(LETTERS[1:fac_levels[4]], size = NROW, replace = TRUE)),\n    facVar_5 = as.factor(sample(LETTERS[1:fac_levels[5]], size = NROW, replace = TRUE)),\n    facVar_6 = as.factor(sample(LETTERS[1:fac_levels[6]], size = NROW, replace = TRUE)),\n    facVar_7 = as.factor(sample(LETTERS[1:fac_levels[7]], size = NROW, replace = TRUE)),\n    facVar_8 = as.factor(sample(LETTERS[1:fac_levels[8]], size = NROW, replace = TRUE))\n)\nstr(input_data)\n\ntibble [10,000 × 8] (S3: tbl_df/tbl/data.frame)\n $ facVar_1: Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 4 4 3 3 2 4 1 3 1 ...\n $ facVar_2: Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 4 1 4 4 4 4 4 3 3 4 ...\n $ facVar_3: Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 5 1 5 5 5 4 4 4 3 2 ...\n $ facVar_4: Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 2 3 1 1 5 5 1 4 2 3 ...\n $ facVar_5: Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 3 5 6 3 5 2 5 2 7 6 ...\n $ facVar_6: Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 5 2 1 3 3 5 5 6 2 2 ...\n $ facVar_7: Factor w/ 9 levels \"A\",\"B\",\"C\",\"D\",..: 3 6 7 5 8 8 8 8 3 8 ...\n $ facVar_8: Factor w/ 9 levels \"A\",\"B\",\"C\",\"D\",..: 5 8 7 3 6 2 7 8 8 2 ...\n\n\nRun microbenchmark…\n\nstats_fn <- function(dat) stats::model.matrix(~.-1,dat)\ndummies_fn <- function(dat) dummies::dummy.data.frame(as.data.frame(dat))\ndummy_fn <- function(dat) dummy::dummy(dat)\ncaret_fn <- function(dat) {caret::dummyVars(formula = ~.,data = dat) %>% predict(newdata = dat)}\nfastDummies_fn <- function(dat) fastDummies::dummy_cols(dat)\n\nmicrobenchmark::microbenchmark(\n    stats =       stats_fn(input_data),\n    dummies =     dummies_fn(input_data),\n    dummy =       dummy_fn(input_data),\n    caret =       caret_fn(input_data),\n    fastDummies = fastDummies_fn(input_data),\n    times = 10L\n    ) %>% autoplot()\n\n\n\n\nstats is still clearly the fastest of all the packages, for this moderately sized dataset.\n\n\nDig a bit deeper\nHow does the performance vary when rows, columns, or number of factors are scaled?\nFirst, make some functions to create dataframes with varying rows/cols/levels per variable, run benchmarks & extract median execution times.\n\nmake_data <- function(NROW = 10, NCOL = 5, NFAC = 5){\n    sapply(1:NCOL, \n           function(x) sample(LETTERS[1:NFAC], \n                              size = NROW, \n                              replace = TRUE)) %>% \n        as_tibble()\n    \n}\nrun_benchmark <- function(dat){\n    microbenchmark::microbenchmark(\n    stats =       stats_fn(dat),\n    dummies =     dummies_fn(dat),\n    dummy =       dummy_fn(dat),\n    caret =       caret_fn(dat),\n    fastDummies = fastDummies_fn(dat),\n    times = 10L\n    )\n}\nextract_median_time <- function(benchmarks){\n    as_tibble(benchmarks) %>% \n        dplyr::group_by(expr) %>% \n        summarize(median_ms = median(time) * 1e-6)\n}\n\nmake_data makes a pretty simple tibble:\n\nmake_data(NROW = 5, NCOL = 6, NFAC = 3)\n\n# A tibble: 5 × 6\n  V1    V2    V3    V4    V5    V6   \n  <chr> <chr> <chr> <chr> <chr> <chr>\n1 A     C     A     C     A     B    \n2 A     A     C     A     A     C    \n3 A     C     C     A     A     A    \n4 C     A     C     B     B     A    \n5 C     A     C     A     A     B    \n\n\n\n\nHow does performance scale by number of rows?\nstats still rocks. With very large datasets, fastDummies approaches similar speed.\n\nexperiment_rows <- tibble::tibble(\n    nrows = 10^(1:6)\n    ) %>% \n    dplyr::mutate(input_data = purrr::map(nrows, ~make_data(NROW = .x, NCOL = 5, NFAC = 5)),\n                  benchmarks = purrr::map(input_data, ~run_benchmark(.x)),\n                  median_times = purrr::map(benchmarks, ~extract_median_time(.x)))\nexperiment_rows %>% \n    dplyr::select(nrows, median_times) %>%\n    tidyr::unnest(cols = c(median_times)) %>%\n    dplyr::rename(Package = expr) %>% \n    tidyr::pivot_wider(names_from = Package, values_from = median_ms) %>% \n    dplyr::mutate(\n        dummies = dummies/stats,\n        dummy = dummy/stats,\n        caret = caret/stats,\n        fastDummies = fastDummies/stats,\n        stats = 1\n    ) %>%\n    tidyr::pivot_longer(-nrows) %>% \n    ggplot(aes(nrows, value, color = name)) +\n    geom_line() +\n    geom_point(aes(text = glue::glue(\"<b>{title}</b> {verb} {y}x\", \n                                     title = name, \n                                     verb = ifelse(name == \"stats\", \":\", \"slower by\"), \n                                     y = ifelse(value > 2,\n                                            round(value),\n                                            round(value, digits = 1))))) +\n    scale_y_log10(labels = scales::label_number(accuracy = 1, suffix = \"x\")) +\n    scale_x_log10(breaks = 10^(1:6), labels = scales::label_number_si()) +\n    labs(x = \"Number of Rows\", y = \"Relative Execution Rates\", \n         title = \"Row Performance (log-log scale)\") -> p\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\nHow does performance scale by number of columns?\nstats is the clear winner here.\n\n\n\n\n\n\n\n\nHow does performance scale by number of levels?\nInterestingly, number of levels per factor have little/no impact on performance for stats, caret and dummies. fastDummies & dummies show a positive correlation to levels."
  },
  {
    "objectID": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#conclusion",
    "href": "posts/2020-12-16-perf-benchmarking-dummy-variables-part-ii/2020-12-16-perf-benchmarking-dummy-variables-part-ii.html#conclusion",
    "title": "Perf Benchmarking Dummy Variables - Part II",
    "section": "Conclusion",
    "text": "Conclusion\nSee tl;dr"
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "",
    "text": "Very recently, at work, we got into a discussion about creation of dummy variables in R code. We were dealing with a fairly large dataset of roughly 500,000 observations for roughly 120 predictor variables. Almost all of them were categorical variables, many of them with a fairly large number of factor levels (think 20-100). The types of models we needed to investigate required creation of dummy variables (think xgboost). There are a few ways to convert categoricals into dummy variables in R. However, I did not find any comparison of performance for large datasets.\nSo here it goes."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#why-do-we-need-dummy-variables",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#why-do-we-need-dummy-variables",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Why do we need dummy variables?",
    "text": "Why do we need dummy variables?\nI won’t say any more here. Plenty of good resources on the web: here, here, and here."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#ways-to-create-dummy-variables-in-r",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#ways-to-create-dummy-variables-in-r",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Ways to create dummy variables in R",
    "text": "Ways to create dummy variables in R\nThese are the methods I’ve found to create dummy variables in R. I’ve explored each of these\n\nstats::model.matrix()\ndummies::dummy.data.frame()\ndummy::dummy()\n\ncaret::dummyVars()\n\nPrepping some data to try these out. Using the HairEyeColor dataset as an example. It consists of 3 categorical vars and 1 numerical var. Perfect to try things out. Adding a response variable Y too.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(magrittr)\ndata(\"HairEyeColor\")\nHairEyeColor %<>% tbl_df()\nHairEyeColor$Y = sample(c(0,1),dim(HairEyeColor)[1],replace = T) %>% factor(levels = c(0,1),labels = c('No','Yes'))\nglimpse(HairEyeColor)\n\nRows: 32\nColumns: 5\n$ Hair <chr> \"Black\", \"Brown\", \"Red\", \"Blond\", \"Black\", \"Brown\", \"Red\", \"Blond…\n$ Eye  <chr> \"Brown\", \"Brown\", \"Brown\", \"Brown\", \"Blue\", \"Blue\", \"Blue\", \"Blue…\n$ Sex  <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ n    <dbl> 32, 53, 10, 3, 11, 50, 10, 30, 10, 25, 7, 5, 3, 15, 7, 8, 36, 66,…\n$ Y    <fct> Yes, Yes, Yes, No, Yes, No, Yes, No, Yes, No, No, Yes, No, No, Ye…\n\n\nLet’s look at each package:\n\nstats package\nThe stats package has a function called model.matrix which converts factor variables to dummy variables. It also drops the response variable.\nSome pros\n\nWorks with tibbles\nReally fast\nRetains numerical columns as is\nFormula interface allows one to specify what Y is\n\nSome cons\n\nNeed to add the response Y back into the mix, if we need it\n\n\nhead(model.matrix(Y~.-1,HairEyeColor),3)\n\n  HairBlack HairBlond HairBrown HairRed EyeBrown EyeGreen EyeHazel SexMale  n\n1         1         0         0       0        1        0        0       1 32\n2         0         0         1       0        1        0        0       1 53\n3         0         0         0       1        1        0        0       1 10\n\n\n\n\ndummies package\ndummies has a command called dummy.data.frame which does the needful.\nSome pros\n\nRetains numerical columns as is\nCan create based dummy variables for numeric columns too\n\nSome cons\n\nDoesn’t work with tibbles\nDoesn’t have a formula interface to specify what Y is. Need to manually remove response variable from dataframe\n\n\nlibrary(dummies)\nhead(dummy.data.frame(data = as.data.frame(HairEyeColor),sep=\".\"),3)\n\n  Hair.Black Hair.Blond Hair.Brown Hair.Red Eye.Blue Eye.Brown Eye.Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye.Hazel Sex.Female Sex.Male  n Y.No Y.Yes\n1         0          0        1 32    0     1\n2         0          0        1 53    0     1\n3         0          0        1 10    0     1\n\n\n\n\ndummy package\ndummy creates dummy variables of all the factors and character vectors in a data frame. It also supports settings in which the user only wants to compute dummies for the categorical values that were present in another data set. This is especially useful in the context of predictive modeling, in which the new (test) data has more or other categories than the training data. 1\nSome pros\n\nWorks with tibbles\nRetains numerical columns as is\nCan create based dummy variables for numeric columns too\np parameter can select terms in terms of frequency\nCan grab only those variables in a separate dataframe\nCan create based dummy variables for numeric columns too\n\nSome cons\n\nDoesn’t have a formula interface to specify what Y is. Need to manually remove response variable from dataframe\n\n\nlibrary(dummy)\nhead(dummy(HairEyeColor),3)\n\n  Hair_Black Hair_Blond Hair_Brown Hair_Red Eye_Blue Eye_Brown Eye_Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye_Hazel Sex_Female Sex_Male Y_No Y_Yes\n1         0          0        1    0     1\n2         0          0        1    0     1\n3         0          0        1    0     1\n\n\nSide note: there’s a useful feature to grab all the categories in a factor variable.\n\ncategories(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Blond\" \"Brown\" \"Red\"  \n\n$Eye\n[1] \"Blue\"  \"Brown\" \"Green\" \"Hazel\"\n\n$Sex\n[1] \"Female\" \"Male\"  \n\n$Y\n[1] \"No\"  \"Yes\"\n\n\n\n\ncaret package\nLastly, there’s the caret package’s dummyVars(). This follows a different paradigm. First, we create reciepe of sorts, which just creates an object that specifies how the dataframe gets dummy-fied. Then, use the predict() to make the actual conversions.\nSome pros\n\nWorks on creating full rank & less than full rank matrix post-conversion\nHas a feature to keep only the level names in the final dummy columns\nCan directly create a sparse matrix\nRetains numerical columns as is\n\nSome cons\n\nY needs a factor\nIf the cateogical variables aren’t factors, you can’t use the sep=' ' feature\n\n\nlibrary(caret)\nHairEyeColor$Hair <- as.factor(HairEyeColor$Hair)\nHairEyeColor$Eye <- as.factor(HairEyeColor$Eye)\nHairEyeColor$Sex <- as.factor(HairEyeColor$Sex)\ndV <- dummyVars(formula = Y~.,data = HairEyeColor)\ndV\n\nDummy Variable Object\n\nFormula: Y ~ .\n5 variables, 4 factors\nVariables and levels will be separated by '.'\nA less than full rank encoding is used\n\n\n\nhead(predict(object = dV, newdata = HairEyeColor),3)\n\n  Hair.Black Hair.Blond Hair.Brown Hair.Red Eye.Blue Eye.Brown Eye.Green\n1          1          0          0        0        0         1         0\n2          0          0          1        0        0         1         0\n3          0          0          0        1        0         1         0\n  Eye.Hazel Sex.Female Sex.Male  n\n1         0          0        1 32\n2         0          0        1 53\n3         0          0        1 10"
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#performance-comparison",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#performance-comparison",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Performance comparison",
    "text": "Performance comparison\nI’ve run these benchmarks on my Macbook Pro with these specs:\n\nProcessor Name: Intel Core i5\nProcessor Speed: 2.4 GHz\nNumber of Processors: 1\nTotal Number of Cores: 2\nL2 Cache (per Core): 256 KB\nL3 Cache: 3 MB\nMemory: 8 GB\n\n\nSmaller datasets\nThe first dataset used is the HairEyeColor. 32 rows, 1 numeric var, 3 categorical var. All the resulting dataframes are as similar as possible… they all retain the Y variable at the end.\n\nlibrary(microbenchmark)\nHairEyeColor_df <- as.data.frame(HairEyeColor)\n\nstats_fn <- function(D){\n    stats::model.matrix(Y~.-1,D) %>% \n        cbind(D$Y)\n}\n\ndummies_fn <- function(D){\n    dummies::dummy.data.frame(D[,-5]) %>% \n        cbind(D$Y)\n}\n\ndummy_fn <- function(D){\n    dummy::dummy(D[,-5]) %>% \n        cbind(D$Y)\n}\n\ncaret_fn <- function(D){\n    dV <- caret::dummyVars(formula = Y~.,data = D)\n    predict(object = dV, newdata = D) %>% \n        cbind(D$Y)\n    }\n\nmicrobenchmark::microbenchmark(\n    stats = stats_fn(D = HairEyeColor),\n    dummies = dummies_fn(D = HairEyeColor_df),\n    dummy = dummy_fn(D = HairEyeColor),\n    caret = caret_fn(D = HairEyeColor),\n    times = 1000L,\n    control = list(order = 'block'),\n    unit = 's'\n    ) -> benchmarks\n\nautoplot(benchmarks)\n\n\n\n\nThe results speak for themself. The stats is clearly the fastest with dummies and caret being a more distant 2nd & 3rd.\n\n\nLarge datasets\nTo leverage a large dataset for this analysis, I’m using the Accident & Traffic Flow dataset, which is fairly big - 570,011 rows and 33 columns. I’ve narrowed down to 7 categorical variables to test the packages, and I’ve created a fake response variable as well.\n\ndata <- read_csv('~/github/github.com/blog-large-data/accidents_2005_to_2007.csv',progress = F)\ndata %<>%\n    transmute(\n        Day_of_Week = as.factor(Day_of_Week),\n        Road_Type = Road_Type %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        Weather = Weather_Conditions %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        RoadSurface = Road_Surface_Conditions %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        PedHC =  `Pedestrian_Crossing-Human_Control` %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        PedPF =  `Pedestrian_Crossing-Physical_Facilities` %>% stringr::str_replace_all('[()/ ]','.') %>% as.factor,\n        Year =  as.factor(Year)\n    ) %>% \n    mutate(\n        Y = sample(c(0,1),dim(data)[1],replace = T) %>% factor(levels = c(0,1),labels = c('No','Yes'))\n    )\ndim(data)\n\n[1] 570011      8\n\n\nIn total, there will be 39 dummy variable columns created for these 7 factor variables, as we can see here:\n\nmap_int(data,~length(levels(.x)))\n\nDay_of_Week   Road_Type     Weather RoadSurface       PedHC       PedPF \n          7           6           9           5           3           6 \n       Year           Y \n          3           2 \n\n\nNow for the benchmarks:\n\ndata_df <- as.data.frame(data)\nstats_fn <- function(D){\n    stats::model.matrix(Y~.-1,D) %>% \n        cbind(D$Y)\n}\n\ndummies_fn <- function(D){\n    dummies::dummy.data.frame(D[,-8]) %>% \n        cbind(D$Y)\n}\n\ndummy_fn <- function(D){\n    dummy::dummy(D[,-8]) %>% \n        cbind(D$Y)\n}\n\ncaret_fn <- function(D){\n    dV <- caret::dummyVars(formula = Y~.,data = D)\n    predict(object = dV, newdata = D) %>% \n        cbind(D$Y)\n    }\n\nmicrobenchmark::microbenchmark(\n    stats = stats_fn(D = data),\n    dummies = dummies_fn(D = data_df),\n    dummy = dummy_fn(D = data),\n    caret = caret_fn(D = data),\n    times = 30L,\n    control = list(order = 'block')\n    ) -> benchmarks\n\nautoplot(benchmarks)\n\n\n\n\nJust like before, stats is clerly the fastest."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#conclusion",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#conclusion",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Conclusion",
    "text": "Conclusion\n\nStick to stats::model.matrix(). It works with tibbles, it’s fast, and it takes a formula.\nIf you like the caret package and it’s interface, it’s the 2nd best choice.\ndummy or dummies doesn’t seem to offer any advantages to these packages."
  },
  {
    "objectID": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#qs",
    "href": "posts/2017-09-26_dummy-variables-one-hot-encoding/2017-09-26-dummy-variables-one-hot-encoding.html#qs",
    "title": "Performance Benchmarking for Dummy Variable Creation",
    "section": "Qs",
    "text": "Qs\n\nAre there other packages you recommend for dummy variable creation? If yes, please let me know in the comments.\nCould you run the bench marks on more powerful machines and larger datasets, and share your results? I’d like to append them here."
  },
  {
    "objectID": "posts/2021-07-19_enhance-etl-text-plots/2021-07-19-enhance-etl-text-plots.html",
    "href": "posts/2021-07-19_enhance-etl-text-plots/2021-07-19-enhance-etl-text-plots.html",
    "title": "Enhance ETL pipeline monitoring with text plots",
    "section": "",
    "text": "Will I use them to visualize my data? No. Are they useful tokeep an eye on your pipelines and quickly diagnose issues? Yes, I’ve been able to diagnose more than a few data quality spills very quickly because I had these rudimentary plots in my logfiles.\nSome examples…\n\nWhat’s the distribution of a variable?\n\ndat <- dplyr::starwars %>% \n  mutate(across(where(is.character), forcats::fct_infreq))\n\ndat %>%\n  tidyr::drop_na(height) %>% \n  pull(height) %>% \n  txtdensity(., xlab = \"starwars: height distribution\", width = 70, height = 12, pch = \"o\")\n\n      +-----------+--------------+-------oo----+-------------+-------+\n 0.02 +                                oooooo                        +\n      |                               oo    oo                       |\n0.015 +                              oo      o                       +\n      |                             oo        o                      |\n 0.01 +                             o         oo                     +\n      |                            o           oo                    |\n0.005 +         ooo              oo              ooo                 +\n      |  oooooooo oooooooooooooooo                 ooooooooo   ooo   |\n    0 +-----------+--------------+-------------+-----------ooooo-----+\n                 100            150           200           250       \n                       starwars: height distribution                  \n\n\n\n\nCounts of a factor?\n\ntxtbarchart(dat$sex, width = 70, height = 12, pch = \"x\")\n\n   +--x--------------+--------------+--------------+--------------+--+\n60 +  x                                                              +\n   |  x                                                              |\n   |  x                                                              |\n40 +  x                                                              +\n   |  x                                                              |\n20 +  x              x                                               +\n   |  x              x                                               |\n   |  x              x              x              x              x  |\n 0 +--x--------------x--------------x--------------x--------------x--+\n      1              2              3              4              5   \nLegend: 1=male, 2=female, 3=none, 4=hermaphroditic, 5=NA's\n\n\n\n\nBoxplots\n\ntxtboxplot(dat$height, dat$mass, width = 70, height = 12)\n\n 0            50            100           150           200           \n |-------------+-------------+-------------+-------------+-----------|\n                                                +---+--+              \n1                                       --------|   |  |---------     \n                                                +---+--+              \n                 +-----+-+                                            \n2     -----------|     | |----------                                  \n                 +-----+-+                                            \n     +                                                                \n3    |                                                                \n     +                                                                \nLegend: 1=dat$height, 2=dat$mass, 3=12\n\n\n\n\nTime series plots\n\ntxtplot(y = LakeHuron, x = 1875:1972, width = 70, height = 12, xlab = \"year\", ylab = \"level\")\n\n  582 +--*--+----------+-----------+----------+-----------+----------+\n      |      ****                                                    |\n  581 +  *****  *                       *            **              +\nl 580 +    *     ** *     ***    **                  * *       ***   +\ne 579 +           **  ****   * * ***     *      ****   *  *     *    +\nv     |             ** *     **    **   *       *       *  *  **     |\ne 578 +                       * *    * *     **     *   *  *  *      +\nl 577 +                               *  ** ** *         ** **       +\n      |                                    *                         |\n  576 +-----+----------+-----------+----------+-----------+-*--------+\n          1880       1900        1920       1940        1960          \n                                   year                               \n\n\nACF plots, though I’ve not used these in production yet.\n\ntxtacf(sunspot.year, width = 70, height = 12)\n\n  1 +--*--------------+-------------+--------------+--------------+--+\n    |  *                                                             |\n    |  *  *                                                          |\n    |  *  *                         *  *                             |\n0.5 +  *  *  *                   *  *  *  *                          +\n    |  *  *  *                   *  *  *  *                       *  |\n    |  *  *  *                 * *  *  *  *  *                    *  |\n  0 +  *  *  *  *  *  *  *  *  * *  *  *  *  *  *  *  *  *  *  *  *  +\n    |              *  *  *  *                   *  *  *  *  *        |\n    |              *  *  *                         *  *  *           |\n    +--+--------------*-------------+--------------+--------------+--+\n       0              5            10             15             20"
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "",
    "text": "This is part one of the two part post related to Docker, PostgreSQL databases and Anomaly data-sets."
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#connecting-via-r",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#connecting-via-r",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "Connecting via R",
    "text": "Connecting via R\nUse postgres.R to test your connection. Run your DBI:: commands you would normally, except for one key difference.\nWhile making the connection, make sure the name of the host is the name of the database service you’ve chosen in docker-compose.yml. (Outside docker, you would have typically used localhost to connect to a local PostgreSQL server).\ncon <- DBI::dbConnect(\n  drv = RPostgres::Postgres(),\n  dbname = \"anomaly\",\n  host = \"db\", # this needs to be the name of the postgres service\n               # (line 3 in docker-compose.yml)\n  user = \"rahul\",\n  password = \"pass\",\n  port = 5432\n)\nThat’s it! You’re off to the races now. Use the DB as you normally would using {DBI}.\ncon %>% DBI::dbListTables()\ncon %>% dplyr::tbl(\"table_name\")"
  },
  {
    "objectID": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#to-stop-services",
    "href": "posts/2021-08-03_docker-based-rstudio-postgres/2021-08-03-docker-based-rstudio-postgres.html#to-stop-services",
    "title": "Docker based RStudio & PostgreSQL",
    "section": "To Stop Services",
    "text": "To Stop Services\nYou have two options here:\n\ndocker-compose stop will stop the services, which you can restart using docker-compose start.\ndocker-compose down will and remove containers as well. Run docker-compose up to get going once again."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rahul S",
    "section": "",
    "text": "Hello, my name is Rahul. This is a blog where I write about R and data science.\nI’m a data scientist at Apple where my work focuses on time-series, R in production and Shiny dashboarding. Before Apple, I worked as a data science manager at Cummins where I focused on developing time-series anomaly detection algorithms for engine health monitoring. In my former career, I worked as a mechanical engineer with a focus on computational stress, vibration and fatigue modeling.\nI have a Master’s degree in Predictive Analytics from Northwestern University and another in Mechanical Engineering from the University of Michigan."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "yHat",
    "section": "",
    "text": "Making the Anomaly Database\n\n\n\nDocker\n\n\nPostgres\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker based RStudio & PostgreSQL\n\n\n\nDocker\n\n\nPostgres\n\n\n\nHow to setup a Docker based workflow for development in RStudio with a local Postgres server, also hosted in Docker\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnhance ETL pipeline monitoring with text plots\n\n\n\nVisualization\n\n\n\nQuick visualizations in command line using {txtplot}\n\n\n\nJul 19, 2021\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTidyTuesday - The Tate Collection\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTidyTuesday - Transit Costs\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTidyTuesday - Big Mac Index\n\n\n\nTidyTuesday\n\n\nVisualization\n\n\n\n\n\n\n\nJan 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerf Benchmarking Dummy Variables - Part II\n\n\n\nBenchmarking\n\n\n\nIs {fastDummies} any better than {stats} to create dummy variables? Let’s find out.\n\n\n\nDec 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM5 Competition Virtual Awards Ceremony\n\n\n\nNotes\n\n\n\nNotes from the M5 Forecasting Competition keynote speakers.\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Work in R\n\n\n\nDocker\n\n\nProgramming Practices\n\n\n\nA few ways I ensure my work is reproducible in R\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing tryCatch for robust R scripts\n\n\n\nProgramming Practices\n\n\n\nA quick introduction to tryCatch below, followed by three use-cases I use on a regular basis.\n\n\n\nDec 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBooks I Reference\n\n\n\nBooks\n\n\n\nA list of Data Science books I reference\n\n\n\nFeb 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance Benchmarking for Dummy Variable Creation\n\n\n\nBenchmarking\n\n\n\nHow do the four popular methods of creating dummy variables perform on large datasets? Let’s find out!\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-02-13-books-i-reference/2018-02-13-books-i-reference.html",
    "href": "posts/2018-02-13-books-i-reference/2018-02-13-books-i-reference.html",
    "title": "Books I Reference",
    "section": "",
    "text": "The full list of the books in my shelf is on my Goodreads account 1. The ones I refer to the most are listed here:\n\n\n\n \n  \n     \n     \n  \n \n\n  Deep Learning\n\n    Deep Learning with R \n    Francois Chollet \n  \n  \n    Handbook Of Neural Computing Applications \n    Alianna J Maren \n  \n  \n    Deep Learning \n    Ian Goodfellow \n  \n  \n    LSTM with Python \n    Jason Brownlee \n  \n  GLM\n\n    Generalized Additive Models: An Introduction with R, Second Edition \n    Simon Wood \n  \n  \n    Applied Regression Modeling \n    Iain Pardoe \n  \n  \n    Generalized Linear Models \n    John P. Hoffmann \n  \n  \n    Introduction to Linear Regression Analysis \n    Douglas Montgomery \n  \n  ML\n\n    Hands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems \n    Aurelien Geron \n  \n  \n    Flexible Imputation of Missing Data \n    Stef van Buuren \n  \n  \n    Applied Predictive Modeling \n    Max Kuhn \n  \n  \n    An Introduction to Statistical Learning: With Applications in R \n    Trevor Hastie \n  \n  Python\n\n    Python for Data Analysis \n    Wes McKinney \n  \n  \n    Introducing Python: Modern Computing in Simple Packages \n    Bill Lubanovic \n  \n  \n    Python Cookbook \n    David Beazley \n  \n  Quality\n\n    Multivariate Statistical Quality Control Using R \n    Edgar Santos \n  \n  \n    Statistics for Experimenters: Design, Innovation, and Discovery \n    George Box \n  \n  \n    Quality Control with R: An ISO Standards Approach \n    Emilio L Cano \n  \n  \n    Design and Analysis of Experiments with R \n    John Lawson \n  \n  R\n\n    R for Data Science: Import, Tidy, Transform, Visualize, and Model Data \n    Hadley Wickham \n  \n  \n    Testing R Code \n    Richard Cotton \n  \n  \n    R Packages \n    Hadley Wickham \n  \n  \n    Advanced R \n    Hadley Wickham \n  \n  Stats\n\n    The Essentials of Probability \n    Richard Durrett \n  \n  \n    Probability and Statistical Inference \n    Robert Hogg \n  \n  \n    Applied Multivariate Statistical Analysis \n    Richard Johnson \n  \n  \n    Multivariate Statistical Methods \n    Donald Morrison \n  \n  \n    Mathematical Statistics and Data Analysis \n    John Rice \n  \n  \n    Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data \n    Jason Osborne \n  \n  \n    Multivariable Modeling and Multivariate Analysis for the Behavioral Sciences \n    Brian Everitt \n  \n  Text\n\n    Natural Language Processing with Python \n    Steven Bird \n  \n  \n    Text Mining with R \n    Julia Silge \n  \n  Visualization\n\n    Information Dashboard Design: Displaying Data for At-a-Glance Monitoring \n    Stephen Few \n  \n  \n    Lattice: Multivariate Data Visualization with R \n    Deepayan Sarkar \n  \n  \n    ggplot2: Elegant Graphics for Data Analysis \n    Hadley Wickham \n  \n  \n    Data Visualisation: A Handbook for Data Driven Design \n    Andy Kirk \n  \n\n\n\n\n\n\n\n\nFootnotes\n\n\nMost of them are a result of my MSPA coursework. Others are from colleagues. I’m always searching for good literature to study from. If you have suggestions, please drop me a note.↩︎"
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html",
    "title": "Using tryCatch for robust R scripts",
    "section": "",
    "text": "Using tryCatch to write robust R code can be a bit confusing. I found the help file dry to read. There are some resources which explore tryCatch, linked below. Over the years, I have developed a few programming paradigms which I’ve repeatedly found useful. A quick introduction to tryCatch below, followed by three use-cases I use on a regular basis."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#hello-world-example",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#hello-world-example",
    "title": "Using tryCatch for robust R scripts",
    "section": "Hello World example",
    "text": "Hello World example\nThis is a toy example showing how a function can use tryCatch to handle execution.\n\nlog_calculator <- function(x){\n    tryCatch(\n        expr = {\n            message(log(x))\n            message(\"Successfully executed the log(x) call.\")\n        },\n        error = function(e){\n            message('Caught an error!')\n            print(e)\n        },\n        warning = function(w){\n            message('Caught an warning!')\n            print(w)\n        },\n        finally = {\n            message('All done, quitting.')\n        }\n    )    \n}\n\nIf x is a valid number, expr and finally are executed:\n\nlog_calculator(10)\n## 2.30258509299405\n## Successfully executed the log(x) call.\n## All done, quitting.\n\nIf x is an invalid number (negative, zero, NA), expr is attempted, and warning and finally are executed:\n\nlog_calculator(-10)\n## Caught an warning!\n## <simpleWarning in log(x): NaNs produced>\n## All done, quitting.\n\nIf x is an invalid entry which raises an error, expr is attempted, and error and finally are executed:\n\nlog_calculator(\"log_me\")\n## Caught an error!\n## <simpleError in log(x): non-numeric argument to mathematical function>\n## All done, quitting."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-within-loops",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-within-loops",
    "title": "Using tryCatch for robust R scripts",
    "section": "Use tryCatch within loops",
    "text": "Use tryCatch within loops\nThere are cases at work where I have quite large datasets to pre-process before model building can begin. The sources of these data can be varied and thus the quality of these data can vary. While each dataset should conform to our data quality standards (datatypes, data dictionaries, other domain-specific constraints), very often these isn’t the case. As a result, common data preprocessing functions might fail on few datasets. We can use tryCatch within the for loop to catch errors without breaking the loop.\nAnother toy example: Say, we have a nested dataframe of the mtcars data, nested on the cylinder numbers, and say, we had a few character values in mpg which is our response variable.\n\n# Example nested dataframe\ndf_nested <- split(mtcars, mtcars$cyl)\n\ndf_nested[[2]][c(4,5),\"mpg\"] <- \"a\"\ndf_nested\n## $`4`\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Toyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n## \n## $`6`\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4        21   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag    21   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Valiant           a   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Merc 280          a   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## \n## $`8`\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\nWe wish to run a few custom preprocessors, including taking the log of mpg.\n\nconvert_gear_to_factors <-\n  function(df) {\n    df %>% \n      mutate(gear = factor(gear, levels = 1:5, labels = paste0(\"Gear_\", 1:5)))\n  }\ntransform_response_to_log <-\n  function(df) {\n    df %>% mutate(log_mpg = log(mpg)) %>% select(-mpg)\n  }\n\nHow do we run our preprocessors over all the rows without error-ing out?\n\nfor (indx in 1:length(df_nested)) {\n    tryCatch(\n        expr = {\n            df_nested[[indx]] <-  df_nested[[indx]] %>% \n                convert_gear_to_factors() %>% \n                transform_response_to_log()\n            message(\"Iteration \", indx, \" successful.\")\n        },\n        error = function(e){\n            message(\"* Caught an error on itertion \", indx)\n            print(e)\n        }\n    )\n}\n## Iteration 1 successful.\n## * Caught an error on itertion 2\n## <error/dplyr:::mutate_error>\n## Error in `mutate()`:\n## ! Problem while computing `log_mpg = log(mpg)`.\n## Caused by error in `log()`:\n## ! non-numeric argument to mathematical function\n## ---\n## Backtrace:\n##   1. base::tryCatch(...)\n##  10. dplyr:::mutate.data.frame(., log_mpg = log(mpg))\n##  11. dplyr:::mutate_cols(.data, dplyr_quosures(...), caller_env = caller_env())\n##  13. mask$eval_all_mutate(quo)\n## Iteration 3 successful.\n\nWe’re able to handle the error on iteration 2, let the user know, and run the remaining iterations."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#catch-issues-early-log-progress-often",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#catch-issues-early-log-progress-often",
    "title": "Using tryCatch for robust R scripts",
    "section": "Catch issues early, log progress often",
    "text": "Catch issues early, log progress often\nAn important component of preparing ‘development’ code to be ‘production’ ready is implementation of good defensive programming and logging practices. I won’t go into details of either here, except to showcase the style of programs I have been writing to prepare code before it goes to our production cluster.\n\npreprocess_data <- function(df, x, b, ...){\n    message(\"-- Within preprocessor\")\n    df %>% \n        assertive::assert_is_data.frame() %>% \n        assertive::assert_is_non_empty()\n    x %>% \n        assertive::assert_is_numeric() %>% \n        assertive::assert_all_are_greater_than(3.14)\n    b %>% \n        assertive::assert_is_a_bool()\n    \n    # Code here...\n    # ....\n    # ....\n    \n    return(df)\n}\nbuild_model <- function(...){message(\"-- Building model...\")}\neval_model  <- function(...) {message(\"-- Evaluating model...\")}\nsave_model  <- function(...) {message(\"-- Saving model...\")}\n\nmain_executor <- function(...){\n    tryCatch(\n        expr = {\n            preprocess_data(df, x, b, more_args,...) %>% \n                build_model() %>% \n                eval_model() %>% \n                save_model()\n        },\n        error = function(e){\n            message('** ERR at ', Sys.time(), \" **\")\n            print(e)\n            write_to_log_file(e, logger_level = \"ERR\") #Custom logging function\n        },\n        warning = function(w){\n            message('** WARN at ', Sys.time(), \" **\")\n            print(w)\n            write_to_log_file(w, logger_level = \"WARN\") #Custom logging function\n        },\n        finally = {\n            message(\"--- Main Executor Complete ---\")\n        }\n    )\n}\n\nEach utility function starts with checking arguments. There are plenty of packages which allow run-time testing. My favorite one is assertive. It’s easy to read the code, and it’s pipe-able. Errors and warnings are handled using tryCatch - they are printed to the console if running in interactive mode, and then written to log files as well. I have written my own custom logging functions, but there are packages like logging and log4r which work perfectly fine."
  },
  {
    "objectID": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-while-model-building",
    "href": "posts/2018-08-02-try-catch/2018-08-02-try-catch.html#use-trycatch-while-model-building",
    "title": "Using tryCatch for robust R scripts",
    "section": "Use tryCatch while model building",
    "text": "Use tryCatch while model building\ntryCatch is quite invaluable during model building. This is an actual piece of code I wrote for a kaggle competition as part of my midterm work at school. Github link here. The details of what’s going on isn’t important. At a high level, I was fitting stlf models using forecast for each shop, among 60 unique shop-ID numbers. For various reasons, for some shops, an stlf model could not be be fit, in which case a default seasonal naive model using snaive was to be used. tryCatch is a perfect way to handle such exceptions as shown below. I used a similar approach while building models at an “item” level: the number of unique items was in the 1000s; manually debugging one at a time is impossible. tryCatch allows us to programatically handle such situations.\n\nstlf_yhats <- vector(mode = 'list', length = length(unique_shops))\nfor (i in seq_along(unique_shops)) {\n    cat('\\nProcessing shop', unique_shops[i])\n    tr_data <- c6_tr %>% filter(shop_id == unique_shops[i])\n    tr_data_ts <-\n        dcast(\n          formula = yw ~ shop_id,\n          data = tr_data,\n          fun.aggregate = sum,\n          value.var = 'total_sales',\n          fill = 0\n        )\n    tr_data_ts <- ts(tr_data_ts[, -1], frequency = 52)\n\n    ##################\n    # <--Look here -->\n    fit <- tryCatch(\n      expr = {tr_data_ts %>% stlf(lambda = 'auto')},\n      error = function(e) { tr_data_ts %>% snaive()}\n      )\n    ##################\n  \n    fc <- fit %>% forecast(h = h)\n    stlf_yhats[[i]] <- as.numeric(fc$mean)\n    stlf_yhats[[i]] <- ifelse(stlf_yhats[[i]] < 0, 0, stlf_yhats[[i]])\n}\n\nHope this is useful to others learning tryCatch. Cheers."
  },
  {
    "objectID": "posts/2021-08-18-making-an-anomaly-database/2021-08-18-making-an-anomaly-database.html",
    "href": "posts/2021-08-18-making-an-anomaly-database/2021-08-18-making-an-anomaly-database.html",
    "title": "Making the Anomaly Database",
    "section": "",
    "text": "This post describes how you populate the anomaly database built in Part 1.\n\nMotivation\nContinuing the theme of end-to-end reproducible workflows, I want to be able to recreate my raw database programmatically as well.\nAt the end of this activity, I’m able to quickly load and manage ~6G of data for my personal use.\n\nThe entire codebase for populating the database is in my GitHub repo.\n\n\nWho should read this?\nIf you’re comfortable using download.file(), system commands, arff file formats, and {DBI} you won’t learn much here. Read on if you’re curious about my approach.\n\n\nSteps\nOnly three files here:\n\nInitial Setup (00-execute.R)\nDownload data from the web (01-download-data.R)\nLoad data into anomaly database in Postgres (02-load-data-to-postgres.R)\n\n\nInitial Setup\nA list helps be keep track of the data sources, and helps me turn any downloads off to save on space/time (the Monash one is a ~2G download, for example).\ndatasets <- list(\n  ionosphere = TRUE,\n  nab = TRUE,\n  monash = TRUE, # 2G download, 6G uncompressed\n  ucr = TRUE\n)\nSome simple housekeeping to ensure directories are setup correctly. Furthermore, if the folder is git controlled, the directory which will house the datasets large_data needs to be in .gitignore. I check for this.\n# create large_data/ if does not exist\nif (!fs::dir_exists(here::here(\"large_data\"))) {\n  cli::cli_alert(\"{here::here('large_data')} does not exist\")\n  resp <-\n    usethis::ui_yeah(\n      \"Create {here::here('large_data')}?\",\n      yes = \"Y\",\n      no = \"N\",\n      shuffle = F\n    )\n  if (!resp)\n    stop()\n  fs::dir_create(here::here(\"large_data\"))\n}\n\n# git but no gitignore?\nif (fs::dir_exists(here::here(\".git\")) &\n    !fs::file_exists(here::here(\".gitignore\"))) {\n  cli::cli_alert_danger(\n    \"You have a git project, but no .gitignore. You must add {here::here('large_data')} to .gitignore since the data are massive.\"\n  )\n  stop()\n}\n\n# gitignore but large_data missing?\nif (fs::file_exists(here::here(\".gitignore\")) &\n    !any(grepl(\"large_data\", readLines(here::here(\".gitignore\"))))) {\n  cli::cli_alert_danger(\n    \"Your .gitignore does not have `large_data` specified. Add this to continue, since the data are massive.\"\n  )\n  stop()\n}\n\n\nDownload Data\nNow, for those datasets in the list above, simply download the data using download.file() for the selected datasets and move/unzip them to the large_data folder. I’m also checking if the folder already exists, and I’d like to overwrite it.\nHere’s an example for the UCR dataset. The code for the rest of the datasets is pretty similar.\nif(datasets$ucr){\n  DIR <- here::here(\"large_data/UCRArchive_2018\")\n  resp <- T\n  if(fs::dir_exists(DIR)){\n    resp <- usethis::ui_yeah(\"{DIR} already exists. Re-download data?\", \"Y\", \"N\", shuffle = F)\n    fs::dir_delete(here::here(\"large_data/UCRArchive_2018\"))\n  }\n  if(resp){\n    download.file(url = \"https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/UCRArchive_2018.zip\",\n                  destfile = here::here(\"large_data/UCRArchive_2018.zip\"))\n    system(command = glue::glue('unzip -P someone {here::here(\"large_data/UCRArchive_2018.zip\")} -d {here::here(\"large_data\")}'))\n    fs::file_delete(here::here(\"large_data/UCRArchive_2018.zip\"))\n  }\n}\n\n\nLoad Data\nNow, it’s as easy as:\n\nConnect to the postgres database using DBI::dbConnect\nRead a dataset from large_data/\nSimple cleanup (janitor::clean_names, all timestamp cols are called time etc)\nUse DBI::dbWriteTable to load the data into postgres\n\nHere’s an example codebase:\n# DB Connection ----\ncon <- DBI::dbConnect(\n  drv = RPostgres::Postgres(),\n  dbname = \"anomaly\",\n  host = \"db\",\n  user = \"rahul\",\n  password = \"pass\",\n  port = 5432\n)\n\nif (datasets$ionosphere) {\n  dat <-\n    read_csv(\"large_data/ionosphere/ionosphere.data\", col_names = F) %>%\n    rename(class = X35)\n  DBI::dbWriteTable(con, \"ionosphere\", dat)\n\n# Quick check\n  con %>% dplyr::tbl(\"ionosphere\")\n  \n}\nFor the monash dataset, you do need to use foreign::read.arff().\n\n\n\n\nTips\nLarge file downloads will timeout within the default timeout-window of 1 min. Handle this before calling download.file().\ntimeout.existing <- getOption(\"timeout\")\non.exit(options(timeout = timeout.existing))\noptions(timeout = 60*60)\nData dictionaries can be stored directly in the DB too. I store the contents of each README.md in the UCR_Archive2018/* folder in a data dictionary table called ucr_00_meta. This allows me to programatically call the dictionary in downstream development.\n\nShiny can be effectively used for quick exploration. Here’s an example of something I’m building for myself. The dashboard pulls data from PostgreSQL directly. UCR metadata is also pulled from the db rendered at the top of each page, making it quick to browse through the datasets. As I add more datasets, I keep expanding this dashboard."
  },
  {
    "objectID": "posts/2021-01-19-tidytuesday-the-tate-collection/2021-01-19-tidytuesday-the-tate-collection.html",
    "href": "posts/2021-01-19-tidytuesday-the-tate-collection/2021-01-19-tidytuesday-the-tate-collection.html",
    "title": "TidyTuesday - The Tate Collection",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidyr)\nlibrary(data.table)\nlibrary(beeswarm)\nlibrary(extrafont)\n# extrafont::font_import(paths = \".\", prompt = F)\n\nartists <- data.table::fread(\"artists.csv\") %>% \n  mutate(gender = ifelse(is.na(gender), \"Unknown\", gender),\n         gender = factor(gender,levels = c(\"Male\",\"Female\", \"Unknown\"), ordered = TRUE),\n         life_yr = yearOfDeath - yearOfBirth,\n         pre_1850 = yearOfBirth < 1850,\n         name_len = stringr::str_length(name) - 2) %>% \n  separate(col = \"placeOfBirth\", sep = \",\", into = c(\"birth_city\", \"birth_country\"), remove = F) %>% \n  separate(col = \"placeOfDeath\", sep = \",\", into = c(\"death_city\", \"death_country\"), remove = F) %>% \n  mutate(moved_countries = birth_country != death_country,\n         birth_country = ifelse(is.na(birth_country), \"Unknown\", birth_country),\n         death_country = ifelse(is.na(death_country), \"Unknown\", death_country))\nartwork <-\n    data.table::fread(\"artwork.csv\") %>%\n    mutate(\n        artistRole = as.factor(artistRole),\n        medium = as.factor(medium),\n        units = as.factor(units),\n        area = width * height,\n        title_len = stringr::str_length(title)\n    ) %>%\n    left_join(\n        y = artists %>% select(name, gender, yearOfBirth, birth_city, birth_country, life_yr),\n        by = c(\"artist\" = \"name\")\n    )\n\n\ncolor_pallete <- c(\"#005780\",\n                   \"#3e487a\",\n                   \"#955196\",\n                   \"#dd5182\",\n                   \"#ff6e54\",\n                   \"#ffa600\")\n\nartwork[,\n        medium_cleaned := case_when(\n            grepl(pattern = \"Graphite\", x = medium) ~ \"Graphite\",\n            grepl(pattern = \"Oil paint\", x = medium) ~ \"Oil Paint\",\n            grepl(pattern = \"Screenprint\", x = medium) ~ \"Screenprint\",\n            grepl(pattern = \"Watercolour\", x = medium) ~ \"Watercolour\",\n            grepl(pattern = \"photograph|Photograph\", x = medium) ~ \"Photograph\",\n            grepl(pattern = \"chalk|Chalk\", x = medium) ~ \"Chalk\"\n        )]\nartwork[,\n        color := case_when(\n            medium_cleaned %like% \"Graphite\" ~ color_pallete[1],\n            medium_cleaned %like% \"Paint\" ~ color_pallete[2],\n            medium_cleaned %like% \"Screenprint\" ~ color_pallete[6],\n            medium_cleaned %like% \"Watercolour\" ~ color_pallete[4],\n            medium_cleaned %like% \"Photograph\" ~ color_pallete[5],\n            medium_cleaned %like% \"Chalk\" ~ color_pallete[3]\n        )]\nmedium_dat_2 <- artwork[, .(year, medium_cleaned, color)]\n\nbees_plot <- medium_dat_2 %>%\n    filter(!is.na(medium_cleaned), !is.na(year)) %>%\n    arrange(year)\nbees_plot[, cutpts_numeric := cut(year, breaks = seq(1500, 2015, 5), labels = F)]\nbees_plot[, cutpts := cut(year, breaks = seq(1500, 2015, 5))]\nbees_plot[, xaxis := as.numeric(substr(cutpts, 2, 5))]\n\nbees_plot_reduced <-\n    bees_plot[, .N, .(xaxis, medium_cleaned, color, cutpts_numeric)]\nbees_plot_reduced[, num_pts := ceiling(N / 100)]\n\ndatlist <- list()\nfor (i in 1:nrow(bees_plot_reduced)) {\n    .nrows = bees_plot_reduced[i, num_pts]\n    .dlist <- list()\n    for (j in 1:.nrows) {\n        .dlist[[j]] <-\n            bees_plot_reduced[i, .(xaxis, medium_cleaned, color, cutpts_numeric)]\n    }\n    datlist[[i]] <- rbindlist(.dlist)\n}\n\nto_plot <- rbindlist(datlist)\nglimpse(to_plot)\n\nRows: 624\nColumns: 4\n$ xaxis          <dbl> 1540, 1555, 1560, 1565, 1570, 1575, 1585, 1590, 1595, 1…\n$ medium_cleaned <chr> \"Oil Paint\", \"Oil Paint\", \"Oil Paint\", \"Oil Paint\", \"Oi…\n$ color          <chr> \"#3e487a\", \"#3e487a\", \"#3e487a\", \"#3e487a\", \"#3e487a\", …\n$ cutpts_numeric <int> 9, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, …\n\n\n\nmake_plot <- function(to_plot) {\n    categories <- unique(to_plot[, .(medium_cleaned, color)])\n    xleg <- c(1.0,  2.8,  4.6,  6.8,  8.2, 10.0)\n    beeswarm(\n        xleg,\n        pwcol = categories$color,\n        horizontal = TRUE,\n        method = \"center\",\n        cex = 1.3,\n        pch = 19,\n        xlim = c(0, 12),\n        axes = FALSE\n    )\n    text(\n        x = xleg + 0.05,\n        y = 1,\n        labels = toupper(categories$medium_cleaned),\n        col = \"gray80\",\n        pos = 4,\n        cex = 0.9\n    )\n}\nmake_plot_2 <- function(to_plot) {\n    beeswarm(\n        x = to_plot$xaxis,\n        pwcol = to_plot$color,\n        horizontal = TRUE,\n        method = \"hex\",\n        spacing = 1.1,\n        cex = 1.5,\n        pch = 19,\n        xlim = c(1550, 2010),\n        axes = FALSE\n    )\n    x <- seq(from = 1550, to = 2000, by = 50)\n    axis(\n        side = 1,\n        labels = x,\n        at = x,\n        col = \"white\",\n        col.ticks = \"gray80\",\n        col.axis = \"gray60\"\n    )\n}\npar(\n    fig = c(0, 1, 0.7, 1),\n    new = TRUE,\n    bg = \"#292929\"\n    # family = \"Monoid\"\n)\nmake_plot(to_plot = to_plot)\npar(fig = c(0, 1, 0, 0.9), new = TRUE)\nmake_plot_2(to_plot = to_plot)\nmtext(\n    text = \"The Tate\",\n    side = 3,\n    cex = 2,\n    col = \"gray80\",\n    padj = -5\n)\nmtext(\n    text = \"THE TOP 6 MEDIUMS FOR 40,000 PIECES OF ART. EACH DOT REPRESENTS UPTO 100 ARTWORKS.\",\n    side = 3,\n    cex = 1,\n    col = \"gray80\",\n    padj = -5.5\n)\nmtext(\n    text = \"Image by @rsangole\",\n    side = 1,\n    cex = 0.9,\n    col = \"#3e487a\",\n    padj = 6,\n    adj = 1\n)"
  },
  {
    "objectID": "posts/2021-01-06-tidytuesday-big-mac-index/2021-01-06-tidytuesday-big-mac-index.html",
    "href": "posts/2021-01-06-tidytuesday-big-mac-index/2021-01-06-tidytuesday-big-mac-index.html",
    "title": "TidyTuesday - Big Mac Index",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(ggdark)\nlibrary(ggtext)\n\nbigmac <- data.table::fread(\"big-mac.csv\")\n\nbigmac_us_dat <- bigmac %>%\n  dplyr::filter(currency_code == \"USD\") %>%\n  dplyr::select(date, bigmac_us_price_USD = local_price)\n\nbigmac_value_dat <- bigmac %>%\n  dplyr::select(date,\n                currency_code,\n                name,\n                bigmac_local_price = local_price,\n                dollar_ex) %>%\n  dplyr::left_join(\n    y = bigmac_us_dat,\n    on = date\n  )\n\nbigmac_value_dat <- bigmac_value_dat %>%\n  dplyr::mutate(\n    bigmac_ex_rate = bigmac_local_price / bigmac_us_price_USD,\n    valuation =  1 - bigmac_ex_rate / dollar_ex\n    )\n\nbigmac_value_dat %>%\n  dplyr::select(date, name, valuation) %>%\n  dplyr::filter(date %in% as.Date(c(\"2020-07-01\", \"2015-07-01\"))) %>%\n  tidyr::pivot_wider(names_from = date, values_from = valuation) %>%\n  janitor::clean_names() %>%\n  dplyr::mutate(movement = x2020_07_01 - x2015_07_01,\n                bw = ifelse(x2020_07_01 > x2015_07_01, \"Valuation worse in 5 years\", \"Valuation better in 5 years\"),\n                bw = ifelse(name == \"United States\", \"ref\", bw),\n                bw = factor(bw, levels = c(\"Valuation worse in 5 years\", \"Valuation better in 5 years\", \"ref\")),\n                name = forcats::fct_reorder(name, x2020_07_01),\n                label_pos = ifelse(bw == \"Valuation worse in 5 years\",\n                                   x2020_07_01 + 0.02,\n                                   x2020_07_01 - 0.02)) %>%\n  dplyr::arrange(movement)  %>%\n  tidyr::drop_na()-> to_plot\n\nglimpse(to_plot)\n\nRows: 42\nColumns: 6\n$ name        <fct> Sri Lanka, Czech Republic, India, Ukraine, Thailand, Singa…\n$ x2015_07_01 <dbl> 0.45409867, 0.40840293, 0.61738434, 0.67662319, 0.33855599…\n$ x2020_07_01 <dbl> 0.359218095, 0.334283358, 0.557499169, 0.619139345, 0.2857…\n$ movement    <dbl> -0.094880571, -0.074119572, -0.059885175, -0.057483840, -0…\n$ bw          <fct> Valuation better in 5 years, Valuation better in 5 years, …\n$ label_pos   <dbl> 0.33921810, 0.31428336, 0.53749917, 0.59913935, 0.26574757…\n\n\n\nto_plot %>%\n  ggplot(aes(y = name, color = bw)) +\n  geom_hline(yintercept = 3, color = \"gray80\", size = 0.3) +\n  geom_vline(xintercept = 0, color = \"gray90\", size = 0.2) +\n  geom_point(aes(x = x2015_07_01), color = \"gray70\", size = 1.5) +\n  geom_segment(aes(x = x2020_07_01, xend = x2015_07_01, yend = name), color = \"gray70\") +\n  geom_point(data = to_plot %>% filter(name != \"United States\"),\n             mapping = aes(x = x2020_07_01), size = 2) +\n  geom_text(data = to_plot %>% filter(bw == \"Valuation worse in 5 years\"),\n            mapping = aes(x = label_pos, y = name, label = name),\n            size = 3.5, hjust = 0, show.legend = FALSE) +\n  geom_text(data = to_plot %>% filter(bw == \"Valuation better in 5 years\"),\n            mapping = aes(x = label_pos, y = name, label = name),\n            size = 3.5, hjust = 1, show.legend = FALSE) +\n  geom_label(data = data.frame(x = c(-0.5, -0.25, 0, 0.25, 0.50, 0.75), y = c(3, 3, 3, 3, 3, 3)),\n             mapping = aes(x = x, y = y, label = scales::label_percent()(x)),\n            size = 2.8, color = \"gray30\", hjust = 0.5,\n            label.size = 0, label.padding = unit(0.2, \"lines\")) +\n  annotate(geom = \"curve\", xend = 0.674, yend = 42.5, x = 0.674 + 0.04, y = 44,\n    curvature = .3, arrow = arrow(length = unit(2, \"mm\")), color = \"#e76f51\") +\n  annotate(geom = \"text\", x = 0.674 + 0.04 + 0.006, y = 44,\n           label = \"2020\", hjust = \"left\", size = 3.4, color = \"#e76f51\") +\n  annotate(geom = \"curve\", xend = 0.563, yend = 42.5, x = 0.563 + 0.04, y = 44,\n           curvature = .3, arrow = arrow(length = unit(2, \"mm\")), color = \"gray70\") +\n  annotate(geom = \"text\", x = 0.563 + 0.04 + 0.006, y = 44,\n           label = \"2015\", hjust = \"left\", size = 3.4, color = \"gray70\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.title.x = element_text(color = \"#264653\")) +\n  theme(legend.position = c(0.14, 0.2),\n        legend.title = element_blank(),\n        legend.text = element_text(color = \"#264653\", size = 9.5)) +\n  scale_color_manual(values = c(\"#e76f51\", \"#2a9d8f\")) +\n  scale_x_continuous(labels = scales::label_percent(),\n                     breaks = c(-0.5, -0.25, 0, 0.25, 0.5, 0.75),\n                     expand = c(0, 0.1)) +\n  scale_y_discrete(expand = c(-0.1, 7))+\n  annotate(geom = \"text\", x = -0.3, y = 44,\n           label = \"Over Valued\", size = 3.4, color = \"#264653\") +\n  annotate(geom = \"text\", x = 0.3, y = 44,\n           label = \"Under Valued\", size = 3.4, color = \"#264653\") +\n  labs(x = \"Big Mac Exchange Rate Valuation\", y = \"\")"
  },
  {
    "objectID": "posts/2021-01-11-tidytuesday-transit-costs/2021-01-11-tidytuesday-transit-costs.html",
    "href": "posts/2021-01-11-tidytuesday-transit-costs/2021-01-11-tidytuesday-transit-costs.html",
    "title": "TidyTuesday - Transit Costs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(ggdark)\nlibrary(ggtext)\n\ntransit_cost <- data.table::fread(\"transit_cost.csv\")\n\ntransit_cost <- transit_cost %>%\n  mutate(tunnel_per = as.numeric(str_replace(tunnel_per, \"%\", \"\")),\n         real_cost = as.numeric(real_cost),\n         country = ifelse(is.na(country), \"Unk\", country),\n         cc_id = paste(country,city))\n\ndat <- transit_cost %>%\n  group_by(country, city) %>%\n  summarise(\n    total_projects = n(),\n    total_stations = sum(stations, na.rm = TRUE),\n    total_tunnel_len = sum(tunnel, na.rm = TRUE),\n    total_len = sum(length, na.rm = TRUE),\n    tunnel_pc = total_tunnel_len/total_len,\n    total_cost = sum(real_cost, na.rm = TRUE) / 1e3, #now in Billions\n    avg_stations = mean(stations, na.rm = TRUE),\n    avg_tunnel_len = mean(tunnel, na.rm = TRUE),\n    avg_len = mean(length, na.rm = TRUE),\n    tunnel_pc = avg_tunnel_len/avg_len,\n    avg_cost = mean(real_cost, na.rm = TRUE)\n  ) %>%\n  filter(total_len < 10000,\n         total_projects > 1,\n         country %in% c(\"CN\", \"IN\")) %>%\n  mutate(cc_id = paste(country,city),\n         country = ifelse(country == \"CN\", \"China\", \"India\"))\nglimpse(dat)\n\nRows: 35\nColumns: 13\nGroups: country [2]\n$ country          <chr> \"China\", \"China\", \"China\", \"China\", \"China\", \"China\",…\n$ city             <chr> \"Beijing\", \"Changchun\", \"Changsha\", \"Chengdu\", \"Chong…\n$ total_projects   <int> 27, 7, 13, 11, 11, 7, 3, 10, 5, 11, 10, 8, 2, 5, 12, …\n$ total_stations   <int> 376, 81, 152, 152, 163, 66, 61, 121, 90, 154, 192, 10…\n$ total_tunnel_len <dbl> 450.0686, 95.5000, 164.5400, 225.1000, 156.3370, 151.…\n$ total_len        <dbl> 721.973, 116.000, 216.860, 252.950, 273.220, 166.940,…\n$ tunnel_pc        <dbl> 0.7650659, 0.8232759, 0.8219666, 0.8898992, 0.7867776…\n$ total_cost       <dbl> 138.86373, 16.92127, 38.33016, 44.59756, 41.87035, 25…\n$ avg_stations     <dbl> 13.925926, 11.571429, 11.692308, 13.818182, 14.818182…\n$ avg_tunnel_len   <dbl> 20.45766, 13.64286, 13.71167, 20.46364, 19.54213, 21.…\n$ avg_len          <dbl> 26.73974, 16.57143, 16.68154, 22.99545, 24.83818, 23.…\n$ avg_cost         <dbl> 5143.101, 2417.324, 2948.474, 4054.324, 3806.395, 364…\n$ cc_id            <chr> \"CN Beijing\", \"CN Changchun\", \"CN Changsha\", \"CN Chen…\n\n\n\nlabel_dat_india <- dat %>%\n  filter(country %in% \"India\",\n         city != \"Gurgaon\")\nlabel_dat_china <- dat %>%\n  filter(city %in% c(\"Shanghai\", \"Beijing\"))\n\nto_plot <- transit_cost %>%\n  tidyr::drop_na() %>%\n  filter(cc_id %in% dat$cc_id,\n         country %in% c(\"CN\", \"IN\"),\n         length < 100) %>%\n  mutate(country = ifelse(country == \"CN\", \"China\", \"India\"))\n\nglimpse(to_plot)\n\nRows: 192\nColumns: 21\n$ e                <int> 7288, 7289, 7290, 7291, 7296, 7297, 7298, 7299, 7304,…\n$ country          <chr> \"India\", \"India\", \"India\", \"India\", \"India\", \"India\",…\n$ city             <chr> \"Mumbai\", \"Mumbai\", \"Mumbai\", \"Mumbai\", \"Mumbai\", \"Mu…\n$ line             <chr> \"Monorail\", \"Line 3\", \"Line 2A\", \"Line 2B\", \"Line 4\",…\n$ start_year       <chr> \"2009\", \"2016\", \"2016\", \"2018\", \"2018\", \"2019\", \"2017…\n$ end_year         <chr> \"2019\", \"2022\", \"2021\", \"2023\", \"2022\", \"2022\", \"2024…\n$ rr               <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ length           <dbl> 20.20, 33.50, 18.60, 23.50, 32.30, 2.70, 24.90, 14.50…\n$ tunnel_per       <dbl> 0.00, 100.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.0…\n$ tunnel           <dbl> 0.000, 33.500, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ stations         <int> 17, 27, 17, 22, 32, 2, 16, 13, 16, 11, 4, 10, 17, 46,…\n$ source1          <chr> \"Plan\", \"Media\", \"Media\", \"Plan\", \"Plan\", \"Plan\", \"Pl…\n$ cost             <dbl> 30000.00, 320000.00, 64100.00, 109860.00, 145490.00, …\n$ currency         <chr> \"INR\", \"INR\", \"INR\", \"INR\", \"INR\", \"INR\", \"INR\", \"INR…\n$ year             <int> 2014, 2019, 2018, 2020, 2020, 2017, 2020, 2016, 2018,…\n$ ppp_rate         <dbl> 0.0540, 0.0470, 0.0480, 0.0470, 0.0470, 0.0480, 0.047…\n$ real_cost        <dbl> 1620.000, 15040.000, 3076.800, 5163.420, 6838.030, 37…\n$ cost_km_millions <dbl> 80.19802, 448.95522, 165.41935, 219.72000, 211.70372,…\n$ source2          <chr> \"Media\", \"Trade\", \"Media\", \"Plan\", \"Plan\", \"Plan\", \"P…\n$ reference        <chr> \"https://indianexpress.com/article/cities/mumbai/indi…\n$ cc_id            <chr> \"IN Mumbai\", \"IN Mumbai\", \"IN Mumbai\", \"IN Mumbai\", \"…\n\n\n\nin_color <- \"#2a9d8f\"\ncn_color <- \"#fca311\"\n\nchennai <- dat %>% filter(city == \"Chennai\")\nchennai_stations <- chennai$total_stations\nchennai_projects <- chennai$total_projects\nchennai_x <- chennai$avg_len\nchennai_y <- chennai$avg_stations\nchennai_cost <- chennai$total_cost\n\nto_plot %>%\n  ggplot(aes(length, stations)) +\n  geom_point(color = \"#8d99ae\", size = 0.8, show.legend = FALSE, alpha = 0.3) +\n  geom_smooth(data = dat %>% filter(city!=\"Wenzhou\"),\n              aes(avg_len, avg_stations, color = country),\n              se=FALSE, linetype=\"dashed\", size=0.3, method = \"lm\", span = 4) +\n  geom_point(data = dat,\n             aes(avg_len, avg_stations, color = country, size = total_cost^1.3),\n             pch = 19, alpha = 0.7) +\n  geom_text_repel(data = label_dat_india,\n                   aes(avg_len, avg_stations,\n                                   label = city,\n                                   color = country),\n                                   min.segment.length = 1,\n                   box.padding = unit(0.5, \"line\"),\n                   nudge_x = -1,\n                   show.legend = FALSE) +\n  annotate(geom = \"curve\",\n           xend = label_dat_china$avg_len[1], yend = label_dat_china$avg_stations[1],\n           x = label_dat_china$avg_len[1] + 6, y = label_dat_china$avg_stations[1] - 6,\n           curvature = .3, arrow = arrow(length = unit(0, \"mm\")), color = \"#fca311\") +\n  annotate(geom = \"text\",\n           x = label_dat_china$avg_len[1] + 5.5, y = label_dat_china$avg_stations[1] - 7,\n           label = \"Shanghai & Beijing\", hjust = \"left\", size = 3.4, color = \"#fca311\") +\n  annotate(geom = \"curve\",\n           xend = 2, yend = 0,\n           x = 2 + 7, y = 0 - 11,\n           curvature = -.3, arrow = arrow(length = unit(2, \"mm\")), color = \"#8d99ae\", alpha = 0.6) +\n  annotate(geom = \"text\",\n           x = 2 + 7.5, y = 0 - 11.5,\n           label = \"Each point is a transit line\", hjust = \"left\", size = 3.4, color = \"#8d99ae\") +\n  annotate(geom = \"curve\",\n           xend = chennai_x, yend = chennai_y + 2,\n           x = chennai_x, y = chennai_y + 16,\n           curvature = 0, arrow = arrow(length = unit(0, \"mm\")), color = in_color, alpha = 0.6) +\n  annotate(geom = \"text\",\n           x = chennai_x + 0.5, y = chennai_y + 13,\n           label = glue::glue(\n             \"{cost} over {stations} stations\n             in {chennai_projects} lines\",\n             x = scales::label_number(accuracy = 1, suffix = \" km\")(chennai_x),\n             y = chennai_y,\n             stations = chennai_stations,\n             chennai_projects = chennai_projects,\n             cost = scales::label_dollar(accuracy = 1, suffix = \"M\")(chennai_cost)\n             ),\n           hjust = \"left\", size = 3.4, color = in_color) +\n  dark_theme_minimal() +\n  scale_x_continuous(breaks = seq(10, 90, 20)) +\n  scale_y_continuous(breaks = seq(10, 70, 20)) +\n  scale_size(name = \"Total City Cost\",\n             breaks = c(20^1.3, 50^1.3, 80^1.3),\n             labels = c(\"$20M\", \"$50M\",\"$80M\"),\n             range = c(1,10)\n             ) +\n  coord_cartesian(ylim = c(0,90), clip = \"off\") +\n  labs(\n    title = \"The Cost of Transit in the 21<sup>st</sup> Century\",\n    subtitle = glue::glue(\"<span style='color:{cn_color};font-family:Inter-Medium;'>China: </span>253 projects in 28 cities, totaling $1T since 1998<br /><span style='color:{in_color};font-family:Inter-Medium;'>India: </span> 29 projects in 7 cities, totaling $2B since 2011<br /><span style='color:{in_color};font-family:Inter-Medium;'>India</span> has longer transit lines with more stations than <span style='color:{cn_color};font-family:Inter-Medium;'>China</span>, driving up costs <br />for each city.\"),\n    x = \"Average Length\",\n    y = \"Average Stations\",\n    caption = \"#tidytuesday\\n@rsangole\"\n    ) +\n  theme(\n    legend.title = element_text(size = 10, color = \"gray60\"),\n    legend.text =  element_text(size = 10, color = \"gray60\"),\n    axis.ticks.x.bottom = element_line(colour = \"gray30\",size = 0.5),\n    axis.ticks.y.left = element_line(colour = \"gray30\"),\n    axis.title.y = element_text(hjust = .9, size = 10, face = \"italic\", color = \"gray60\"),\n    axis.title.x = element_text(hjust = .9, size = 10, face = \"italic\", color = \"gray60\"),\n    plot.title = element_markdown(family = \"Inter-Medium\", color = \"#f8f8f2\", size = 22,\n                                  margin = margin(0, 0, 0.5, 0, unit = \"line\")),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(color = \"#f8f8f2\", size = 12, lineheight = 1.2,\n                                     margin = margin(0, 0, 1, 0, unit = \"line\")),\n    plot.margin = margin(1.5, 1.5, 1, 1.5, unit = \"line\"),\n    legend.position = c(0.9,0.1)\n    ) +\n  scale_discrete_manual(aesthetics = \"color\",\n                        values = c(\"India\" = in_color, \"China\" = cn_color),\n                        guide = F)"
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#mran-time-machine",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#mran-time-machine",
    "title": "Reproducible Work in R",
    "section": "1. MRAN Time Machine",
    "text": "1. MRAN Time Machine\nMicrosoft R Application Network (MRAN) offers a “time machine”. This service takes a daily snapshot of the CRAN repository repository - going as far back as Sep 2014. You can browse the snapshots on their page:\n\nMRAN snapshots help us lock-down package versions using a date as the ‘index’. For example, running:\ninstall.packages(\"lattice\", \n                 repos = \"https://mran.microsoft.com/snapshot/2020-10-01\")\nwill install the version of {lattice} as of 1st Oct, 2020.\nNow, this approach doesn’t make it easier to choose specific versions of packages released over time, but instead allows you to lock down a date and get only those versions available on the selected date. What that means is that running “Update Packages” on any date after 1st of Oct won’t change your package configuration.\n> options(repos = \"https://mran.microsoft.com/snapshot/2020-10-01\")\n> getOption(\"repos\")\n[1] \"https://mran.microsoft.com/snapshot/2020-10-01\""
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-image",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-image",
    "title": "Reproducible Work in R",
    "section": "2. Docker Image",
    "text": "2. Docker Image\n\nThe Dockerfile\nThe Dockerfile holds the definition of how a docker image is built. The Dockerfile which I use to maintain this blog is saved here. It’s hosted on hub.docker.com too.\nHere’s a quick explanation of the file. For a deeper dive into Dockerfiles, there are lots of resources online 1, 2, 3.\n\nFROM\nI’m using the rocker/tidyverse:4.0.0 image, which offers a great starting point. It has R version 4.0.0 and the tidyverse packages preinstalled.\nFROM rocker/tidyverse:4.0.0\n\n\nRUN\nThis installs many of the linux libraries needed for the subsequent R packages to work. I’ve also installed some useful utility packages like curl, jq and vim.\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        libgit2-dev \\\n        libxml2-dev \\\n        ... \\\n        ... \\\n        curl \\\n        tree \\\n        jq \\\n        htop \\\n        texinfo \\\n        vim \\\n        man-db \\\n        less\n\n\nENV + R PKG INSTALL\nHere’s where I set the MRAN Build Date and then install the R packages I need using install2.r with the -r argument to point to MRAN Time Machine instead of CRAN.\nENV MRAN_BUILD_DATE=2020-09-01\n\n# Install Basic Utility R Packages\nRUN install2.r -r https://cran.microsoft.com/snapshot/${MRAN_BUILD_DATE} \\\n    --error \\\n    rgl \\\n    data.table \\\n    reprex \\\n    # ~ 30 more R Packages\n    ... \\\n    ... \\\n\n\nBuild & Push\nBuild the docker image and push it to hub.docker.com.\ndocker build . -t hatmatrix/blog:latest\ndocker push hatmatrix/blog:latest\nYour docker image is now avaiable online for anyone running your project to pull."
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#develop-in-docker",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#develop-in-docker",
    "title": "Reproducible Work in R",
    "section": "3. Develop in Docker",
    "text": "3. Develop in Docker\nNow I have a stable docker image to use for this blog. I can run the image with this shell cmd:\ndocker run \n    -d \n    -e PASSWORD=1234 \n    -v ~/github/:/home/rstudio/projects/ \n    -p 3838:3838 \n    -p 8787:8787 \n    hatmatrix/blog:latest\nThe components of this command are:\n\ndocker run : Run a docker image…\n-d : in detached mode, i.e. once the image is run in the background, you get your shell prompt\n-e PASSWORD=1234 : -e are additional arguments. Here, we’ve set the Rstudio password to 1234\n-v : this maps ~/github/ on my local machine to ~/home/rstudio/projects/ within the docker container\n-p : these args map ports from my local machine to ports within docker. We need one for rstudio (8787) and one for any shiny apps we launch from within rstudio (3838)\nhatmatrix/blog:latest : this is the name of the docker image\n\nThe importance of -v: Without -v you won’t have access to any of your local files within the docker container. Remember, docker containers are fully isolated from your local machine. Also, since containers are ephemeral (i.e. short-lived & temporary), once the container is shutdown, you will lose any data stored within it permanently. Mapping to a local folder allows you to work on projects stored locally within the container."
  },
  {
    "objectID": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-images-accompany-r-projects",
    "href": "posts/2020-10-10-reproducible-work-in-R/2020-10-10-reproducible-work-in-R.html#docker-images-accompany-r-projects",
    "title": "Reproducible Work in R",
    "section": "4. Docker images accompany R projects",
    "text": "4. Docker images accompany R projects\nJust create a /docker folder in your working project directory, and save your Dockerfile. Here’s my example for this blog: example docker folder. Optionally, create a docker-build.sh to save on some typing down the line."
  }
]