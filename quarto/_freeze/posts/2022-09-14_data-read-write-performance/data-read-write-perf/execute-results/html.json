{
  "hash": "cca77bbfd5b16de9180ba67e340176db",
  "result": {
    "markdown": "---\ntitle: Performance Benchmarking Data Read Write\ndate: '2022-09-17'\ndescription: Which of the popular data read write methods is faster? Let's find out.\nimage: _image.jpg\ncategories:\n  - Benchmarking\n---\n\n\n\n\n# Background\n\nIn a recent conversation, I was asked how to speed up execution of scripts dealing with computation on big data sets (biggish i.e. in-memory). The scripts had some data read and write features with some ETL sandwiched in between. Upon reviewing the code base, I found some immediate improvements by moving a bunch of ETL code from `{dplyr}` to `{data.table}`, which has been my defacto go to for large data sets.\n\nOn the data read/write side of the equation, I used to be an ardent `{qs}` user with intermittent forays into `{data.table::fread/fwrite}` functions. However, recently, I've switched majority of my work to `{arrow}` which has proven itself a strong ally in the war against slow data transaction times.\n\nThe question remains - which one works better? Particularly, which one works better on the machines I'm using as well as the server I have access to? My configuration has changed from my last performance review posting. I'm now running:\n\n-   MacBook Pro, running MacOS 12.5\n-   2.4 GHz 8-Core Intel Core i9\n-   64 GB 2667 MHz DDR4\n-   R version 4.2.0 on x86_64-apple-darwin17.0 (64-bit)\n\n# Execution\n\nAt a high level, the method is quite simple.\n\n1.  Create fake large data sets\n2.  For each, measure read and write speed for all approaches\n3.  Plot and compare\n\nThe devil is in the details though, especially concerning scaling this approach. While myy first approaches were linear script based, this quickly uncovered the ineffectiveness of scripts - the lack of ability to cache & reference previous results (at least without significant coding). I quickly pivoted to using `{targets}` to build this measurement pipeline, learning 'dynamic branching' along the way, and I must say - I'm glad I did so.\n\n::: column-margin\nIn case you're not familiar, `{targets}` is a \"*...Make-like pipeline tool for Statistics and data science ... \\[to\\] maintain a reproducible workflows...*\" Learn more [here](https://books.ropensci.org/targets/).\n:::\n\nNot only is the pipeline *much* easier to grok, but is extremely salable. Since `{targets}` caches previous runs, I can very rapidly experiment with data, modeling & plotting combinations while maintaining end-to-end verifyable reproducibility, without wasting any time reexecuting long-running experiments.\n\nI won't delve into how I created this pipeline in this post, but the code base is available [here](https://github.com/rsangole/perf-tests).\n\n## Data\n\n\n::: {.cell .column-margin hash='data-read-write-perf_cache/html/unnamed-chunk-1_9fe00822f668201a19cbb2a77d1c008d'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic-2\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> cat_A </th>\n   <th style=\"text-align:left;\"> cat_B </th>\n   <th style=\"text-align:left;\"> date </th>\n   <th style=\"text-align:right;\"> num_1 </th>\n   <th style=\"text-align:right;\"> num_2 </th>\n   <th style=\"text-align:right;\"> num_3 </th>\n   <th style=\"text-align:right;\"> num_4 </th>\n   <th style=\"text-align:left;\"> chr_1 </th>\n   <th style=\"text-align:left;\"> chr_2 </th>\n   <th style=\"text-align:left;\"> chr_3 </th>\n   <th style=\"text-align:left;\"> chr_4 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Alabama </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2023-03-01 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 0.13 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n   <td style=\"text-align:left;\"> h </td>\n   <td style=\"text-align:left;\"> p </td>\n   <td style=\"text-align:left;\"> i </td>\n   <td style=\"text-align:left;\"> u </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alabama </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2022-10-18 </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.29 </td>\n   <td style=\"text-align:right;\"> 0.14 </td>\n   <td style=\"text-align:left;\"> s </td>\n   <td style=\"text-align:left;\"> o </td>\n   <td style=\"text-align:left;\"> h </td>\n   <td style=\"text-align:left;\"> a </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alaska </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> 2023-03-20 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:right;\"> 0.45 </td>\n   <td style=\"text-align:right;\"> 0.21 </td>\n   <td style=\"text-align:left;\"> n </td>\n   <td style=\"text-align:left;\"> g </td>\n   <td style=\"text-align:left;\"> l </td>\n   <td style=\"text-align:left;\"> n </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Arkansas </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2023-08-07 </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 0.46 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n   <td style=\"text-align:left;\"> c </td>\n   <td style=\"text-align:left;\"> p </td>\n   <td style=\"text-align:left;\"> r </td>\n   <td style=\"text-align:left;\"> b </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nI created some fake data with categoricals, numerics and date-time vars. I've used 'long' data here, since it represented the use case I typically have, although I wonder if the results would change if the data were wider. After a few iterations, I settled on number of rows: 100k, 1M, and 5M. The results for number of rows = 1M+ did not change, to be honest.\n\n## Approaches\n\nI tested the packages I tend to use the most, along with a new one `vroom` and the traditional RDS format from R. All in all, I tested:\n\n1.  `data.table` - both the traditional CSV, and `yaml = TRUE` formulations\n2.  `arrow` - two configurations: `parquet` and `arrow_csv`\n3.  `qs`\n4.  `vroom`\n5.  `RDS`\n\nEach approach is shown by a green square in the middle here. Each square represents a read and write using `{microbenchmark}` to measure the execution times. In total, this pipeline runs {7 read/write approaches x 3 data sets x 10 repeats} for a total of 210 runs.\n\n\n::: {.cell hash='data-read-write-perf_cache/html/unnamed-chunk-2_9651d2b1bc6827b6ec87a479318d34bc'}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-061ae233854d8c8cdc98\" style=\"width:100%;height:464px;\" class=\"visNetwork html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-061ae233854d8c8cdc98\">{\"x\":{\"nodes\":{\"name\":[\"arrow_csv\",\"arrow_parquet\",\"data\",\"datatable_classic\",\"datatable_yaml\",\"desired_nrows\",\"export\",\"plot_comparison\",\"plot_comparison_scaled\",\"plot_sizes\",\"qs\",\"rds\",\"results\",\"vroom\"],\"type\":[\"pattern\",\"pattern\",\"pattern\",\"pattern\",\"pattern\",\"stem\",\"stem\",\"stem\",\"stem\",\"stem\",\"pattern\",\"pattern\",\"stem\",\"pattern\"],\"status\":[\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\",\"uptodate\"],\"seconds\":[64.381,34.753,3.416,45.842,0,0.522,0.11,0.047,0.154,0.082,27.987,445.982,0.01,114.748],\"bytes\":[398,391,114360337,332,0,61,247454,96348,96901,67200,323,333,3811,330],\"branches\":[3,3,3,3,3,null,null,null,null,null,3,3,null,3],\"label\":[\"arrow_csv\",\"arrow_parquet\",\"data\",\"datatable_classic\",\"datatable_yaml\",\"desired_nrows\",\"export\",\"plot_comparison\",\"plot_comparison_scaled\",\"plot_sizes\",\"qs\",\"rds\",\"results\",\"vroom\"],\"color\":[\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\",\"#354823\"],\"id\":[\"arrow_csv\",\"arrow_parquet\",\"data\",\"datatable_classic\",\"datatable_yaml\",\"desired_nrows\",\"export\",\"plot_comparison\",\"plot_comparison_scaled\",\"plot_sizes\",\"qs\",\"rds\",\"results\",\"vroom\"],\"level\":[3,3,2,3,3,1,6,5,5,5,3,3,4,3],\"shape\":[\"square\",\"square\",\"square\",\"square\",\"square\",\"dot\",\"dot\",\"dot\",\"dot\",\"dot\",\"square\",\"square\",\"dot\",\"square\"]},\"edges\":{\"from\":[\"data\",\"data\",\"plot_comparison\",\"plot_comparison_scaled\",\"plot_sizes\",\"data\",\"data\",\"data\",\"results\",\"data\",\"desired_nrows\",\"results\",\"arrow_csv\",\"arrow_parquet\",\"datatable_classic\",\"datatable_yaml\",\"qs\",\"rds\",\"vroom\",\"data\",\"results\"],\"to\":[\"qs\",\"datatable_yaml\",\"export\",\"export\",\"export\",\"datatable_classic\",\"vroom\",\"rds\",\"plot_comparison_scaled\",\"arrow_csv\",\"data\",\"plot_sizes\",\"results\",\"results\",\"results\",\"results\",\"results\",\"results\",\"results\",\"arrow_parquet\",\"plot_comparison\"],\"arrows\":[\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\"]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\",\"physics\":false},\"manipulation\":{\"enabled\":false},\"edges\":{\"smooth\":{\"type\":\"cubicBezier\",\"forceDirection\":\"horizontal\"}},\"physics\":{\"stabilization\":false},\"interaction\":{\"zoomSpeed\":1},\"layout\":{\"hierarchical\":{\"enabled\":true,\"direction\":\"LR\"}}},\"groups\":null,\"width\":null,\"height\":null,\"idselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"highlight\":{\"enabled\":true,\"hoverNearest\":false,\"degree\":{\"from\":1,\"to\":1},\"algorithm\":\"hierarchical\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":true,\"fit\":false,\"resetHighlight\":true,\"clusterOptions\":null,\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"},\"legend\":{\"width\":0.2,\"useGroups\":false,\"position\":\"right\",\"ncol\":1,\"stepX\":100,\"stepY\":100,\"zoom\":true,\"nodes\":{\"label\":[\"Up to date\",\"Pattern\",\"Stem\"],\"color\":[\"#354823\",\"#899DA4\",\"#899DA4\"],\"shape\":[\"dot\",\"square\",\"dot\"]},\"nodesToDataframe\":true},\"tooltipStay\":300,\"tooltipStyle\":\"position: fixed;visibility:hidden;padding: 5px;white-space: nowrap;font-family: verdana;font-size:14px;font-color:#000000;background-color: #f5f4ed;-moz-border-radius: 3px;-webkit-border-radius: 3px;border-radius: 3px;border: 1px solid #808074;box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n# Results\n\nSo what do the results look like? To be honest, they quite agree with what I had experienced in my work with large data sets.\n\n## Speeds\n\nThe plots below show the absolute read & write speeds on the left (measured in seconds), with relative speed decrease as compared to the best performer on the right.\n\n**tldr**\n\n-   `arrow-parquet` are the clear winner when it comes to read-speed. The next best is `vroom` which is 2x slower. Almost every other option is 5x-7x slower than `arrow`.\n\n-   `qs` performs the best for write-speeds. `arrow` and `data.table` are the next best at 2-3x slower speeds than `qs`.\n\n-   For `data.table` , the `yaml = TRUE` setting doesn't make any difference to read-speeds. *This one surprised me, since I thought it would speed it up at least a bit.*\n\n-   `rds` is the slowest, unsurprisingly, at 20-30x slower than `arrow`\n\n\n::: {.cell .column-screen .fig-column-screen-inset-shaded layout-ncol=\"1\" layout-align=\"center\" hash='data-read-write-perf_cache/html/unnamed-chunk-3_86c1f08d2b023daf32b2c53c4e1157cd'}\n::: {.cell-output-display}\n![](data-read-write-perf_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n## File Sizes\n\nThis one is a doozey! For large 5M row data sets, there's a marked level shift between the compressed and uncompressed file formats!\n\n\n::: {.cell hash='data-read-write-perf_cache/html/unnamed-chunk-4_f8a65df556b7b2112a396f01f60fa882'}\n::: {.cell-output-display}\n![](data-read-write-perf_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nI'm curious to see if this result varies for wider datasets, or datasets with more/less characters and numerics.\n\n# Takeaway\n\nFor the foreseeable future, I'm indexing very heavily to arrow for my structured datasets. It's fast read and write times, combined with ability for lazy execution of queries on partitioned data makes it a clear winner for large data for my use cases.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/vis-9.1.0/vis-network.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/vis-9.1.0/vis-network.min.js\"></script>\n<script src=\"../../site_libs/visNetwork-binding-2.1.0/visNetwork.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}