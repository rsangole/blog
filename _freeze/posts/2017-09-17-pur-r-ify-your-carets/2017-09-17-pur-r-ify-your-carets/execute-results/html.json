{
  "hash": "e2ea6ab4e6e39fa6517234469b8fdc95",
  "result": {
    "markdown": "---\ntitle: Pur(r)ify Your Carets\ndate: '2017-09-17'\ncategories:\n  - Programming Practices\ndescription: You'll learn how to use `purrr`, `caret` and `list-cols` to quickly create hundreds of dataset + model combinations, store data & model objects neatly in one tibble, and post process programatically. These tools enable succinct functional programming in which a lot gets done with just a few lines of code.\n\nimage: carrots.jpg\n---\n\n\n\n\n## The motivation\n\nI want to write a quick blogpost on two phenomenal pieces of code written by Kuhn et al, and Wickham et al, namely - [purrr](https://cran.r-project.org/web/packages/purrr/index.html), [caret](https://cran.r-project.org/web/packages/caret/index.html). These play so well with the [tidyverse](https://cran.r-project.org/web/packages/tidyverse/index.html) that they have become an indispensible part of my repertoire. \n\nIn any datascience project, I want to investigate the effect of various combinations of:\n\n* Variable transformations\n* Variable selection\n* Grouped vs ungrouped categorical variables\n* Models of different types\n* Different hyperparameter tuning methods\n\nFor each of the various combinations possible, I want to quantify model performance using common performance metrics like AIC or SBC. Commonly, I'll select the model that has the 'best' possible performance among all such models.\n\nTraditionally, I end up with many R objects: one for each new combination of transformation-model_type-tuning_method. For example, `boostFit`, `xgbFit`, `glmFit`, `elastinetFit` for untransformed variables. If I have any transformations, I might also have `boostFit.xform`, `xgbFit.xform`, `glmFit.xform` etc. Add to that, investigation of grouped vs ungrouped variables... `boostFit.xform.grouped`, `xgbFit.xform.ungrouped` etc. You get the idea. \n\nThe challenge with this approach is that the data and the models remain separated, there's a lot of repeat code for object management, manipulation and plotting, and in order to compare all the models together, we have to somehow stitch the results together. (For the last point, `resamples()` in `caret` works beautifully, but requires the same number of resamples in each model.)\n\nThe approach I'm presenting below is a combination of a few approaches I learnt through the [APM](http://appliedpredictivemodeling.com/) book, the `caret` documentation,  grammar and verbage in `tidyverse`, as well as a couple of useful talks in the 2017 R Studio conferenence in Orlando [Notably ones on [purrr](https://www.rstudio.com/resources/videos/happy-r-users-purrr/) and  [list-cols](https://www.rstudio.com/resources/videos/using-list-cols-in-your-dataframe/)]. What you'll also see is that the code is extremely succint, which is simply a joy to write and read.\n\n  \n## An example using `BostonHousing` data\n  \n  \n### Load libs & data\n\nThe libraries I'm using here are `tidyr`, `tibble`, `dplyr`, `magrittr`, `purrr`, and `caret`. The dataset is from `mlbench`. \n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-1_9a4ffa60cd7ca987c7085d8e5aa98770'}\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(caret)\nlibrary(mlbench)\nlibrary(xgboost)\ndata(\"BostonHousing\")\n```\n:::\n\n***\n  \n#### Transformations on Xs\n\nFor the purposes of this demonstration, I'll simply create two new sets variables using a Box-Cox transformation - `caret`'s `preProcess()` makes this easy - and the squared values of the originals. Save each new variable-set in a new character vector which follows the naming convention `preds.xxxx`.[^1]\n\n[^1]: This makes it super easy to find all such variable sets quickly using `ls(pattern = 'preds')` and store it in a character vector.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-2_29e74589ad734698932afef7eec1dfe4'}\n\n```{.r .cell-code}\n# The originals\nresponse <- 'medv'\npreds.original <- colnames(BostonHousing[,1:13])\n\n# Box-Cox transformation\nprepTrain <- preProcess(x = BostonHousing[,preds.original], method = c('BoxCox'))\nboxcoxed <- predict(prepTrain,newdata = BostonHousing[,preds.original])\ncolnames(boxcoxed) <- paste0(colnames(boxcoxed),'.boxed')\npreds.boxcoxed <- colnames(boxcoxed)\n\n# Squaring\nsquared <- (BostonHousing[,c(1:3,5:13)])^2\ncolnames(squared) <- paste0(colnames(squared),'.sq')\npreds.sq <- colnames(squared)\n\n# All together now...\nBostonHousing %<>% \n  cbind(boxcoxed,squared)\n\n# Make sure everything is a numerical (for xgboost to work), and also NOT a tibble (some caret functions have trouble with tibbles)\nBostonHousing %<>% \n  map_df(.f = ~as.numeric(.x)) %>% as.data.frame()\n\nstr(BostonHousing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t506 obs. of  39 variables:\n $ crim         : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn           : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus        : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas         : num  1 1 1 1 1 1 1 1 1 1 ...\n $ nox          : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm           : num  6.58 6.42 7.18 7 7.15 ...\n $ age          : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis          : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad          : num  1 2 2 3 3 3 5 5 5 5 ...\n $ tax          : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio      : num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b            : num  397 397 393 395 397 ...\n $ lstat        : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv         : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ crim.boxed   : num  -5.06 -3.6 -3.6 -3.43 -2.67 ...\n $ zn.boxed     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus.boxed  : num  0.994 2.966 2.966 0.914 0.914 ...\n $ chas.boxed   : num  1 1 1 1 1 1 1 1 1 1 ...\n $ nox.boxed    : num  -0.83 -1.09 -1.09 -1.13 -1.13 ...\n $ rm.boxed     : num  2.81 2.76 3 2.94 2.99 ...\n $ age.boxed    : num  175 224 161 110 137 ...\n $ dis.boxed    : num  1.41 1.6 1.6 1.8 1.8 ...\n $ rad.boxed    : num  0 0.693 0.693 1.099 1.099 ...\n $ tax.boxed    : num  1.88 1.87 1.87 1.87 1.87 ...\n $ ptratio.boxed: num  117 158 158 174 174 ...\n $ b.boxed      : num  78764 78764 77157 77866 78764 ...\n $ lstat.boxed  : num  1.89 2.78 1.61 1.2 1.99 ...\n $ crim.sq      : num  3.99e-05 7.46e-04 7.45e-04 1.05e-03 4.77e-03 ...\n $ zn.sq        : num  324 0 0 0 0 ...\n $ indus.sq     : num  5.34 49.98 49.98 4.75 4.75 ...\n $ nox.sq       : num  0.289 0.22 0.22 0.21 0.21 ...\n $ rm.sq        : num  43.2 41.2 51.6 49 51.1 ...\n $ age.sq       : num  4251 6225 3733 2098 2938 ...\n $ dis.sq       : num  16.7 24.7 24.7 36.8 36.8 ...\n $ rad.sq       : num  1 4 4 9 9 9 25 25 25 25 ...\n $ tax.sq       : num  87616 58564 58564 49284 49284 ...\n $ ptratio.sq   : num  234 317 317 350 350 ...\n $ b.sq         : num  157530 157530 154315 155733 157530 ...\n $ lstat.sq     : num  24.8 83.54 16.24 8.64 28.41 ...\n```\n:::\n:::\n\n\nHere's our new predictor variable sets:\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-3_ee822c8b11a752c59b7b922277cda175'}\n\n```{.r .cell-code}\npred_varsets <- ls(pattern = 'preds')\npred_varsets\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"preds.boxcoxed\" \"preds.original\" \"preds.sq\"      \n```\n:::\n:::\n\n***\n### Create a starter dataframe\n\nI first create a starter dataframe where the input data is repeated as many times as the number of predictor variable sets. `enframe()` allows us to embed objects a dataframe column.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-4_9e284ada7a6c4c3db3543efec5ea662e'}\n\n```{.r .cell-code}\nnum_var_select <- length(pred_varsets)\nlist(BostonHousing) %>% \n    rep(num_var_select) %>% \n    enframe(name = 'id', value = 'rawdata') %>% \n    mutate(pred_varsets = pred_varsets) -> starter_df\nstarter_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n     id rawdata         pred_varsets  \n  <int> <list>          <chr>         \n1     1 <df [506 × 39]> preds.boxcoxed\n2     2 <df [506 × 39]> preds.original\n3     3 <df [506 × 39]> preds.sq      \n```\n:::\n:::\n\n\nNow, I split the raw data into `train.X` column which houses data only for those predictor variables identified in the `pred_varsets` column. `map2` is a great function which allows a mapping to be done over two variables and passed to a function.  \n\nI also create a `train.Y` for the response variable here.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-5_b55085a588748f0c5aa71ed9a14e6088'}\n\n```{.r .cell-code}\n# Function to select columns in the raw data\nfilterColumns <- function(x,y){\n    x[,(colnames(x) %in% eval(parse(text=y)))]\n}\n\n# Create X and Y columns\nstarter_df %<>% \n  transmute(\n  id,\n  pred_varsets,\n  train.X = map2(rawdata, pred_varsets,  ~ filterColumns(.x, .y)),\n  train.Y = map(rawdata, ~ .x$medv)\n  )\n\nstarter_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n     id pred_varsets   train.X         train.Y    \n  <int> <chr>          <list>          <list>     \n1     1 preds.boxcoxed <df [506 × 13]> <dbl [506]>\n2     2 preds.original <df [506 × 13]> <dbl [506]>\n3     3 preds.sq       <df [506 × 12]> <dbl [506]>\n```\n:::\n:::\n\n***\n### Select the models\n\nThis is where I can select which models I want in the analysis. Each model should be in a function of this style:\n\n    modelName <- function(X, Y){\n        ctrl <- trainControl(\n            ...\n        )\n        train(\n            x = X,\n            y = Y,\n            trContrl = ctrl,\n            method = '## modelname ##',\n            ...\n        )\n    }\n\nI'm using `caret` exclusively, so each function needs a `trainControl()` and a `train()`. Learn more about `caret` [here](http://topepo.github.io/caret/index.html).\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-6_1853505d25be86dd9c108bbdec28be93'}\n\n```{.r .cell-code}\nrpartModel <- function(X, Y) {\n    ctrl <- trainControl(\n        ## 5-fold CV\n        method = \"repeatedcv\",\n        number = 5\n    )\n    train(\n        x = X,\n        y = Y,\n        method = 'rpart2',\n        trControl = ctrl,\n        tuneGrid = data.frame(maxdepth=c(2,3,4,5)),\n        preProc = c('center', 'scale')\n    )\n}\nxgbTreeModel <- function(X,Y){\n    ctrl <- trainControl(\n        ## 5-fold CV\n        method = \"repeatedcv\",\n        number = 5\n    )\n    train(\n        x=X,\n        y=Y,\n        method = 'xgbTree',\n        trControl = ctrl,\n        tuneGrid = expand.grid(nrounds = c(100,300,500), \n                              max_depth = c(2,4,6) ,\n                              eta = 0.1,\n                              gamma = 1, \n                              colsample_bytree = 1, \n                              min_child_weight = 1, \n                              subsample = 1),\n        preProc = c('center', 'scale')\n    )\n}\n```\n:::\n\n\nOnce these functions are setup, `enframe` these into a dataframe.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-7_35c9e34f23227ac64cfaad4a53b5d6e2'}\n\n```{.r .cell-code}\nmodel_list <- list(rpartModel=rpartModel,\n                   xgbModel=xgbTreeModel) %>%\n    enframe(name = 'modelName',value = 'model')\n\nmodel_list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  modelName  model \n  <chr>      <list>\n1 rpartModel <fn>  \n2 xgbModel   <fn>  \n```\n:::\n:::\n\n***\n### Create data-model combinations\n\nNow, we're ready to combine the two together. `train_df` has all the predictor varset combinations, `model_list` has the list of all models. I'm assuming I want to run each combination of the two; so if I have 3 variable sets, and 2 models, I have a total of 6 models to run. This code sets that up:\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-8_1597d47504d4c74a2fc7b7f2f3116b92'}\n\n```{.r .cell-code}\ntrain_df <-\n    starter_df[rep(1:nrow(starter_df),nrow(model_list)),]\n\ntrain_df %<>%\n    bind_cols(\n        model_list[rep(1:nrow(model_list),nrow(starter_df)),] %>% arrange(modelName)\n    ) %>%\n    mutate(id=1:nrow(.))\ntrain_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n     id pred_varsets   train.X         train.Y     modelName  model \n  <int> <chr>          <list>          <list>      <chr>      <list>\n1     1 preds.boxcoxed <df [506 × 13]> <dbl [506]> rpartModel <fn>  \n2     2 preds.original <df [506 × 13]> <dbl [506]> rpartModel <fn>  \n3     3 preds.sq       <df [506 × 12]> <dbl [506]> rpartModel <fn>  \n4     4 preds.boxcoxed <df [506 × 13]> <dbl [506]> xgbModel   <fn>  \n5     5 preds.original <df [506 × 13]> <dbl [506]> xgbModel   <fn>  \n6     6 preds.sq       <df [506 × 12]> <dbl [506]> xgbModel   <fn>  \n```\n:::\n:::\n\n***\n### Solve the models\n\nThe data is almost all setup now. `invoke_map()` is a function which can call functions and pass it arguments. Since we need to pass both `train.X` and `train.Y` together, there's an intermediate call to `map2()` to \"listify\" these first into `params`.\n\nAll them models solve, and their results (the model object itself) is stored in `modelFits`.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-9_42b58f75322d562792c3e217a7234b26'}\n\n```{.r .cell-code}\ntrain_df %<>%\n  mutate(params = map2(train.X, train.Y,  ~ list(X = .x, Y = .y)),\n                 modelFits=invoke_map(model,params)\n         )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:12] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:13] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:14] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:15] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:16] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:17] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:19] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:20] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:21] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:22] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:00:23] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n```\n:::\n\n```{.r .cell-code}\ntrain_df %>% dplyr::select(pred_varsets,modelName,params,modelFits)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  pred_varsets   modelName  params           modelFits\n  <chr>          <chr>      <list>           <list>   \n1 preds.boxcoxed rpartModel <named list [2]> <train>  \n2 preds.original rpartModel <named list [2]> <train>  \n3 preds.sq       rpartModel <named list [2]> <train>  \n4 preds.boxcoxed xgbModel   <named list [2]> <train>  \n5 preds.original xgbModel   <named list [2]> <train>  \n6 preds.sq       xgbModel   <named list [2]> <train>  \n```\n:::\n:::\n\n***\n### Extract results\n\nNow, I can extract pretty much any model performance or hypertuning parameter using `purrr`. Since `caret` is so lovingly standardized, it doesn't matter if I'm using a glm, xgboost, rpart2, or ann - the code remains the same. \n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-10_a585e5429551fb1fdc63b4d509857074'}\n\n```{.r .cell-code}\ntrain_df %<>% \n    mutate(\n        RMSE=map_dbl(modelFits,~max(.x$results$RMSE)),\n        RMSESD=map_dbl(modelFits,~max(.x$results$RMSESD)),\n        Rsq=map_dbl(modelFits,~max(.x$results$Rsquared)),\n        bestTune=map(modelFits,~.x$bestTune)\n    )\ntrain_df %>% dplyr::select(-train.X,-train.Y,-params,-modelFits)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n     id pred_varsets   modelName  model   RMSE RMSESD   Rsq bestTune    \n  <int> <chr>          <chr>      <list> <dbl>  <dbl> <dbl> <list>      \n1     1 preds.boxcoxed rpartModel <fn>    6.15  0.698 0.711 <df [1 × 1]>\n2     2 preds.original rpartModel <fn>    5.87  1.15  0.742 <df [1 × 1]>\n3     3 preds.sq       rpartModel <fn>    6.28  0.950 0.719 <df [1 × 1]>\n4     4 preds.boxcoxed xgbModel   <fn>    3.44  0.532 0.887 <df [1 × 7]>\n5     5 preds.original xgbModel   <fn>    3.27  0.493 0.902 <df [1 × 7]>\n6     6 preds.sq       xgbModel   <fn>    3.31  0.683 0.895 <df [1 × 7]>\n```\n:::\n:::\n\n\nThis allows us to very quickly visualize the results using `lattice` or `ggplot` *across all models*.\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-11_f1ec4de0dda308fb0e74f8d412a29af5'}\n\n```{.r .cell-code}\nlattice::dotplot(Rsq~pred_varsets|modelName,train_df)\n```\n\n::: {.cell-output-display}\n![](2017-09-17-pur-r-ify-your-carets_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntrain_df %>% \n    ggplot(aes(x=pred_varsets,color=modelName))+\n    geom_point(aes(y=RMSE),size=2)+\n    geom_errorbar(aes(ymin = RMSE-RMSESD,ymax= RMSE+RMSESD),size=.5,width=.15)\n```\n\n::: {.cell-output-display}\n![](2017-09-17-pur-r-ify-your-carets_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\nSince the model fit objects themselves are embedded, I can still look *at each model's* internals. For example, to plot the results of the 5-fold CV on the grid search for the xgboost model:\n\n\n::: {.cell hash='2017-09-17-pur-r-ify-your-carets_cache/html/unnamed-chunk-12_d79bd1d7c6cc3e1ea15285fff027dc54'}\n\n```{.r .cell-code}\nplot(train_df$modelFits[train_df$modelName=='xgbModel' & train_df$pred_varsets=='preds.original'][[1]])\n```\n\n::: {.cell-output-display}\n![](2017-09-17-pur-r-ify-your-carets_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n## In conclusion\n\nThese packages make investigating a *very* large number of datasets and models easy. With just a few lines of code, I ran a total of 6 different models - and for each model: a 5-fold cross validation, and for the xgboost models: a grid search across two tuning parameters - to select the best model on any number of performance criteria. Yet, everything remains neatly arranged in one dataframe, which can be saved as .RData and retrived later.\n\nAlso remember that the raw data replicated in the `data` column of `starter_df` doesn't have to be the exact same dataset for each row either. so you could leverage this methodology for a train-validate-test approach, or for resampled training sets, where each row has completely different datasets embedded within. Really depends on your creativity and how you write subsequent code. You're definitely going to find more material online on this topic, be sure to check [r-bloggers](https://www.google.com/url?q=https://www.r-bloggers.com&sa=D&ust=1506507002058000&usg=AFQjCNGK9mqK55zVwsCvoDn7e40KFBTrbg).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}