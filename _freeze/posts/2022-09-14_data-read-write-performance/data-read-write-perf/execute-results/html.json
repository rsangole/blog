{
  "hash": "23f1be073c051de58cd3a322936c46d3",
  "result": {
    "markdown": "---\ntitle: Performance Benchmarking Data Read Write\ndate: '2022-09-17'\ndescription: Which of the popular data read write methods is faster? Let's find out.\nimage: _image.jpg\ncategories:\n  - Benchmarking\n---\n\n\n\n\n# Background\n\nIn a recent conversation, I was asked how to speed up execution of scripts dealing with computation on big data sets (biggish i.e. in-memory). The scripts had some data read and write features with some ETL sandwiched in between. Upon reviewing the code base, I found some immediate improvements by moving a bunch of ETL code from `{dplyr}` to `{data.table}`, which has been my defacto go to for large data sets.\n\nOn the data read/write side of the equation, I used to be an ardent `{qs}` user with intermittent forays into `{data.table::fread/fwrite}` functions. However, recently, I've switched majority of my work to `{arrow}` which has proven itself a strong ally in the war against slow data transaction times.\n\nThe question remains - which one works better? Particularly, which one works better on the machines I'm using as well as the server I have access to? My configuration has changed from my last performance review posting. I'm now running:\n\n-   MacBook Pro, running MacOS 12.5\n-   2.4 GHz 8-Core Intel Core i9\n-   64 GB 2667 MHz DDR4\n-   R version 4.2.0 on x86_64-apple-darwin17.0 (64-bit)\n\n# Execution\n\nAt a high level, the method is quite simple.\n\n1.  Create fake large data sets\n2.  For each, measure read and write speed for all approaches\n3.  Plot and compare\n\nThe devil is in the details though, especially concerning scaling this approach. While my first approaches were linear script based, this quickly uncovered the ineffectiveness of scripts - the lack of ability to cache & reference previous results (at least without significant coding). I quickly pivoted to using `{targets}` to build this measurement pipeline, learning 'dynamic branching' along the way, and I must say - I'm glad I did so.\n\n::: column-margin\nIn case you're not familiar, `{targets}` is a \"*...Make-like pipeline tool for Statistics and data science ... \\[to\\] maintain a reproducible workflows...*\" Learn more [here](https://books.ropensci.org/targets/).\n:::\n\nNot only is the pipeline *much* easier to grok, but is extremely scalable. Since `{targets}` caches previous runs, I can very rapidly experiment with data, modeling & plotting combinations while maintaining end-to-end verifiable reproducibility, without wasting any time re-executing long-running experiments.\n\nI won't delve into how I created this pipeline in this post, but the code base is available [here](https://github.com/rsangole/perf-tests).\n\n## Data\n\n\n::: {.cell .column-margin hash='data-read-write-perf_cache/html/unnamed-chunk-1_9fe00822f668201a19cbb2a77d1c008d'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic-2\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> cat_A </th>\n   <th style=\"text-align:left;\"> cat_B </th>\n   <th style=\"text-align:left;\"> date </th>\n   <th style=\"text-align:right;\"> num_1 </th>\n   <th style=\"text-align:right;\"> num_2 </th>\n   <th style=\"text-align:right;\"> num_3 </th>\n   <th style=\"text-align:right;\"> num_4 </th>\n   <th style=\"text-align:left;\"> chr_1 </th>\n   <th style=\"text-align:left;\"> chr_2 </th>\n   <th style=\"text-align:left;\"> chr_3 </th>\n   <th style=\"text-align:left;\"> chr_4 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Alabama </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2023-03-01 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 0.13 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n   <td style=\"text-align:left;\"> h </td>\n   <td style=\"text-align:left;\"> p </td>\n   <td style=\"text-align:left;\"> i </td>\n   <td style=\"text-align:left;\"> u </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alabama </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2022-10-18 </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.29 </td>\n   <td style=\"text-align:right;\"> 0.14 </td>\n   <td style=\"text-align:left;\"> s </td>\n   <td style=\"text-align:left;\"> o </td>\n   <td style=\"text-align:left;\"> h </td>\n   <td style=\"text-align:left;\"> a </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alaska </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> 2023-03-20 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:right;\"> 0.45 </td>\n   <td style=\"text-align:right;\"> 0.21 </td>\n   <td style=\"text-align:left;\"> n </td>\n   <td style=\"text-align:left;\"> g </td>\n   <td style=\"text-align:left;\"> l </td>\n   <td style=\"text-align:left;\"> n </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Arkansas </td>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:left;\"> 2023-08-07 </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 0.46 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n   <td style=\"text-align:left;\"> c </td>\n   <td style=\"text-align:left;\"> p </td>\n   <td style=\"text-align:left;\"> r </td>\n   <td style=\"text-align:left;\"> b </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nI created some fake data with categoricals, numerics and date-time vars. I've used 'long' data here, since it represented the use case I typically have, although I wonder if the results would change if the data were wider. After a few iterations, I settled on number of rows: 100k, 1M, and 5M. The results for number of rows = 1M+ did not change, to be honest.\n\n## Approaches\n\nI tested the packages I tend to use the most, along with a new one `vroom` and the traditional RDS format from R. All in all, I tested:\n\n1.  `data.table` - both the traditional CSV, and `yaml = TRUE` formulations\n2.  `arrow` - two configurations: `parquet` and `arrow_csv`\n3.  `qs`\n4.  `vroom`\n5.  `RDS`\n6.  `feather`\n7.  `fst`\n8.  `duckdb`\n\nEach approach is shown by a green square in the middle here. Each square represents a read and write using `{microbenchmark}` to measure the execution times. In total, this pipeline runs {10 read/write approaches x 3 data sets x 10 repeats} for a total of 300 runs.\n\n::: column-page\n![](images/paste-925DBC69.png){fig-align=\"center\" width=\"861\"}\n:::\n\n# Results\n\nSo what do the results look like? To be honest, they quite agree with what I had experienced in my work with large data sets.\n\n## Speeds\n\nThe plots below show the absolute read & write speeds on the left (measured in seconds), with relative speed decrease as compared to the best performer on the right.\n\nLet's just look at the largest data size of 5 million rows.\n\n**tldr**\n\n- _updated!_ `feather` (as opposed to `arrow-parquet`) is now the clear winner for read speeds! The next best is `arrow-parquet` at 2.5x slower. `feather` is simply _blazing!_ At sub-second read speeds, I had to confirm something wasn't going wrong in the code-base!\n\n-   `qs`, `fst` and `feather` all perform admirably for write-speeds. `arrow` and `data.table` are the next best at 2-3x slower speeds than `qs`.\n\n-   For `data.table` , the `yaml = TRUE` setting doesn't make any difference to read-speeds. *This one surprised me, since I thought it would speed it up at least a bit.*\n\n-  `rds` is the slowest, unsurprisingly, at 20-30x slower than `arrow`.\n\n- `duckdb` and `vroom` performance were the middle of the pack.\n\n\n::: {.cell .column-screen .fig-column-screen-inset-shaded layout-ncol=\"1\" layout-align=\"center\" hash='data-read-write-perf_cache/html/unnamed-chunk-2_fb6f5d14ed19d768572318900eb7817d'}\n::: {.cell-output-display}\n![](data-read-write-perf_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n## File Sizes\n\nThis one is a doozey! Significant differences across all the packages, especially for large 5M row data sets. There seem to be three groupings: [qs, rds and arrow-parquet], [fst, feather & duckdb], and finally [datatable, vroom and arrow-csv].\n\n\n::: {.cell hash='data-read-write-perf_cache/html/unnamed-chunk-3_ef07255b6743c1c08f06b9c9f099cfa9'}\n::: {.cell-output-display}\n![](data-read-write-perf_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nI'm curious to see if this result varies for wider data sets, or data sets with more/less characters and numerics.\n\n# Takeaway\n\nFor the foreseeable future, I'm _still_ indexing very heavily to arrow for my structured data sets, despite `feather` being faster (for some other reasons). It's fast read and write times, combined with ability for lazy execution of queries on partitioned data makes it a clear winner for large data for my use cases.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}